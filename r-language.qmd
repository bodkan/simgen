# R programming language

⚠️⚠️⚠️
**This chapter is a work-in-progress!**
⚠️⚠️⚠️

In a wider programming world, R sometimes has a slightly unfortunate reputation
as a badly designed "calculator language". A computing environment which is
(maybe!) good for working with data frames and creating figures, but that's about
it. However, while certainly very useful for data science, R is a full-blown
programming language which is actually quite powerful even from a purely
computer science perspective.

But still, this is a book about population genomics and data science in R.
Why does this matter?

Well, although this entire workshop will primarily focus on using R primarily
as a statistical and visualization environment, neglecting the aspects of R
which make it a "proper" programming language (or even considering data science
as "not real programming") is a huge problem.

Even when "just" doing data science and statistics, we still use typical
programming constructs, we need to be aware of underlying data types behind
our data (mostly contents of tables), and we need to still think algorithmically.
In short, even when we "just" do data science, we are still computer programmers.
Neglecting these things makes it easy to introduce bugs into our code, make
it hard to find those bugs, and make our programs less efficient even when
they do work.

This chapter will help you get familiar with some of the less obvious aspects
of the R language or programming in general, certainly the parts which are often
skipped in undergratuate courses in the life sciences. The good thing is,
there isn't that much we need to learn.

Let's say it again, because people with non-computational backgrounds often feel
inadequate when it comes to computational aspects of their work: **Even when you're
"just" writing data analysis scripts, even when you're "just" plotting results,
you're still writing programs. You're a programmer.** How exciting, right?
Exercises in this chapter are designed to make you comfortable with programming
and algorithmic thinking.

## Part 1: Atomic data types

```{r}
w1 <- 3.14
x1 <- 42
y1 <- "hello"
z1 <- TRUE
```

```{r}
w1
x1
y1
z1
```

```{r}
mode(w1)
mode(x1)
mode(y1)
mode(z1)
```

## Part 2: Vectors

```{r}
w2 <- c(1.0, 2.72, 3.14)
x2 <- c(1, 13, 42)
y2 <- c("hello", "folks", "!")
z2 <- c(TRUE, FALSE)
```

```{r}
w2
x2
y2
z2
```

```{r}
mode(w2)
mode(x2)
mode(y2)
mode(z2)
```

```{r}
is.vector(w2)
is.vector(x2)
is.vector(y2)
is.vector(z2)
```

Even scalars (singular values) are formally vectors:

```{r}
is.vector(1)
```

This is why we see the [1] index when we type a single number:

```{r}
1
```


In fact, even when we create a vector of length 1, we still get a scalar result:

```{r}
c(1)
```

Try creating a vector with values `1`, `"42"`, and `"hello"`. Can you do it? What happens when you try?

```{r}
mixed_vector1 <- c(1, "42", "hello")
mixed_vector1
```

As you can see, vectors must carry values of just one type. If they don't, they are converted by a straightforward "cascade" of coercions. Can you figure out some of these rules based on different composition of value types you put into the `c()` vector constructing function?

**Hint:** Try creating a vector which has integers and strings, integers and floats, integers and logicals, floats and logicals, floats and strings, and logicals and strings:

```{r}
c(1, "42", "hello")

c(1, 42.13, 123)

c(1, 42, TRUE)

c(1.12, 42.13, FALSE)

c(42.13, "hello")

c(TRUE, "hello")
```




## Part 3: Lists

```{r}
w3 <- list(1.0, 2.72, 3.14)
x3 <- list(1, 13, 42)
y3 <- list("hello", "folks", "!")
z3 <- list(TRUE, FALSE)
```

```{r}
w3
```

```{r}
mode(w3)
```

```{r}
is.list(w3)
```

Lists are vectors!

```{r}
is.vector(w3)
```

But vectors are not lists!

```{r}
is.list(w2)
```


## Part 4: Data frames

```{r}
df <- data.frame(
  w = c(1.0, 2.72, 3.14),
  x = c(1, 13, 42),
  y = c("hello", "folks", "!"),
  z = c(TRUE, FALSE, FALSE)
)

df
```

Data frames are lists!

```{r}
is.list(df)
```

Two ways of extracting a data-frame column:

- `$` operator (column name as a symbol):

```{r}
df$w
```

- `[[` operator (column name as a string):

```{r}
df[["w"]]
```

Note that the operator must be `[[`, because `[` does something slightly different

## Part 5: Inspecting column types

Let's go back to our example data frame:

```{r}
df1 <- data.frame(
  w = c(1.0, 2.72, 3.14),
  x = c(1, 13, 42),
  y = c("hello", "folks", "!"),
  z = c(TRUE, FALSE, FALSE)
)

df1
```

**Use the function `str()` and by calling `str(df1)`, inspect the types of
columns in the table.**

```{r}
str(df1)
```

Sometimes (usually when we read data from disk, like from another software),
a data point sneaks in which makes a column apparently non numeric. Consider
this new table called `df2`:

```{r}
df2 <- data.frame(
  w = c(1.0, 2.72, 3.14),
  x = c(1, "13", 42),
  y = c("hello", "folks", "!"),
  z = c(TRUE, FALSE, FALSE)
)

df2
```

Just by looking at this, the table looks the same as `df1` above. **Use `str()`
again to see where the problem is.**

```{r}
str(df2)
```



## Part 6: Functions

Let's say you have the following numeric vector:

```{r}
vec <- c(-0.32, 0.78, -0.68, 0.28, -1.96, 0.21, -0.07, -1.01, 0.06, 0.74,
         -0.37, -0.6, -0.08, 1.81, -0.65, -1.23, -1.28, 0.11, -1.74,  -1.68)
```

With numeric vectors (which could be base qualities, genotype qualities, 
$f$-statistics, etc.), we often need to compute a couple of summary
statistics (mean, median, quartile, minimum, maximum, etc.).

::: {.aside}
In R, we have a very useful built-in function `summary()`, which does exactly that.
But let's ignore this for the moment, for learning purposes.
:::

Here is how you can compute those summary statistics individually:

```{r}
min(vec)
quantile(vec, probs = 0.25)
median(vec)
mean(vec)
quantile(vec, probs = 0.75)
max(vec)
```

Now, you can imagine that you have many more of such vectors (results for different
sequenced samples, different computed population genetic metrics, etc.). Having
to type out all of these commands for every single one of those vectors would
very quickly get extremely tiresome. Worse still, when we would do this, we would
certainly resort to copy-pasting, which is guaranteed to lead to errors.

**Write a custom function called `my_summary`, which will accept a single input
named `values`, and returns a list which binds all the six summary statistics together.
Name the elements of that list as `"min"`, `"quartile_1"`, `"median"`, `"mean"`,
`"quartile_3"`, and `"max"`.**

```{r}
my_summary <- function(values) {
  a <- min(vec)
  b <- quantile(vec, probs = 0.25)
  c <- median(vec)
  d <- mean(vec)
  e <- quantile(vec, probs = 0.75)
  f <- max(vec)
  
  result <- list(min = a, quartile_1 = b, median = c, mean = d, quartile_3 = e, max = f)
  
  return(result)
}
```

Note that yes, we had to write the code anyway, but it is now "encapsulated" in
a fully self-contained form and can be called repeatably, without any copy-pasting.

In other words, if we imagine having these three vectors of numeric values:

```{r}
vec1 <- runif(10)
vec2 <- runif(10)
vec3 <- runif(10)
```

We can now compute our summary statistics by calling our function `my_summary()`:

```{r}
my_summary(vec1)
my_summary(vec2)
my_summary(vec3)
```

And, surprise! This is what the incredibly useful built-in function `summary()`
provided with every R installation does!

```{r}
summary(vec1)
summary(vec2)
summary(vec3)
```




## Part 7: Iteration and loops

Functions help us take pieces of code and _generalize_ them to reduce the amount
of code needed to do similar things, repeatedly, multiple times. You could think
of _iteration_ as generalizing those repetitions.

This is very vague and abstract



# Take home message

It's worth acknowledging that developing code as proper functions, using iteration,
and generally think about solving problems using "proper" program approaches
like that is a lot of work. It might seem that "just copying a bit of code
a couple of times" is easier. In the moment, it actually is.

Except we rarely, if ever, do things only once. We have to come back to old
scripts, update them, or add new steps. Doing this for every single bit of
copy pasted code (sometimes in multiple scripts) is awful, and almost always
results in bugs. Even missing one edit in one copy of a piece of code will
make our results wrong.

**It's always worth investing a bit extra time into extracting repeated
pieces of code into individual functions. It will always result in _less work_
in the long run.**
