[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Population Genomics and Data Science in R",
    "section": "",
    "text": "Introduction\nThis workbook contains materials for a work-in-progress course on the fundamentals of data science, computational population genomics and statistical inference in R, with a strong focus on good practices of reproducible research.\nThe general structure of the final product (to be developed over the course of 2025-2026) will be a set of interlinked worksheets with exercises, which students will be able to walk through during an interactive course in the classroom, but which will also serve as practical reference in their own projects later.\nThe git repository containing the sources of all materials for the entire course are available on GitHub at https://github.com/bodkan/simgen.\nThe intended audience are novice researchers who have just started (or are about to start) their careers in population genomics and evolutionary genomics, primarily master students or doctoral students in the early parts of their PhD journey. That said, the more advanced latter parts of the book focusing on simulation-based inference of demography and selection will be beneficial even to more seasoned researchers, who are looking for more efficient means to fit models using novel inference tools in the R ecosystem.\n\nThe work-in-progress rendering of the book is available at https://bodkan.github.io/simgen.\n\nCurrently planned outline\nA draft of a subset of the planned content is available in the menu on the left. However, there are still many parts missing, even in the chapters already present. That said, here’s an overview of some of the things the final course will include:\n\nR\n\nIntroduction to R\n\nBasic data types and container types\nData frames, functions, iteration\nAbsolute minimum on manipulation and plotting data with base R\n\nReproducible computing in R\n\nWhat makes a good project structure\nWhat is algorithmic thinking?\nCreating self-contained R command-line scripts\nReproducible reports and presentations with Quarto\n\nData science with tidyverse\n\nFiltering, subsetting, modifying, and manipulating tabular data\nData visualization with ggplot2\nBasics of spatial data science using sf\n\nComputational genomics with R\n\nGenomicRanges and friends\n\n\nslendr\n\nIntroduction to the slendr R package\nBuilding traditional demographic models with slendr\nSimulating genomic data\n\nWhat is a tree sequence?\nVCF files, EIGENSTRAT file format, genotype tables\n\n\nFundamentals of population genetics with slendr\n\nComputing tree sequence summary statistics\ndiversity, divergence, AFS\n\\(f\\)-statistics, \\(f_4\\)-ratio statistics\n\\(F_{st}\\)\nPCA\nIdentity-by-descent (IBD)\nAncestry tracts / chromosome painting\nAdmixture dating\n\nNatural selection with slendr\n\nNatural selection theory\nSimple one-locus simulation\nUseful selection summary statistics\nMore complex epistatic selection\n\nSimulation-based inference with demografr\n\nToy grid-based inference of \\(N_e\\) with AFS\nGrid-based inference with demografr (\\(f_4\\) and \\(f_4\\)-ratio)\nGrid-based admixture tract dating\nApproximate Bayesian Computation (ABC)\n\nIntroducing the workhorses of applied population genetics\n\nMDS / PCA\nADMIXTOOLS - \\(f\\)-statistics, qpAdm\nADMIXTURE / STRUCTURE\nIBD\nSelection scans\n\n\n\nAll content is available under the CC BY-SA 4.0 license.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "r-intro.html",
    "href": "r-intro.html",
    "title": "R language",
    "section": "",
    "text": "In a wider programming world, R sometimes has a slightly unfortunate reputation as a badly designed “calculator language”. A computing environment which is (maybe!) good for working with data frames and creating figures, but that’s about it. However, while certainly very useful for data science, R is a full-blown programming language which is actually quite powerful even from a purely computer science perspective.\nBut still, this is a book about population genomics and data science in R. Why does this matter how much “real coding” we do in it?\nWell, although this entire workshop will primarily focus on R primarily as as a statistical and visualization environment, neglecting the aspects of R which make it a “proper” programming language (or even considering data science as “not real programming”) is a huge problem.\nFirst, even when “just” doing data science and statistics, we still use typical programming constructs, we need to be aware of underlying data types behind our data (mostly contents of tables), and we need to think algorithmically. Neglecting these things makes it easy to introduce bugs into our code, make it hard to find those bugs, and make our programs less efficient even when they do work.\nThis chapter will help you get familiar with some of the less obvious aspects of the R language or programming in general, certainly the parts which are often skipped in undergratuate courses in the life sciences in favor of just teaching plotting and running statistical tests. The good thing is, there isn’t that much you need to learn. And what you do learn will continue paying dividents for the rest of your research career!\nLet’s say it again, because people with non-computational backgrounds often feel inadequate when it comes to computational aspects of their work: Even when you’re “just” writing data analysis scripts, even when you’re “just” plotting results, you’re still writing programs. You’re a programmer. How exciting, right? Exercises in this chapter are designed to make you comfortable with programming and algorithmic thinking.",
    "crumbs": [
      "R language"
    ]
  },
  {
    "objectID": "r-bootcamp.html",
    "href": "r-bootcamp.html",
    "title": "R bootcamp",
    "section": "",
    "text": "Getting help\nIn this first chapter, you will be exploring the fundamental, more technical aspects of the R programming language. We will focus on topics which are normally taken for granted and never explained in basic data science courses, which generally immediately jump to data manipulation and plotting.\nI strongly believe that getting familiar with the fundamentals of R as a complete programming language from a “lower-level” perspective, although it might seem a little overwhelming at the beginning, will pay dividends over and over your scientific career.\nThe alternative is relying on magical black box thinking, which might work when everything works smoothly… except things rarely work smoothly in anything related to computing. Bugs appear, programs crash, incorrect results are produced—only by understanding the fundamentals can you troubleshoot problems.\nWe call this chapter a “bootcamp” on purpose – we only have a limited amount of time to go through all of these topics, and we have to rush things through a bit. After all, the primary reason for the existence of this workshop is to make you competent researchers in computational population genomics, so the emphasis will still be on practical applications and solving concrete data science issues.\nStill, when we get to data science work in the following chapters, you will see that many things which otherwise remain quite obscure and magical boil down to a set of very simple principles introduced here. The purpose of this chapter is to show you these fundamentals.\nThis knowledge will make you much more confident in the results of your work, and much easier to debug issues and problems in your own projects, but also track down problems in other people’s code. The later happens much more often than you might think!\nBefore we even get started, there’s one thing you should remember: R (and R packages) have an absolutely stellar documentation and help system. What’s more, this documentation is standardized, has always the same format, and is accessible in the same way. The primary way of interacting with it from inside R (and RStudio) is the ? operator. For instance, to get help about the hist() function (histograms), you can type ?hist in the R console. This documentation has a consistent format and appears in the “Help” pane in your RStudio window.\nThere are a couple of things to look for:\nWhenever you’re lost or can’t remember some detail about some piece of R functionality, looking up ? documentation is always very helpful.\nAs a practice and to build a habit, whenever we introduce a new function like new_function() in this course, use ?new_function to open its documentation and skim through it. I do this many times a day to refresh my memory on how something works.",
    "crumbs": [
      "R language",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "r-bootcamp.html#getting-help",
    "href": "r-bootcamp.html#getting-help",
    "title": "R bootcamp",
    "section": "",
    "text": "On the top of the documentation page, you will always see a brief description of the arguments of each function. This is what you’ll be looking for most of the time (“How do I do specify this or that? How do I modify the behavior of the function?”).\nOn the bottom of the page are examples. These are small bits of code which often explain the behavior of some functionality in a very helpful way.",
    "crumbs": [
      "R language",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "r-bootcamp.html#exercise-0-creating-an-r-script-and-general-workflow",
    "href": "r-bootcamp.html#exercise-0-creating-an-r-script-and-general-workflow",
    "title": "R bootcamp",
    "section": "Exercise 0: Creating an R Script and general workflow",
    "text": "Exercise 0: Creating an R Script and general workflow\nLet’s start easy. Open RStudio, create a new R script (File -&gt; New file -&gt; R Script), save it somewhere on your computer as r-bootcamp.R (File -&gt; Save, doesn’t really matter where you save it).\nEvery time you encounter a new bit of code, looking like this (i.e., text shown in a grey box like this):\n\n# here\n# is\n# some\n123\n# R\n\"code\"\n\nplease copy it into your script. You can then put your cursor on the first line of that code, and hit CTRL + Enter (on Windows/Linux) or CMD + Enter (on macOS) to execute it, which will step over to the next executable line. (Generally up to the next following # comment block prefixed with '#'). Alternatively, you can also type it out to the R Console directly, and evaluate it by hitting Enter.\nIt will sound very annoying, but try to limit copy-pasting code only to very long commands. Typing things out by hand forces you to think about every line of code and that is very important! At least at the beginning.",
    "crumbs": [
      "R language",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "r-bootcamp.html#exercise-1-basic-data-types-and-variables",
    "href": "r-bootcamp.html#exercise-1-basic-data-types-and-variables",
    "title": "R bootcamp",
    "section": "Exercise 1: Basic data types and variables",
    "text": "Exercise 1: Basic data types and variables\nEvery time you create a value in R, an object is created in computer memory. For instance, when you type this and execute this command in the R console by pressing Enter, R creates a bit of text in memory:\n\n\"this is a bit of text\"\n\nYou can use the assignment operator in R &lt;- to store an object in a variable, here a variable called text_var:\n\ntext_var &lt;- \"this is a bit of text\"\n\nExcept saying that this “stores and object in a variable” is not correct, even though we always use this phrase. Instead, the &lt;- operator actually stores a reference to a bit of computer memory where that value is located. This means that even after you run this command next, \"this is a bit of text\" is still sitting in memory, even though it appears to have been overwritten by the number 42 (we just don’t have access to that text anymore):\n\ntext_var &lt;- \"this is a bit of text\"\ntext_var &lt;- 42\n\nSimilarly, when you run this bit of code, you don’t create a duplicate of that text value, the second variable refers to the same bit of computer memory:\n\ntext_var1 &lt;- \"some new text value\"\ntext_var2 &lt;- text_var1\n\nTo summarize: values (lines of text, numbers, data frames, matrices, lists, etc.) don’t have “names”. They exist “anonymously” in computer memory. Variables are nothing but “labels” for those values.\n\nIt might be very strange to start with something this technical (and almost philosophical!), but it is very much worth keeping this in mind, especially in more complex and huge data sets which we’ll get to later in our workshop.\nWe will continue saying things like “variable abc contains this or that value” (instead of “contains reference to that value in memory”) because of convenience, but this is just an oversimplification.\n\nWrite the following variable definitions in your R script and then evaluate this code in your R console by CTRL / CMD + Enter. (Note that hitting this shortcut will move the cursor to the next line, allowing you to step by step evaluate longer bits of code.)\n\nw1 &lt;- 3.14\nx1 &lt;- 42\ny1 &lt;- \"hello\"\nz1 &lt;- TRUE\n\nWhen you type the variable names in your R console, you’ll get them printed back, of course:\n\nw1\n\n[1] 3.14\n\nx1\n\n[1] 42\n\ny1\n\n[1] \"hello\"\n\nz1\n\n[1] TRUE\n\n\n\nProgramming involves assigning values (or generally some objects in computer memory in general) to variables. In our code, variables change values, so we often need to check what is a type of some variable—are we working with a number, or some text, etc.? typeof() is one of the functions that are useful for this.\nWhat are the data “types” you get when you apply function typeof() on each of these variables, i.e. when you type and evaluate a command like typeof(w1)? Compare the result to the values you saved in those variables—what do you get from typeof() on each of them?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nA “floating point” number is of a so-called type “double”:\n\ntypeof(w1)\n\n[1] \"double\"\n\n\nInterestingly, “integer number” written like 42 is also represented as “double”, even though we don’t see a decimal point:\n\ntypeof(x1)\n\n[1] \"double\"\n\n\nIn order to force a variable to be “really integer”, this is possible (note that strange L letter). But you can regard this as a quirk of R. It’s almost never a distinction that’s necessary. For 99.99% of my coding needs, a “number” is just a number, and “double” is OK even for integers:\n\nx1 &lt;- 42L\ntypeof(x1)\n\n[1] \"integer\"\n\n\nGenerally speaking, “text” is represented by the data type “character” and is always defined by surrounding something in \" double quotes \":\n\ntypeof(y1)\n\n[1] \"character\"\n\n\nAnd the last important data type is “logical”, indicating whether something is TRUE or FALSE:\n\ntypeof(z1)\n\n[1] \"logical\"\n\n\n\n\n\n\nYou can test whether or not a specific variable is of a specific type using functions such as is.numeric(), is.integer(), is.character(), is.logical(). See what results you get when you apply these functions on these four variables w1, x1, y1, z1. Pay close attention to the difference (or lack thereof?) between applying is.numeric() and is.integer() on variables containing “values which look like numbers” (42, 3.14, etc.).\nNote: This might seem incredibly boring and useless but trust me. In your real data, such as in data frames (discussed below), you will encounter variables with thousands of rows, sometimes millions. Being able to make sure that the values you get in your data-frame columns are of the expected type is something you will be doing very very often, especially when troubleshooting! So this is a good habit to get into.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nw1\n## [1] 3.14\nis.numeric(w1)\n## [1] TRUE\nis.integer(w1)\n## [1] FALSE\n\n\nx1\n## [1] 42\nis.numeric(x1)\n## [1] TRUE\nis.integer(x1)\n## [1] TRUE\n\n\ny1\n## [1] \"hello\"\nis.character(y1)\n## [1] TRUE\n\n\nz1\n## [1] TRUE\nis.logical(z1)\n## [1] TRUE\nis.numeric(z1)\n## [1] FALSE\nis.integer(z1)\n## [1] FALSE\n\n\n\n\nTo summarize (and oversimplify a little bit) R allows variables to have several types of data, most importantly:\n\nintegers (such as 42)\nnumerics (such as 42.13)\ncharacters (such as \"text value\")\nlogicals (TRUE or FALSE)\n\n\nWe will also encounter two types of “non-values”. We will not be discussing them in detail here, but they will be relevant later. For the time being, just remember that there are also:\n\nmissing values represented by NA—you will see this very often in data!\nundefined values represented by NULL\n\n\nWhat do you think is the practical difference between NULL and NA? In other words, when you encounter one or the other in the data, how would you interpret this?",
    "crumbs": [
      "R language",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "r-bootcamp.html#exercise-2-vectors",
    "href": "r-bootcamp.html#exercise-2-vectors",
    "title": "R bootcamp",
    "section": "Exercise 2: Vectors",
    "text": "Exercise 2: Vectors\nVectors are, roughly speaking, collections of values. We create a vector by calling the c() function (the “c” stands for “concatenate”, or “joining together”).\nCreate the following variables containing these vectors. Then inspect their data types by calling the typeof() function on them again, just like you did for “single-value variables” above. Again, copy-paste this into your script and evaluate using CTRL / CMD + Enter or paste it directly into your R Console and hit Enter:\n\nw2 &lt;- c(1.0, 2.72, 3.14)\nx2 &lt;- c(1, 13, 42)\ny2 &lt;- c(\"hello\", \"folks\", \"!\")\nz2 &lt;- c(TRUE, FALSE)\n\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\ntypeof(w2)\n\n[1] \"double\"\n\ntypeof(x2)\n\n[1] \"double\"\n\ntypeof(y2)\n\n[1] \"character\"\n\ntypeof(z2)\n\n[1] \"logical\"\n\n\n\n\n\n\nWe can use the function is.vector() to test that a given object really is a vector. Try this on your vector variables.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nis.vector(w2)\n\n[1] TRUE\n\nis.vector(x2)\n\n[1] TRUE\n\nis.vector(y2)\n\n[1] TRUE\n\nis.vector(z2)\n\n[1] TRUE\n\n\n\n\n\n\nWhat happens when you call is.vector() on the variables x1, y1, etc. from the previous Exercise (i.e., those which contain single values)?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nis.vector(42)\n\n[1] TRUE\n\n\nYes, even scalars (i.e., singular values) are formally vectors!\nThis is why we see the [1] index when we type a single number:\n\n1\n\n[1] 1\n\n\nIn fact, even when we create a vector of length 1, we still get a scalar result:\n\nc(1)\n\n[1] 1\n\n\nThe conclusion is, R doesn’t actually distinguish between scalars and vectors! A scalar (a single value) is simply a vector of length 1. Think of it this way: in a strange mathematically-focused way, even a single tree is a forest. 🙃\n\n\n\n\nDo elements of vectors need to be homogeneous (i.e., of the same data type)? Try creating a vector with values 1, \"42\", and \"hello\" using the c() function again, maybe save it into the variable mixed_vector. Can you do it? What happens when you try (and evaluate this variable in the R console)? Inspect the result in the R console (take a close look at how the result is presented in text and the quotes that you will see), or use the typeof() function again.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nmixed_vector &lt;- c(1, \"42\", \"hello\")\nmixed_vector\n\n[1] \"1\"     \"42\"    \"hello\"\n\n\n\ntypeof(mixed_vector)\n\n[1] \"character\"\n\n\nNotice that your values were all converted to text!\n\n\n\n\nYou can see that if vectors are not created with values of the same type, they are converted by a cascade of so-called “coercions”. A vector defined with a mixture of different values (i.e., the four “atomic types” we discussed in Exercise 1, doubles, integers, characters, and logicals) will be coreced to be only one of those types, given certain rules.\nDo a little detective work and try to figure out some of these coercion rules. Make a couple of vectors with mixed values of different types using the function c(), and observe what type of vector you get in return.\nHint: Try creating a vector which has integers and strings, integers and decimal numbers, integers and logicals, decimals and logicals, decimals and strings, and logicals and strings. Observe the format of the result that you get, and build your intuition on the rules of coercions by calling typeof() on each vector object to verify this intuition.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nNumbers mixed with characters are forced to become characters:\n\n\nv1 &lt;- c(1, \"42\", \"hello\")\nv1\n\n[1] \"1\"     \"42\"    \"hello\"\n\ntypeof(v1)\n\n[1] \"character\"\n\n\n\nIntegers and decimals are considered all doubles (this is what you saw above as R basically not distinguishing both that much, most of the time, for most practical reasons):\n\n\nv2 &lt;- c(1, 42.13, 123)\nv2\n\n[1]   1.00  42.13 123.00\n\ntypeof(v2)\n\n[1] \"double\"\n\n\n\nA logical mixed with numbers is forced into a number! TRUE is 1 and FALSE is 0. This is used extremely often, so remember this rule!\n\n\nv3 &lt;- c(1, 42, TRUE, FALSE)\nv3\n\n[1]  1 42  1  0\n\ntypeof(v3)\n\n[1] \"double\"\n\n\n\nv4 &lt;- c(1.12, 42.13, FALSE)\nv4\n\n[1]  1.12 42.13  0.00\n\ntypeof(v4)\n\n[1] \"double\"\n\n\n\nNumber is again mixed with text, giving type character again:\n\n\nv5 &lt;- c(42.13, \"hello\")\nv5\n\n[1] \"42.13\" \"hello\"\n\ntypeof(v5)\n\n[1] \"character\"\n\n\n\nLogical is also forced into a character:\n\n\nv6 &lt;- c(TRUE, \"hello\")\nv6\n\n[1] \"TRUE\"  \"hello\"\n\ntypeof(v6)\n\n[1] \"character\"\n\n\n\n\n\n\nOut of all these data type explorations, this Exercise is probably the most crucial for any kind of data science work. Why do you think I say this? Think about what can happen when someone does incorrect manual data entry in Excel.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nImagine what kinds of trouble can happen if you just load a table data from somewhere, if the values are not properly formatted. For instance, if a “numeric” column of your table has accidentally some characters (which can very easily happen when manually entering data in Excel, etc.). This will be much clearer when we get to data frames below.\n\n\n\n\nAlthough creating vectors manually c(\"using\", \"an\", \"approach\", \"like\", \"this\") is often helpful, particularly when testing bits of code and experimenting in the R console, it is impossible to create dozens or more values like this by hand.\nYou can create vector of consecutive values using several useful approaches. Try and experiment these options:\n\nCreate a sequence of values from i to j with a shortcut i:j. Create a vector of numbers from 7 to 23 like this.\nDo the same using the function seq(). Read ?seq to find out what parameters you should specify (and how) to get the same result as the i:j shortcut above to get vector 7 to 23.\nModify the arguments given to seq() so that you create a vector of numbers from 20 to 1.\nUse the by = argument of seq() to create a vector of only odd values starting from 1.\n\nseq() is one of the most useful utility functions in R, so keep it in mind!\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\n# 1\n7:23\n\n [1]  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n\n# 2\nseq(from = 7, to = 23)\n\n [1]  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n\n# 3\nseq(from = 20, to = 1)\n\n [1] 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4  3  2  1\n\n# 4\nseq(1, 20, by = 2)\n\n [1]  1  3  5  7  9 11 13 15 17 19\n\n\nThis might look boring, but these functions are super useful to generate indices for data, adding indices as columns to tabular data, etc.\n\n\n\n\nAnother very useful built-in helper function (especially when we get to the iteration Exercise below) is seq_along(). What does it give you when you run it on this vector, for instance?\n\nv &lt;- c(1, \"42\", \"hello\", 3.1416)\n\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nseq_along(v)\n\n[1] 1 2 3 4\n\n\nThis function allows you to quickly iterate over elements of a vector (or a list) using indices into that vector (or a list).",
    "crumbs": [
      "R language",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "r-bootcamp.html#exercise-3-lists",
    "href": "r-bootcamp.html#exercise-3-lists",
    "title": "R bootcamp",
    "section": "Exercise 3: Lists",
    "text": "Exercise 3: Lists\nLists (created with the list() function, equivalent to the c() function for vectors) are a little similar to vectors but very different in a couple of important respects. Remember how we tested what happens when we put different types of values in a vector (reminder: vectors must be “homogeneous” in terms of the data types of their elements!)?\nWhat happens when you create lists with different types of values using the code in the following chunk? Use typeof() on the resulting list variables and compare your results to those you got on “mixed value” vectors above.\n\nw3 &lt;- list(1.0, \"2.72\", 3.14)\nx3 &lt;- list(1, 13, 42, \"billion\")\ny3 &lt;- list(\"hello\", \"folks\", \"!\", 123, \"wow a number follows again\", 42)\nz3 &lt;- list(TRUE, FALSE, 13, \"string\")\n\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nWhen we type the list variable in the R console, we no longer see the “coercion” we observed for vectors (numbers remain numbers even though the list contains strings):\n\ny3\n\n[[1]]\n[1] \"hello\"\n\n[[2]]\n[1] \"folks\"\n\n[[3]]\n[1] \"!\"\n\n[[4]]\n[1] 123\n\n[[5]]\n[1] \"wow a number follows again\"\n\n[[6]]\n[1] 42\n\n\n\n\n\nCalling typeof() on the list in the R console will (disappointingly) not tell us much about the data types of each individual element. Why is that? Think about the mixed elements possible in a list.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\ntypeof(y3)\n\n[1] \"list\"\n\n\nWell, if a list can have multiple different types of elements, there’s no such a thing as a “type of a list”, in a way we can say that there’s a “numeric vector”:\n\ntypeof(c(1, 10, 135))\n\n[1] \"double\"\n\n\nOr a logical vector:\n\ntypeof(c(TRUE, FALSE, FALSE, TRUE))\n\n[1] \"logical\"\n\n\nMore on this below!\n\n\n\n\nTry also a different function called for str() (“str” standing for “structure”) and apply it on one of those lists in your R console. Is typeof() or str() more useful to inspect what kind of data is stored in a list (str will be very useful when we get to data frames for — spoiler alert! — exactly this reason). Why?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nThe structure function peeks into every element of the list:\n\nstr(y3)\n\nList of 6\n $ : chr \"hello\"\n $ : chr \"folks\"\n $ : chr \"!\"\n $ : num 123\n $ : chr \"wow a number follows again\"\n $ : num 42\n\n\nAs we said above, typeof() can only test the variable itself, but that variable has (potentially) multiple types of values in it:\n\ntypeof(w3)\n\n[1] \"list\"\n\n\n\n\n\n\nApply is.vector() and is.list() on one of the lists above (like w3 perhaps). What result do you get? Why do you get that result? Then run both functions on one of the vectors you created above (like w2). What does this mean?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nTesting both functions on a list like w3:\n\n\nw3\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"2.72\"\n\n[[3]]\n[1] 3.14\n\n\nLists are vectors!\n\nis.vector(w3)\n\n[1] TRUE\n\n\nLists are lists (obviously!):\n\nis.list(w3)\n\n[1] TRUE\n\n\n\nTesting both functions on a vector like w2:\n\n\nw2\n\n[1] 1.00 2.72 3.14\n\n\nVectors are not lists!\n\nis.list(w2)\n\n[1] FALSE\n\n\nIn conclusion:\n\nEvery list is (formally speaking) also a vector.\nBut vectors are not lists (because lists can have values of multiple types).\n\n\n\n\n\nNot only can lists contain arbitrary values of mixed types (atomic data types from Exercise 1 of this exercise), they can also contain “non-atomic” data as well, such as other lists! In fact, you can, in principle, create lists of lists of lists of… lists!\nTry creating a list() which, in addition to a couple of normal values (numbers, strings, doesn’t matter) also contains one or two other lists (we call these lists “nested lists” for this reason, or also “recursive lists”). Don’t think about this too much, just create something arbitrary “nested lists” to get a bit of practice. Save this in a variable called weird_list and type it back in your R console, just to see how R presents such data back to you. In the next Exercise, we will learn how to explore this type of data better.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nHere’s an example of such “nested list”:\n\nweird_list &lt;- list(\n  1,\n  \"two\",\n  list(\n    \"three\",\n    4,\n    list(5, \"six\", 7)\n  )\n)\n\nWhen we type it out in the R console, we see that R tries to lay out the structure of this data with numerical indices (we’ll talk about indices below!) indicating the “depth” of each nested pieces of data (either a plain number or character, or another list!)\n\nweird_list\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"two\"\n\n[[3]]\n[[3]][[1]]\n[1] \"three\"\n\n[[3]][[2]]\n[1] 4\n\n[[3]][[3]]\n[[3]][[3]][[1]]\n[1] 5\n\n[[3]][[3]][[2]]\n[1] \"six\"\n\n[[3]][[3]][[3]]\n[1] 7\n\n\n\n\n\nNote: If you are confused (or even annoyed) why we are even doing this, in the later discussion of data frames and spatial data structures, it will become much clearer why putting lists into other lists allows a whole another level of data science work. Please bear with me for now! This is just laying the groundwork for some very cool things later down the line… and, additionally, it’s intended to bend your mind a little bit and get comfortable with how complex data can be represented in computer memory.",
    "crumbs": [
      "R language",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "r-bootcamp.html#exercise-4-logicalboolean-expressions-and-conditionals",
    "href": "r-bootcamp.html#exercise-4-logicalboolean-expressions-and-conditionals",
    "title": "R bootcamp",
    "section": "Exercise 4: Logical/boolean expressions and conditionals",
    "text": "Exercise 4: Logical/boolean expressions and conditionals\nThis exercise is probably the most important thing you can learn to do complex data science work on data frames or matrices. It’s not necessary to remember all of this, just keep in mind we did these exercise so that you can refer to this information on the following days!\nLet’s recap some basic Boolean algebra in logic. The following basic rules apply (take a look at the truth table for a bit of a high school refresher) for the “and”, “or”, and “negation” operations:\n\nThe AND operator (represented by & in R, or often ∧ in math):\n\nBoth conditions must be TRUE for the expression to be TRUE.\n\nTRUE & TRUE == TRUE\nTRUE & FALSE == FALSE\nFALSE & TRUE == FALSE\nFALSE & FALSE == FALSE\n\n\nThe OR operator (represented by | in R, or often ∨ in math):\n\nAt least one condition must be TRUE for the expression to be TRUE.\n\nTRUE | TRUE == TRUE\nTRUE | FALSE == TRUE\nFALSE | TRUE == TRUE\nFALSE | FALSE == FALSE\n\n\nThe NOT operator (represented by ! in R, or often ¬ in math):\n\nThe negation operator turns a logical value to its opposite.\n\n!TRUE == FALSE\n!FALSE == TRUE\n\n\nComparison operators == (“equal to”), != (“not equal to”), &lt; or &gt; (“lesser / greater than”), and &lt;= or &gt;= (“lesser / greater or equal than”):\n\nComparing two things with either of these results in TRUE or FALSE result.\nNote: There are other operations and more complex rules, but we will be using these four exclusively (plus, the more complex rules can be derived using these basic operations anyway).\n\nLet’s practice working with logical conditions on some toy problems.\nCreate two logical vectors with three elements each using the c() function (pick random TRUE and FALSE values for each of them, it doesn’t matter at all), and store them in variables named A and B. What happens when you run A & B, A | B, and !A or !B in your R console? How do these logical operators work when you have multiple values, i.e. vectors?\nFor extra challenge, try to figure out the results of A & B, A | B, and !A in your head before you run the code in your R console!\nHint: Remember that a “single value” in R is a vector like any other (specifically vector of length one).\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nIt turns out that we can compare not just single values (scalars) but also multiple values like vectors. When we do this, R performs the given operation for every pair of elements at once!\n\nA &lt;- c(TRUE, FALSE, TRUE)\nB &lt;- c(FALSE, FALSE, TRUE)\n\n\nA & B\n\n[1] FALSE FALSE  TRUE\n\nA | B\n\n[1]  TRUE FALSE  TRUE\n\n!A\n\n[1] FALSE  TRUE FALSE\n\n!B\n\n[1]  TRUE  TRUE FALSE\n\n\n\n\n\n\nWhat happens when you apply base R functions all() and any() on your A and B (or !A and !B) vectors?\nNote: Remember the existence of all() and any() because they are very useful in daily data science work!\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nThese functions reduce a logical vector down to a single TRUE or FALSE value.\n\nA\n\n[1]  TRUE FALSE  TRUE\n\nall(A)\n\n[1] FALSE\n\nany(A)\n\n[1] TRUE\n\n\n\n\n\n\nIf the above all feels too technical and mathematical, you’re kind of right. That said, when you do data science, you will be using these logical expressions literally every single day. Why? Let’s return from abstract programming concepts back to reality for a second.\nThink about a table which has a column with some values, like sequencing coverage. Every time you filter for samples with, for instance, coverage &gt; 10, you’re performing exactly this kind of logical operation. You essentially ask, for each sample (each value in the column), which samples have coverage &gt; 10 (giving you TRUE) and which have less than 10 (giving you FALSE).\nFiltering data is about applying logical operations on vectors of TRUE and FALSE values (which boils down to “logical indexing” introduced below), even though those logical values rarely feature as data in the tables we generally work with. Keep this in mind!\n\nConsider the following vectors of coverages and origins of some set of example aDNA individuals (let’s imagine these are columns in a table you got from a bioinformatics lab) and copy them into your R script:\n\ncoverage &lt;- c(15.09, 48.85, 36.5, 47.5, 16.65, 0.79, 16.9, 46.09, 12.76, 11.51)\norigin &lt;- c(\"mod\", \"mod\", \"mod\", \"anc\", \"mod\", \"anc\", \"mod\", \"mod\", \"mod\", \"mod\")\n\nThen create a variable is_high which will contain a TRUE / FALSE vector indicating whether a given coverage value is higher than 10. Then create a variable is_ancient which will contain another logical vector indicating whether a given sample is \"anc\" (i.e., “ancient”).\nHint: Remember that evaluating coverage &gt; 10 gives you a logical vector and that you can store that vector in a variable.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nis_high &lt;- coverage &gt; 10\nis_high\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n\n\n\nis_ancient &lt;- origin == \"anc\"\nis_ancient\n\n [1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE\n\n\n\n\n\n\nUse the AND operator & to test if there is any high coverage sample (is_high) which is also ancient (is_ancient).\nHint: Apply the any() function to a logical expression you get by comparing both variables using the & operation.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nThis tests whether individual high coverage samples are also ancient:\n\nis_high & is_ancient\n\n [1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nAnd this tests whether any high coverage samples are also ancient:\n\nany(is_high & is_ancient)\n\n[1] TRUE\n\n\nNote: You don’t always create dedicated throwaway temporary variables like this. You could easily do the same much more concisely (although maybe less readably). Both approaches are useful.\n\nany(coverage &gt; 10 & origin == \"anc\")\n\n[1] TRUE\n\n\n\n\n\n\nNow let’s say that you have a third vector in this hypothetical table, indicating whether or not is a given sample from Estonia:\n\nestonian &lt;- c(FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, \nFALSE)\n\nWrite now a more complex conditional expression, which will test if a given individual has (again) coverage higher than 10 and is “ancient” OR whether it’s Estonian (and so it’s coverage or “mod” state doesn’t matter).\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\n(coverage &gt; 10 & origin == \"ancient\") | estonian\n\n [1] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE\n\n\n\n\n\n\nThis was just a simple example of how you can use logical expressions to do filtering based on values of (potentially many) variables, all at once, in a so-called “vectorized” way (i.e., testing on many different values at once, getting a vector of TRUE / FALSE values as a result).\nYou’ll have much more opportunity to practice this in our sessions on tidyverse, but let’s continue with other fundamentals first, which will make your understanding of basic data manipulation principles even more solid.\n\nR has also equivalent operators && and ||. What do they do and how are they different from the & and | you already worked with? Pick some exercises from above and experiment with both versions of logical AND and OR operators to figure out what they do and how are they different.",
    "crumbs": [
      "R language",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "r-bootcamp.html#exercise-5-indexing-into-vectors-and-lists",
    "href": "r-bootcamp.html#exercise-5-indexing-into-vectors-and-lists",
    "title": "R bootcamp",
    "section": "Exercise 5: Indexing into vectors and lists",
    "text": "Exercise 5: Indexing into vectors and lists\nVectors and lists are sequential collections of values.\nTo extract a specific values(s) of a vector or a list (or to assign some value at its given position(s)), we use a so-called “indexing” operation (often also called “subsetting” operation for reasons that will become clear soon). Generally speaking, we can do indexing in three ways:\n\nnumerical-based indexing (by specifying a set of integer numbers, each representing a position in the vector/list we want to extract),\nlogical-based indexing (by specifying a vector of TRUE / FALSE values of the same length as the vector/list we’re indexing into, each representing whether or not – TRUE or FALSE – should a position be included in the indexing result)\nname-based indexing (by specifying names of elements to index)\n\nLet’s now practice those for vectors and lists separately. Later, when we introduce data frames, we will return to the topic of indexing again.\n\nVectors\n\n1. Numerical-based indexing\nTo extract an i-th element of a vector xyz, we can use the [] operator like this: xyz[i]. For instance, we can take the 13-th element of this vector as xyz[13].\nFamiliarize yourselves with the [] operator by taking out a specific value from this vector, let’s say its 5th element.\n\nv &lt;- c(\"hi\", \"folks\", \"what's\", \"up\", \"folks\")\n\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nv[5]\n\n[1] \"folks\"\n\n\n\n\n\n\nLike many operations in R, the [] operator is “vectorized”, meaning that it can actually accept multiple values given as a vector themselves (i.e, something like v[c(1, 3, 4)] will extract the first, third, and fourth element of the vector v.\nExtract the first and fifth element of the vector v. What happens if you try to extract a tenth element from v?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nv[c(1, 5)]\n\n[1] \"hi\"    \"folks\"\n\n\nAccessing a non-existent element gives us a “not available” or “missing” value.\n\nv[10]\n\n[1] NA\n\n\n\n\n\n\n\n2. Logical-based indexing\nRather than giving the [] operator a specific set of integer numbers, we can provide a vector of TRUE / FALSE values which specify which element of the input vector do we want to “extract”. Note that this TRUE / FALSE indexing vector must have the same length as our original vector!\n\nCreate variable containing a vector of five TRUE or FALSE values (i.e., with something like index &lt;- c(TRUE, FALSE, ...), the exact combination of logical values doesn’t matter), and use that index variable in a v[index] indexing operation on the v vector you created above.\nNote: As always, don’t forget that you can experiment in your R Console, write bits of “throwaway” commands just to test things out. You don’t have to “know” how to program something immediately in your head—I never do, at least. I always experiment in the R Console to build up understanding of some problem on some small example.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nHere’s our vector:\n\nv\n\n[1] \"hi\"     \"folks\"  \"what's\" \"up\"     \"folks\" \n\n\nThis index variable specifies the first, third, and fourth position of a vector:\n\nindex &lt;- c(TRUE, FALSE, TRUE, TRUE, FALSE)\n\nAnd we can index using this indexing vector variable like this, picking only elements of interest:\n\nv[index]\n\n[1] \"hi\"     \"what's\" \"up\"    \n\n\n\n\n\n\nUsually we never want to create this “indexing vector” manually (imagine doing this for a vector of million values – impossible!). Instead, we create this indexing vector “programmatically”, based on a certain expression which produces this TRUE / FALSE indexing vector as a result (or multiple such expressions combined with &, | and ! operations), like this:\n\n# v is our vector of values\nv\n\n[1] \"hi\"     \"folks\"  \"what's\" \"up\"     \"folks\" \n\n# this is how we get the indexing vector to get only positions matching\n# some condition\nindex &lt;- v == \"up\"\n\nThis checks which values of v are equal to “three”, creating a logical TRUE / FALSE vector in the process, storing it in the variable index:\n\nindex\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\n\nUse the same principle to extract the elements of the vector v matching the value “folks”.\nNote: As always, don’t forget that you can experiment in your R Console, write bits of “throwaway” commands just to test things out.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nindex &lt;- v == \"folks\"\nindex\n\n[1] FALSE  TRUE FALSE FALSE  TRUE\n\n\nLet’s extract our matching values:\n\nv[index]\n\n[1] \"folks\" \"folks\"\n\n\n\n\n\n\nA nice trick is that summing a logical vector using sum() gives you the number of TRUE matches:\n\nindex\n\n[1] FALSE  TRUE FALSE FALSE  TRUE\n\n\n\nsum(index)\n\n[1] 2\n\n\nThis is actually why we do this indexing operation on vectors in the first place, most of the time — when we want to count how many data points match a certain criterion.\n\nThere’s another very useful operator is %in%. It tests which elements of one vector are among the elements of another vector. This is another extremely useful operation which you will be doing all the time when doing data analysis. It’s good for you to get familiar with it.\nFor instance, if we take this vector again:\n\nv &lt;- c(\"hi\", \"folks\", \"what's\", \"up\", \"folks\", \"I\", \"hope\", \"you\",\n       \"aren't\", \"(too)\", \"bored\")\n\nYou can then ask, for instance, “which elements of v are among a set of given values?” by writing this command:\n\nv %in% c(\"folks\", \"up\", \"bored\")\n\n [1] FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE\n\n\nWith our example vector v, it’s very easy to glance this with our eyes, of course. But when working with real world data, we often operate on tables with thousands or even millions of columns.\nTry the above example of using the %in% operator in your R Console to get familiar with.\n\nLet’s imagine we don’t need to test whether a given vector is a part of a set of pre-defined values, but we want to ask the opposite question: “are any of my values of interest in my data”? Let’s say your values of interest are values &lt;- c(\"hope\", \"(too)\", \"blah blah\") and your whole data is again v. How would you use the %in% operator to get a TRUE / FALSE vector for each of your values?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nAlthough the question is phrased differently, it’s still the same logical operation. So, if this is our set of values of interest:\n\nvalues &lt;- c(\"hope\", \"(too)\", \"blah blah\")\n\nOur test is then this:\n\nvalues %in% v\n\n[1]  TRUE  TRUE FALSE\n\n\nIndeed, the third element “blah blah” is not among the elements of v.\n\n\n\n\n\n\nLists\nWe went through the possibilities of indexing into vectors.\nThis section will be a repetition on the previous exercises about vectors. Don’t forget — lists are just vectors, except that they can contain values of heterogeneous types (numbers, characters, anything). As a result, everything that applies to vectors above applies also here.\nBut practice makes perfect, so let’s go through a couple of examples anyway. Run this code to create the following list variable:\n\nl &lt;- list(\"hello\", \"folks\", \"!\", 123, \"wow a number follows again\", 42)\nl\n\n[[1]]\n[1] \"hello\"\n\n[[2]]\n[1] \"folks\"\n\n[[3]]\n[1] \"!\"\n\n[[4]]\n[1] 123\n\n[[5]]\n[1] \"wow a number follows again\"\n\n[[6]]\n[1] 42\n\n\n\n1. Numerical-based indexing\nThe same applies to numerical-based indexing as what we’ve shown for vectors.\n\nExtract the second and fourth elements from l.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nl[c(2, 4)]\n\n[[1]]\n[1] \"folks\"\n\n[[2]]\n[1] 123\n\n\n\n\n\n\n\n2. Logical-based indexing\nSimilarly, you can do the same with TRUE / FALSE indexing vectors for lists as what we did with normal vectors. Rather than go through the variation of the same exercises, let’s introduce another very useful pattern related to logical-based indexing and that’s removing invalid elements.\nConsider this list (run this code in your R Console again):\n\nxyz &lt;- list(\"hello\", \"folks\", \"!\", NA, \"wow another NAs are coming\", NA, NA, \"42\")\n\nxyz\n\n[[1]]\n[1] \"hello\"\n\n[[2]]\n[1] \"folks\"\n\n[[3]]\n[1] \"!\"\n\n[[4]]\n[1] NA\n\n[[5]]\n[1] \"wow another NAs are coming\"\n\n[[6]]\n[1] NA\n\n[[7]]\n[1] NA\n\n[[8]]\n[1] \"42\"\n\n\nNotice the NA values. One operation we have to do very often (particularly in data frames, whose columns are vectors as we will see below!) is to remove those invalid elements, using the function is.na().\nThis function returns a TRUE / FALSE vector which, as you now already know, can be used for logical-based indexing!\n\nis.na(xyz)\n\n[1] FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE\n\n\nAs you’ve seen in the section on logical expressions, a very useful trick in programming is negation (using the ! operator), which flips the TRUE / FALSE states. In other words, prefixing with ! returns a vector saying which elements of the input vector are not NA:\n\n!is.na(xyz)\n\n[1]  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE\n\n\n\nUse is.na(xyz) and the negation operator ! to remove the NA elements of the list from the variable xyz!\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nxyz[!is.na(xyz)]\n\n[[1]]\n[1] \"hello\"\n\n[[2]]\n[1] \"folks\"\n\n[[3]]\n[1] \"!\"\n\n[[4]]\n[1] \"wow another NAs are coming\"\n\n[[5]]\n[1] \"42\"\n\n\nOften se used this in conjunction with assignment, to get rid of some values altogether by assigning to the same variable:\n\nxyz &lt;- xyz[!is.na(xyz)]\n\n# the variable no longer has any NA values!\nxyz\n\n[[1]]\n[1] \"hello\"\n\n[[2]]\n[1] \"folks\"\n\n[[3]]\n[1] \"!\"\n\n[[4]]\n[1] \"wow another NAs are coming\"\n\n[[5]]\n[1] \"42\"\n\n\n\n\n\n\n\n\n[] vs [[ ]] operators\nLet’s move to a topic which can be quite puzzling for a lot of people. There’s another operator useful for lists, and that’s [[ ]] (not [ ]!). Extract the third element of the list l using l[4] and l[[4]]]. What’s the difference between the results? If you’re unsure, use the already familiar typeof() function on l[3] and l[[3]] to help you.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nStrange, isn’t it? The [ ] operator seems to return a list, even though we expect the result 123?\n\nl[4]\n\n[[1]]\n[1] 123\n\ntypeof(l[4])\n\n[1] \"list\"\n\n\nOn the other hand, l[[4]] gives us just a number!\n\nl[[4]]\n\n[1] 123\n\ntypeof(l[[4]])\n\n[1] \"double\"\n\n\nI simply cannot not link to this brilliant figure, which explains this result in a very fun way:\n\nThe left picture shows our list l, the middle picture shows l[4], the right picture shows l[[4]]. Spend some time experimenting with the behavior of [ ] and [[ ]] on our list l! This will come in handy many times in your R carreer!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNamed indexing for vectors and lists\nHere’s a neat thing we can do with vectors and lists. They don’t have to contain just values themselves (which can be then extracted using integer or logical indices as we’ve done above), but those values can be assigned names too.\nConsider this vector and list (create them in your R session console):\n\nv &lt;- c(1, 2, 3, 4, 5)\nv\n\n[1] 1 2 3 4 5\n\nl &lt;- list(1, 2, 3, 4, 5)\nl\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n\nAs a recap, we can index into them in the usual manner like this:\n\n# extract the first, third, and fifth element\nv[c(1, 3, 5)]\n## [1] 1 3 5\nl[c(1, 3)]\n## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 3\n\nBut we can also name the values like this (note that the names appear in the print out you get from R in the console):\n\nv &lt;- c(first = 1, second = 2, third = 3, fourth = 4, fifth = 5)\nv\n\n first second  third fourth  fifth \n     1      2      3      4      5 \n\nl &lt;- list(first = 1, second = 2, third = 3, fourth = 4, fifth = 5)\nl\n\n$first\n[1] 1\n\n$second\n[1] 2\n\n$third\n[1] 3\n\n$fourth\n[1] 4\n\n$fifth\n[1] 5\n\n\nWhen you have a named data structure like this, you can index into it using those names and not just integer numbers, which can be very convenient. Imagine having data described not by indices but actually readable names (such as names of people, or excavation sites!). Try this:\n\nl[[\"third\"]]\n\n[1] 3\n\nl[c(\"second\", \"fifth\")]\n\n$second\n[1] 2\n\n$fifth\n[1] 5\n\n\nNote: This is exactly what data frames are, under the hood (named lists!), as we’ll see in the next section.\n\nFor named list, an alternative, more convenient means to select an element by its name is a $ operator. I’m mentioning it here in the section on lists, but it is most commonly used with data frames:\n\n# note the lack of \"double quotes\"!\nl$third\n\n[1] 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNegative indexing\nA final topic on indexing (I think you are quite convinced now how crucial role does indexing play in data science in R)—negative indices.\nConsider this vector again:\n\nv &lt;- c(\"hi\", \"folks\", \"what's\", \"up\", \"folks\")\n\nWhat happens when you index into v using the [] operator but give it a negative number between 1 and 5?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nNegative indices remove elements!\n\nv[-1]\n\n[1] \"folks\"  \"what's\" \"up\"     \"folks\" \n\n\nWhen we exclude all indices 1:5, we remove everything, oops!\n\nv[-(1:5)]\n\ncharacter(0)\n\n\n\n\n\n\nIn this context, a very useful function is length(), which gives the length of a given vector (or a list — remember, lists are vectors!). Use length() to remove the last element of v using the idea of negative indexing.\nHow would you remove both the first and last element of a vector or a list (assuming you don’t know the length beforehand, i.e., you can’t put a fixed number as the index of the last element)?\nHint: Remember that you can use a vector created using the c() function inside the [ ] indexing operator.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nv[-length(v)]\n\n[1] \"hi\"     \"folks\"  \"what's\" \"up\"    \n\n\n\n# this gives us the index of the first and last element\nc(1, length(v))\n\n[1] 1 5\n\n# then we can prefix this with the minus sign to remove them\nv[-c(1, length(v))]\n\n[1] \"folks\"  \"what's\" \"up\"",
    "crumbs": [
      "R language",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "r-bootcamp.html#exercise-6-data-frames",
    "href": "r-bootcamp.html#exercise-6-data-frames",
    "title": "R bootcamp",
    "section": "Exercise 6: Data frames",
    "text": "Exercise 6: Data frames\nEvery scientists works with tables of data, in one way or another. R provides first class support for working with tables, which are formally called “data frames”. We will be spending most of our time of this workshop learning to manipulate, filter, modify, and plot data frames, usually using example data too big to look at all at once.\nTherefore, for simplicity, and just to get started and to explain the basic fundamentals, let’s begin with something trivially easy, like this little data frame here (evaluate this in your R Console):\n\ndf &lt;- data.frame(\n  v = c(\"one\", \"two\", \"three\", \"four\", \"five\"),\n  w = c(1.0, 2.72, 3.14, 1000.1, 1e6),\n  x = c(1, 13, 42, NA, NA),\n  y = c(\"folks\", \"hello\", \"from\", \"data frame\", \"!\"),\n  z = c(TRUE, FALSE, FALSE, TRUE, TRUE)\n)\n\ndf\n\n      v          w  x          y     z\n1   one       1.00  1      folks  TRUE\n2   two       2.72 13      hello FALSE\n3 three       3.14 42       from FALSE\n4  four    1000.10 NA data frame  TRUE\n5  five 1000000.00 NA          !  TRUE\n\n\nFirst, here’s the first killer bit of information: data frames are normal lists of vectors!\n\nis.list(df)\n\n[1] TRUE\n\n\nHow is this even possible? And why is this even the case? Explaining this in full would be too much detail, even for a course which tries to go beyond “R only as a plotting tool” as I promised you in the introduction. Still, for now let’s say that R objects can store so called “attributes” (normally hidden), which — in the case of data frame objects — makes them behave as “something more than just a list”. These hidden attributes are called “classes”. A list with this class attribute called “data frame” then behaves like a data frame, i.e. a table.\n\nYou can poke into these internals but “unclassing” an object, which removes that hidden “class” attribute. Call unclass(df) in your R console and observe what result you get..\nNote: Honest admission on my part — you will never need this unclass() stuff in practice, ever. I’m really showing you to demonstrate what “data frame” actually is on a lower-level of R programming. If you’re confused, don’t worry. The fact that data frames are lists matters infinitely more than knowing exactly how is that accomplished inside R.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nThis is what a normal data frame looks like to us:\n\ndf\n\n      v          w  x          y     z\n1   one       1.00  1      folks  TRUE\n2   two       2.72 13      hello FALSE\n3 three       3.14 42       from FALSE\n4  four    1000.10 NA data frame  TRUE\n5  five 1000000.00 NA          !  TRUE\n\n\nHere is how a data frame is represented under the hood, after we remove the class attribute:\n\nunclass(df)\n\n$v\n[1] \"one\"   \"two\"   \"three\" \"four\"  \"five\" \n\n$w\n[1]       1.00       2.72       3.14    1000.10 1000000.00\n\n$x\n[1]  1 13 42 NA NA\n\n$y\n[1] \"folks\"      \"hello\"      \"from\"       \"data frame\" \"!\"         \n\n$z\n[1]  TRUE FALSE FALSE  TRUE  TRUE\n\nattr(,\"row.names\")\n[1] 1 2 3 4 5\n\n\nIt really is just a list of vectors, each column being one (named) element of that list!\n\n\n\n\nBonus exercise\nFeel free to skip unless you are a more advanced R user and are interested in the very technical internal peculiarities of R data frames and their relationships to lists.\n\n\n\n\n\n\nClick to see the bonus challenge\n\n\n\n\n\nCreate a variable df_list with a normal plain R list, which has (as named elements) the vectors of the little data frame df above. We talked about how behavior of R objects can be tweaked with setting a “class” attribute. You can annotate a plain list with the respective class attribute like this class(df_list) &lt;- \"data.frame\" and make that object a data frame. But What happens when you type df_list to the R console? Do you get a data frame print out, like you’re used to? If not, run attributes(df) on the original data frame to see what other attributes you are missing on your home-baked df_list data frame object. Then read up on the ?attr function to see how you can fix this.**\n\n\n\n\nSo, remember how we talked about “named lists” in the previous section? Yes, data frames really are just normal named lists with extra bit of behavior added to them (namely the fact that these lists are printed in a nice, readable, tabular form).\n\nSelecting columns\nQuite often we need to extract values of an entire column of a data frame. In the Exercise about indexing, you have learned about the [] operator (for vectors and lists), and also about the $ and [[]] operator (for lists). Now that you’ve learned that data frames are (on a lower level) just lists, what does it mean for wanting to extract a column from a data frame?\n\nTry to use the three indexing options to extract the column named \"z\" from your data frame df. How do the results differ depending on the indexing method chosen? Is the indexing (and its result) different to indexing a plain list? Create a variable with the list with the same vectors as are in the columns of the data frame df to answer the last question.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nWe can extract a given column with…\n\nthe $ operator (column name as a symbol), which gives us a vector:\n\n\ndf$z\n\n[1]  TRUE FALSE FALSE  TRUE  TRUE\n\n\n\nthe [ operator (column name as a string), which gives us a (single-column) data frame:\n\n\ndf[\"z\"]\n\n      z\n1  TRUE\n2 FALSE\n3 FALSE\n4  TRUE\n5  TRUE\n\n\n\nthe [[ operator (column name as a string), which gives us vector again:\n\n\ndf[[\"w\"]]\n\n[1]       1.00       2.72       3.14    1000.10 1000000.00\n\n\nLet’s create a list-version of this data frame:\n\ndf_list &lt;- as.list(df)\ndf_list\n\n$v\n[1] \"one\"   \"two\"   \"three\" \"four\"  \"five\" \n\n$w\n[1]       1.00       2.72       3.14    1000.10 1000000.00\n\n$x\n[1]  1 13 42 NA NA\n\n$y\n[1] \"folks\"      \"hello\"      \"from\"       \"data frame\" \"!\"         \n\n$z\n[1]  TRUE FALSE FALSE  TRUE  TRUE\n\n\nThe indexing results match what we get for the data frame. After all, a data frame really is just list (with some very convenient behavior, such as presenting the data in a tabular form). The only exception is the result of df_list[\"v\"] which results a data frame but only returns a list when applied on a list:\n\ndf_list$v\n\n[1] \"one\"   \"two\"   \"three\" \"four\"  \"five\" \n\ndf_list[\"v\"]\n\n$v\n[1] \"one\"   \"two\"   \"three\" \"four\"  \"five\" \n\ndf_list[[\"v\"]]\n\n[1] \"one\"   \"two\"   \"three\" \"four\"  \"five\" \n\n\n\n\n\n\nThe tidyverse approach\nIn the chapter on tidyverse, we will learn much more powerful and easier tools to do these types of data-frame operations, particularly the select() function which selects specific columns of a table. That said, even when you use tidyverse exclusively, you will still encounter code in the wild which uses this base R way of doing things. Additionally, for certain trivial actions, doing “the base R thing” is just quicker to types. This is why knowing the basics of $, [ ], and [[ ]] will always be useful. I use them every day.\n\n\n\n\nSelecting rows (“filtering”)\nOf course, we often need to refer not just to specific columns of data frames (which we do with the $, [ ], and [[ ]] operators), but also to given rows. Let’s consider our data frame again:\n\ndf\n\n      v          w  x          y     z\n1   one       1.00  1      folks  TRUE\n2   two       2.72 13      hello FALSE\n3 three       3.14 42       from FALSE\n4  four    1000.10 NA data frame  TRUE\n5  five 1000000.00 NA          !  TRUE\n\n\nIn the section on indexing into vectors and lists above, we learned primarily about two means of indexing into vectors. Let’s revisit them in the context of data frames:\n\n1. Integer-based indexing\n\nWhat happens when you use the [1:3] index into the df data frame, just as you would do by extracting the first three elements of a vector?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nSomewhat curiously, you get the first three columns, not rows!\n\ndf[1:3]\n\n      v          w  x\n1   one       1.00  1\n2   two       2.72 13\n3 three       3.14 42\n4  four    1000.10 NA\n5  five 1000000.00 NA\n\n\n\n\n\nWhen indexing into a data frame, you need to distinguish the dimension along which you’re indexing: either a row, or a column dimension. Just like in referring to a cell coordinate in Excel, for example.\nThe way you do this for data frames in R is to separate the dimensions into which you’re indexing with a comma in this way: df[row indices,  column names].\n\nExtract the first three rows (1:3) of the data frame df.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nExtract the first three rows of df:\n\ndf[1:3, ]\n\n      v    w  x     y     z\n1   one 1.00  1 folks  TRUE\n2   two 2.72 13 hello FALSE\n3 three 3.14 42  from FALSE\n\n\n\n\n\n\nThen select a subset of the df data frame to only show the first and fourth row and columns \"x\" and \"z\".\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nExtract the rows number 1 and 4 for columns “x” and “z”:\n\ndf[c(1, 4), c(\"x\", \"z\")]\n\n   x    z\n1  1 TRUE\n4 NA TRUE\n\n\nOf course, the actual indexing dimensions can be (and often is) specified in variables. For instance, we often have code which firsts computes the indexes, and then we access them into the data frames. The equivalent of this here would be:\n\nrow_indices &lt;- 1:3\n\ndf[row_indices, ]\n\n      v    w  x     y     z\n1   one 1.00  1 folks  TRUE\n2   two 2.72 13 hello FALSE\n3 three 3.14 42  from FALSE\n\n\nExtract the rows number 1 and 4 for columns “x” and “z”:\n\nrow_indices &lt;- c(1, 4)\ncol_indices &lt;- c(\"x\", \"z\")\n\ndf[row_indices, col_indices]\n\n   x    z\n1  1 TRUE\n4 NA TRUE\n\n\n\n\n\n\n\n2. Logical-based indexing\nSimilarly to indexing into vectors, you can also specify which rows should be extracted “at once”, using a single logical vector (you can also do this for columns but I honestly don’t remember the last time I had to do this).\n\nThe most frequent use for this is to select all rows of a data frame for which a given column (or multiple columns) carry a certain value.\nSelect only those rows of df for which the column “y” has a value “hello”:\nHint: You can get the required TRUE / FALSE indexing vector with df$y == \"hello\". Again, if you’re confused, first evaluate this expression in your R console, then try to use it to filter the rows of the data frame df to get your answer.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nLet’s first use a vectorized comparison to get a TRUE / FALSE vector indicating which values of the “v” column contain the string “hello”. Remember, that if you take a vector (of arbitrary length) and compare it to some value, you will get a TRUE / FALSE vector of the same length:\n\n# this is what the column (vector, really) contains\ndf$y\n\n[1] \"folks\"      \"hello\"      \"from\"       \"data frame\" \"!\"         \n\n\n\n# this is how we can find out, which element(s) of the vector match\ndf$y == \"hello\"\n\n[1] FALSE  TRUE FALSE FALSE FALSE\n\n\n\n# let's save the result to a new variable\nrow_indices &lt;- df$y == \"hello\"\nrow_indices\n\n[1] FALSE  TRUE FALSE FALSE FALSE\n\n\nNow we can use this vector as a row index into our data frame (don’t forget the comma ,, without which you’d be indexing into the column-dimension, not the row-dimension!).\n\ndf[row_indices, ]\n\n    v    w  x     y     z\n2 two 2.72 13 hello FALSE\n\n\nOf course, you can also both filter (remember this word) for a subset of rows and also, at the same time, select (remember this word too) a subset of columns at the same time:\n\ndf[row_indices, c(\"v\", \"y\", \"z\")]\n\n    v     y     z\n2 two hello FALSE\n\n\n\n\n\n\nNow, instead of filtering rows where column y matches “hello”, filter for rows of the df data frame where w column is less than 1000.\nHint: Same idea as above — first get the TRUE / FALSE vector indicating which rows of the data frame (alternatively speaking, which elements of the column/vector) pass the filtering criterion, then use that vector as a row index into the data frame.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nWe can again store the filtered rows in a separate variable, and then use that variable to index into the data frame:\n\nrow_indices &lt;- df$w &lt; 1000\nrow_indices\n\n[1]  TRUE  TRUE  TRUE FALSE FALSE\n\ndf[row_indices, ]\n\n      v    w  x     y     z\n1   one 1.00  1 folks  TRUE\n2   two 2.72 13 hello FALSE\n3 three 3.14 42  from FALSE\n\n\nOften, we want to be more consise and do everything in one go:\n\ndf[df$w &lt; 1000, ]\n\n      v    w  x     y     z\n1   one 1.00  1 folks  TRUE\n2   two 2.72 13 hello FALSE\n3 three 3.14 42  from FALSE\n\n\n\n\n\n\nRemember how we used to filter out elements of a vector using the !is.na(...) operation? You can see that df contains some NA values in the x column. Use the fact that you can filter rows of a data frame using logical-based vectors (as demonstrated above) to filter out rows of df at which the x column contains NA values.\nHint: You can get indices of the rows of df we you want to retain with !is.na(df$x).\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nThis gives us indices of the rows we want to keep:\n\n!is.na(df$x)\n\n[1]  TRUE  TRUE  TRUE FALSE FALSE\n\n\nThis is how we can filter out unwanted rows:\n\ndf[!is.na(df$x), ]\n\n      v    w  x     y     z\n1   one 1.00  1 folks  TRUE\n2   two 2.72 13 hello FALSE\n3 three 3.14 42  from FALSE\n\n\n\n\n\n\n\nCreating, modifying, and deleting columns\nThe $ and [ ] operators can be used to create, modify, and delete columns.\n\nCreating a column\nThe general pattern to create a new column is this:\n\ndf$NAME_OF_THE_NEW_COLUMN &lt;- VECTOR_OF_VALUES_TO_ASSIGN_AS_THE_NEW_COLUMN\n\nor this:\n\ndf[\"NAME_OF_THE_NEW_COLUMN\"] &lt;- VECTOR_OF_VALUES_TO_ASSIGN_AS_THE_NEW_COLUMN\n\n\nFor instance, the paste() function in R can be used to combine a pair of values into one. Try running paste(df$v, df$y) to see what the result of this operation is.\nCreate a new column of df called \"new_col\" and assign to it the result of paste(df$v, df$y). Try to guess what will be this column’s values before you write and evaluate the code for your answer!\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\ndf[\"new_col\"] &lt;- paste(df$v, df$y)\n\n# new column appears!\ndf\n\n      v          w  x          y     z         new_col\n1   one       1.00  1      folks  TRUE       one folks\n2   two       2.72 13      hello FALSE       two hello\n3 three       3.14 42       from FALSE      three from\n4  four    1000.10 NA data frame  TRUE four data frame\n5  five 1000000.00 NA          !  TRUE          five !\n\n\n\n\n\n\n\nModifying a column\nThe above pattern doesn’t only apply to creating new columns. You can also update and change values of existing columns using the same technique!\nOur data frame df has the following values in the column z:\n\ndf$z\n\n[1]  TRUE FALSE FALSE  TRUE  TRUE\n\n\nInvert the logical values of this column using the ! negation operator. In other words, the values of the modified column should be the result of !df$z.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nThis is how we can invert the values of the logical vector:\n\n# original vector/column:\ndf$z\n\n[1]  TRUE FALSE FALSE  TRUE  TRUE\n\n# inverted values:\n!df$z\n\n[1] FALSE  TRUE  TRUE FALSE FALSE\n\n\nModifying an existing column is the same as creating a column of the same name:\n\ndf$z &lt;- !df$z\n\n# result after this modification\ndf\n\n      v          w  x          y     z         new_col\n1   one       1.00  1      folks FALSE       one folks\n2   two       2.72 13      hello  TRUE       two hello\n3 three       3.14 42       from  TRUE      three from\n4  four    1000.10 NA data frame FALSE four data frame\n5  five 1000000.00 NA          ! FALSE          five !\n\n\n\n\n\n\n\nRemoving a column\nWhen we want to remove a column from a data frame (for instance, we only used it to store some temporary result in a script), we actually do the same thing, except we assign to it the value NULL.\nRemove the column new_col using this technique.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nBefore:\n\ndf\n\n      v          w  x          y     z         new_col\n1   one       1.00  1      folks FALSE       one folks\n2   two       2.72 13      hello  TRUE       two hello\n3 three       3.14 42       from  TRUE      three from\n4  four    1000.10 NA data frame FALSE four data frame\n5  five 1000000.00 NA          ! FALSE          five !\n\n\nAfter:\n\ndf$new_col &lt;- NULL\n\n# and the column is gone\ndf\n\n      v          w  x          y     z\n1   one       1.00  1      folks FALSE\n2   two       2.72 13      hello  TRUE\n3 three       3.14 42       from  TRUE\n4  four    1000.10 NA data frame FALSE\n5  five 1000000.00 NA          ! FALSE\n\n\n\n\n\n\n\n\n“Improper” column names\nMost column names you will be using in your own script will (well, should!) follow the same rules as apply for variable names – they can’t start with a number, have to compose of alphanumeric characters, and can’t contain any other characters except for underscores (and occasionally dots). To quote from the venerable R language reference:\n\nIdentifiers consist of a sequence of letters, digits, the period (‘.’) and the underscore. They must not start with a digit or an underscore, or with a period followed by a digit.\n\nFor instance, these are examples of proper identifiers which can serve as variable names, column names and (later) function names:\n\nvariable1\na_longer_var_42\nanotherVariableName\n\nUnfortunately, when you encounter data in the wild, especially in tables you get from other people or download as supplementary information from the internet, they are rarely this perfect. Here’s a little example of such data frame (evaluate this code to have it in your R session as weird_df):\n\nweird_df &lt;- data.frame(\n  `v with spaces` = c(\"one\", \"two\", \"three\", \"four\", \"five\"),\n  w = c(1.0, 2.72, 3.14, 1000.1, 1e6),\n  `y with % sign` = c(\"folks\", \"hello\", \"from\", \"data frame\", \"!\")\n)\nnames(weird_df) &lt;- c(\"v with spaces\", \"w\", \"y with % sign\")\n\n\nweird_df\n\n  v with spaces          w y with % sign\n1           one       1.00         folks\n2           two       2.72         hello\n3         three       3.14          from\n4          four    1000.10    data frame\n5          five 1000000.00             !\n\n\nIf you look closely, you see that some columns have spaces \" \" and also strange characters % which are not allowed. Which of the $, [ ] and [[ ]] operators can you use to extract columns named \"v with spaces\" and \"y with % sign\" columns as vectors?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n[ ] and [[ ]] work just as before, because they accept a string by default anyway, so spaces and other characters are not a problem:\n\nweird_df[\"v with spaces\"]\n\n  v with spaces\n1           one\n2           two\n3         three\n4          four\n5          five\n\nweird_df[[\"y with % sign\"]]\n\n[1] \"folks\"      \"hello\"      \"from\"       \"data frame\" \"!\"         \n\n\nThe $ operator needs bit more work. When you encounter an “improper” column name in a data frame, you have to enclose the whole “symbol” or “identifier” in “back ticks” like this:\n\nweird_df$`y with % sign`\n\n[1] \"folks\"      \"hello\"      \"from\"       \"data frame\" \"!\"         \n\n\nThis is super useful when working with tabular data you get from someone else, especially if they prepared it in Excel. But you should never create data frames with these weird column names yourself. Always use names that would be appropriate as normal standard R identifiers on their own (just alphanumeric symbols or underscores).\n\n\n\n\n\n\nThe tidyverse approach\nSimilarly to previous section on column selection, there’s a much more convenient and faster-to-type way of doing filtering, using the tidyverse function filter(). Still, as with the column selection, sometimes doing the quick and easy thing is just more convenient. The minimum on filtering rows of data frames introduced in this section will be enough for you, even in the long run!",
    "crumbs": [
      "R language",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "r-bootcamp.html#exercise-7-inspecting-column-types",
    "href": "r-bootcamp.html#exercise-7-inspecting-column-types",
    "title": "R bootcamp",
    "section": "Exercise 7: Inspecting column types",
    "text": "Exercise 7: Inspecting column types\nLet’s go back to our example data frame. Create it in your R session as the variable df1 by running this code:\n\ndf1 &lt;- data.frame(\n  w = c(1.0, 2.72, 3.14),\n  x = c(1, 13, 42),\n  y = c(\"hello\", \"folks\", \"!\"),\n  z = c(TRUE, FALSE, FALSE)\n)\n\ndf1\n\n     w  x     y     z\n1 1.00  1 hello  TRUE\n2 2.72 13 folks FALSE\n3 3.14 42     ! FALSE\n\n\n\nUse the function str() and by calling str(df1), inspect the types of columns in the table.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nstr() gives you a nice overview of the data types in each column:\n\nstr(df1)\n\n'data.frame':   3 obs. of  4 variables:\n $ w: num  1 2.72 3.14\n $ x: num  1 13 42\n $ y: chr  \"hello\" \"folks\" \"!\"\n $ z: logi  TRUE FALSE FALSE\n\n\n\n\n\n\nSometimes (usually when we read data from disk, like from another software), a data point sneaks in which makes a column apparently non numeric. Consider this new table called df2 (evaluate this in your R session too):\n\ndf2 &lt;- data.frame(\n  w = c(1.0, 2.72, 3.14),\n  x = c(1, \"13\", 42.13),\n  y = c(\"hello\", \"folks\", \"!\"),\n  z = c(TRUE, FALSE, FALSE)\n)\n\ndf2\n\n     w     x     y     z\n1 1.00     1 hello  TRUE\n2 2.72    13 folks FALSE\n3 3.14 42.13     ! FALSE\n\n\nJust by looking at this, the table looks the same as df1 above. In particular, the columns w and x contain numbers. But is it really true? You know that an R vector can be only of one type, and that if this isn’t true, a coercion is applied to force heterogeneous types to be of one type. Take a look at the column x above. Use str() to verify where the problem is with the data frame df2 with regards to what our expectation is.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nFor some reason, the whole column x has a data type “character”. There’s one sneaky “number” encoded as “13” and because a vector can have values of only one type (and because data frame columns are vectors), the entire column x is forced to be a text data type “character”:\n\nstr(df2)\n\n'data.frame':   3 obs. of  4 variables:\n $ w: num  1 2.72 3.14\n $ x: chr  \"1\" \"13\" \"42.13\"\n $ y: chr  \"hello\" \"folks\" \"!\"\n $ z: logi  TRUE FALSE FALSE\n\n\n\n\n\n\nA useful way of fixing these kinds of data type mismatch problems are conversion functions, primarily as.numeric(), as.integer(), and as.character().\nWhich one of these conversion functions would you use to fix the issue with a (what we want to be a numeric) column x having incorrectly the data type “character”?\nHint: Remember that just like you can create a column with df[\"NEW_COLUMN\"] &lt;- VECTOR_OF_VALUES or df$NEW_COLUMN &lt;- VECTOR_OF_VALUES, you can also modify an existing column, like we did above!\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nHere are the types of our mis-specified data frame df2:\n\nstr(df2)\n\n'data.frame':   3 obs. of  4 variables:\n $ w: num  1 2.72 3.14\n $ x: chr  \"1\" \"13\" \"42.13\"\n $ y: chr  \"hello\" \"folks\" \"!\"\n $ z: logi  TRUE FALSE FALSE\n\n\nWe can fix the improper data types of the column x like this:\n\ndf2$x &lt;- as.numeric(df2$x)\n\nNotice the data type of the column x after this modification:\n\nstr(df2)\n\n'data.frame':   3 obs. of  4 variables:\n $ w: num  1 2.72 3.14\n $ x: num  1 13 42.1\n $ y: chr  \"hello\" \"folks\" \"!\"\n $ z: logi  TRUE FALSE FALSE\n\n\n\n\n\n\nCreate a new column of df2 called abc which will contain a transformation of the TRUE / FALSE column z into integers 1 or 0. Use as.integer(z) to do this (run this function first in your R console to get and idea of what it does, if you need a reminder from earlier).\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\ndf$abc &lt;- as.integer(df$z)\n\nAnd here’s the table with the new column abc:\n\ndf\n\n      v          w  x          y     z abc\n1   one       1.00  1      folks FALSE   0\n2   two       2.72 13      hello  TRUE   1\n3 three       3.14 42       from  TRUE   1\n4  four    1000.10 NA data frame FALSE   0\n5  five 1000000.00 NA          ! FALSE   0",
    "crumbs": [
      "R language",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "r-bootcamp.html#exercise-8-base-r-plotting",
    "href": "r-bootcamp.html#exercise-8-base-r-plotting",
    "title": "R bootcamp",
    "section": "Exercise 8: Base R plotting",
    "text": "Exercise 8: Base R plotting\nAs a final short section, it’s worth pointing out some very basic base R plotting functions. We won’t be getting into detail because tidyverse provides a much more powerful and more convenient set of functionality for visualizing data implemented in the R package ggplot2 (a topic for the next day).\nStill, base R is often convenient for quick troubleshooting or quick plotting of data at least in the initial phases of data exploration.\nSo far we’ve worked with a really oversimplified data frame. For more interesting demonstration, R bundles with a realistic data frame of penguin data. Use a helper function head() to take at the first couple of rows:\nYou might first have to install the packages palmerpenguins:\n\n#install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\n\n\nAttaching package: 'palmerpenguins'\n\n\nThe following objects are masked from 'package:datasets':\n\n    penguins, penguins_raw\n\n\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\nBase R histograms\nOne of the most basic types of visualizations is a histogram. This is provided by a built-in function hist(), which simply accepts a numeric vector as its only parameter. You know you can get all values from a column of a data frame using the $ or [[ ]] indexing operators. Use the function hist() to plot a histogram of the body mass of the entire penguins data set (take a look at the overview of the table above to get the column name).\nHow can you adjust the number of bins of the histogram? Check out the ?hist help page to find the answer.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nhist(penguins$body_mass_g)\n\n\n\n\n\n\n\n\nSometimes it is convenient to adjust the bin width:\n\nhist(penguins$body_mass_g, breaks = 50)\n\n\n\n\n\n\n\n\n\n\n\n\nThe dataset also contains the measure of flipper lengths. Is there a relationship between this measure and body mass? Use the function plot() which plots a scatter plot of two numeric vectors (both given as x and y parameters of the function — look at ?plot again if needed!) to visualize the relationship between those two variables / columns: penguins$flipper_length_mm against penguins$body_mass_g. Is there an indication of a relationship between the two metrics?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nHere is the plain scatter plot — there does seem to be a linear relationship indeed, which makes sense! The larger the penguin (body mass) the longer its flippers!\n\nplot(penguins$flipper_length_mm, penguins$body_mass_g)\n\n\n\n\n\n\n\n\nJust for fun, we can also overlay a linear fit (first computed with the lm() function, then visualized as a red dashed line using another built-in function abline():\n\nplot(penguins$flipper_length_mm, penguins$body_mass_g)\n\nlm_fit &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins)\nabline(lm_fit, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nWe can also see that we have data for three different species of penguins. We can therefore partition the visualization for each species individually (again, you can always find additional options and examples in help pages such as ?plot):\n\nplot(penguins$flipper_length_mm, penguins$body_mass_g, col = penguins$species)\n\n\n\n\n\n\n\n\n\n\n\n\nBase R plotting is very convenient for quick and dirty data summaries, particularly immediately after reading unknown data. However, for anything more complex (and anything more pretty), ggplot2 is unbeatable. For this reason, there’s no point in digging into base R graphics any further.\nWe will be looking at ggplot2 visualization in the session on tidyverse but as a sneakpeak, you can take a look at the beautiful figures you’ll learn how to make later.\n\n\n\n\n\n\nSee the sneakpeak\n\n\n\n\n\nLook how comparatively little code we need to make beautiful informative figures (much prettier than those made with base R functions like we did just above) which immediately tell a clear story! Stay tuned for later! :)\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n\nCode\nggplot(penguins) +\n  geom_histogram(aes(flipper_length_mm, fill = species), alpha = 0.5) +\n  theme_minimal() +\n  ggtitle(\"Distribution of flipper lengths across species\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(penguins, aes(flipper_length_mm, body_mass_g, shape = sex, color = species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  facet_wrap(~ species) +\n  theme_minimal() +\n  ggtitle(\"Body mass as a function of flipper length across penguin species\")\n\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "R language",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "r-bootcamp.html#exercise-9-working-with-directories-and-files",
    "href": "r-bootcamp.html#exercise-9-working-with-directories-and-files",
    "title": "R bootcamp",
    "section": "Exercise 9: Working with directories and files",
    "text": "Exercise 9: Working with directories and files\nWhen we move on to the reproducibility session and begin discussing the benefits of writing self-contained, single-purpose R scripts as components of reproducible pipelines, it will come in handy for our scripts to do more than just munge on tables and create plots. Sometimes our scripts will create files. And those files need directories to be put in.\nYes, we can always do this manually in a file explorer of some sorts, but that is not automated and, as a result, it’s not reproducible.\n\nOrienting ourselves\nYour R session (where your script lives and where your R console exists) always has an “active directory” in which it is assumed to operate, unless this is changed.\nYou can always get this by running getwd() (“Get Working Directory”). Where is your R session’s working directory right now?\nYou can change this directory by executing the command setwd() (“Set Working Directory”).\nBoth can be useful in automated workflow scripts, which we will get to in later sessions. For now just keep in mind that R gives you a lot of possibilities to work with your filesystem.\n\n\nCreating a directory\nYou can create a directory using a function dir.create(). Try it by creating a directory somewhere on your file system, named “testing-directory”. Look up ?dir.create help page to see useful options.\n\nWhat happens when you call a dir.create(\"testing-directory\") twice? Do you get an error or a warning? Is that warning necessarily a problem, or is it more that it’s annoying? (Imagine having a script which does this repeatedly, perhaps because it can be run repeatedly in a loop). How would you silence this problem? Again, look up the help under ?dir.create.\n\n\nListing files\nOften you have many different tables stored across multiple files. You then need to read all those tables, but calling a function like read_tsv() (which reads a data frame from disk — we’ll use it a lot in later sections!) a hundred times, manually, is impossible.\nA useful trick is to get a listing of files in a particular directory, using the function list.files().\nUse list.files() to list all files in your ~/Documents directory. Again, look up help for ?list.files to see how you should run this function. Experiment in your R console! Do you get full paths to each file? Or just filenames? How would you get full paths?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nThis gives only filenames:\n\nlist.files(path = \"~/Documents\")\n\n [1] \"archive\"        \"competence.key\" \"games\"          \"Garageband\"    \n [5] \"latexCV\"        \"postdoc\"        \"Renoise\"        \"slides\"        \n [9] \"talks\"          \"WeAreKU\"        \"whoami_files\"   \"whoami.html\"   \n\n\nFor reading from a script from any location, full.names = TRUE is important:\n\nlist.files(path = \"~/Documents\", full.names = TRUE)\n\n [1] \"/Users/mp/Documents/archive\"        \"/Users/mp/Documents/competence.key\"\n [3] \"/Users/mp/Documents/games\"          \"/Users/mp/Documents/Garageband\"    \n [5] \"/Users/mp/Documents/latexCV\"        \"/Users/mp/Documents/postdoc\"       \n [7] \"/Users/mp/Documents/Renoise\"        \"/Users/mp/Documents/slides\"        \n [9] \"/Users/mp/Documents/talks\"          \"/Users/mp/Documents/WeAreKU\"       \n[11] \"/Users/mp/Documents/whoami_files\"   \"/Users/mp/Documents/whoami.html\"   \n\n\n\n\n\nUsing list.files() is particularly helpful in conjunction with the concept of looping introduced towards the end of this Bootcamp session.\nDon’t worry too much about this technical stuff. When the moment comes that you’ll need it in your project, just refer back to this!\n\n\nTesting the existence of a directory or a file\nIn other setting, we need our script to test whether a particular directory (or, more often, a file) already exists. This can be done with the functions dir.exists() or file.exists().\nPick an arbitrary file you got in your ~/Documents listing. Then try file.exists() on that file. Then add a random string to that path to your file and run file.exists() again.\nThese functions are also super useful for building more comprehensive pipelines! Imagine a situation in which an output file with your results already exists, and you don’t want to overwrite it (but you want to run your script as a whole anyway).",
    "crumbs": [
      "R language",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "r-bootcamp.html#exercise-10-functions",
    "href": "r-bootcamp.html#exercise-10-functions",
    "title": "R bootcamp",
    "section": "Exercise 10: Functions",
    "text": "Exercise 10: Functions\nThe motivation for this Exercise could be summarized by an ancient principle of computer programming known as “Don’t repeat yourself” (DRY) which is present when:\n\n“[…] a modification of any single element of a system does not require a change in other logically unrelated elements.”.\n\nLet’s demonstrate this idea in practice on the most common way this is applied in practice, which is encapsulating (repeatedly used) code in self-contained functions.\nLet’s say you have the following numeric vector (these could be base qualities, genotype qualities, \\(f\\)-statistics, sequencing coverage, anything normally probably stored in a data-frame column, but here represented as a plain numeric vector):\n\nvec &lt;- c(0.32, 0.78, 0.68, 0.28, 1.96, 0.21, 0.07, 1.01, 0.06, 0.74,\n         0.37, 0.6, 0.08, 1.81, 0.65, 1.23, 1.28, 0.11, 1.74,  1.68)\n\nWith numeric vectors, we often need to compute some summary statistics (mean, median, quartile, minimum, maximum, etc.). What’s more, in a given project, we often have to do this computation multiple times in a number of places.\nNote: In R, we have a very useful built-in function summary(), which does exactly that. But let’s ignore this for the moment, for learning purposes.\nHere is how you can compute those summary statistics individually:\n\nmin(vec)\n## [1] 0.06\n\n# first quartile (a value which is higher than the bottom 25% of the data)\nquantile(vec, probs = 0.25)\n##    25% \n## 0.2625\n\nmedian(vec)\n## [1] 0.665\n\nmean(vec)\n## [1] 0.783\n\n# third quartile (a value which is higher than the bottom 75% of the data)\nquantile(vec, probs = 0.75)\n##    75% \n## 1.2425\n\nmax(vec)\n## [1] 1.96\n\nNow, you can imagine that you have many more of such vectors (results for different sequenced samples, different computed population genetic metrics, etc.). Having to type out all of these commands for every single one of those vectors would very quickly get extremely tiresome. Worse still, when we would do this, we would certainly resort to copy-pasting, which is guaranteed to lead to errors.\nCustom functions are the best solution to this problem and a key thing most scientists are not aware of which can tremendously improve reproducibility.\n\n\nHow are functions defined\nA custom function in R is defined by the following patterns. Note that it looks like a variable definition, except the variable (later refered to as a “function name”, here fun_name) is assigned &lt;- a more complex code structure known as “function” body, with the code enclosed in { and } block delimiters.\n\nCustom functions can take several forms, most importantly:\n\nThe function can be without parameters, defined like this:\n\n\nfun_name &lt;- function() {\n  # here goes your code\n}\n\nAnd then called like this (without parameters, and not returning anything):\n\nfun_name()\n\n\n\nOr it can have some parameters, which are variables that are passed to that function (and can be used inside the function like any other variable — here just parameter variables par1, par2):\n\n\nfun_name &lt;- function(par1, par2) {\n  # here goes your code\n}\n\nAnd then called like this (still not returning anything):\n\n# variable1 will be assigned to par1 inside the function\n# variable2 will be assigned to par2 inside the function\nfun_name(variable1, variable2) \n\n\n\nA function can also return a result, represented by a variable created inside that function, and returned by the return() statement typically at the end of the function body (but not always):\n\n\nfun_name &lt;- function(par1, par2) {\n  # here goes your code\n  \n  # the code could create a variable like this:\n  result &lt;- # however you would compute the result\n  \n  return(result)\n}\n\nAnd then used like this:\n\n# the returned value from the function is assigned to a variable\n# result_from_function which we can work with later\nresult_from_function &lt;- fun_name(par1, par2)\n\n\n\nSpoilers ahead for more advanced researchers\nIf you’re a more advanced researcher, in the later parts on practical reproducibility techniques, extracting your repetitive code into self-contained, well-documented functions, and putting collections of such functions into independent “utility module” scripts which can be sourced into data analysis scripts using the source() command is the single best, most important thing you can do do increase the robustness and reproducibility of your own projects!\nWhen you find yourself with free time, use these lessons to improve your projects by doing this!\n\nWrite a custom function called my_summary, which will accept a single input named values (a vector of numbers like vec above), and return a list which combines all the six summary statistics together. Name the elements of that list as \"min\", \"quartile_1\", \"median\", \"mean\", \"quartile_3\", and \"max\".\nHint: You can use the following “template”, and just fill in the required bits of code within the “function body” represented by { and }:\n\n# copy this template into your script and add relevant bits of code to\n# turn it into a full-blown R function\n\nmy_summary &lt;- function(values) {\n  # compute\n  # your\n  # summary\n  # statistics\n  # like\n  # you\n  # did\n  # above\n  \n  result &lt;- list(... combine your statistics into a named list as instructed ...)\n  \n  # then return that result\n  return(result)\n}\n\nThen test that your function works correctly by executing it like this in your R console:\n\nmy_summary(vec)\n\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nHere is how we could write the function:\n\nmy_summary &lt;- function(values) {\n  a &lt;- min(vec)\n  b &lt;- quantile(vec, probs = 0.25)\n  c &lt;- median(vec)\n  d &lt;- mean(vec)\n  e &lt;- quantile(vec, probs = 0.75)\n  f &lt;- max(vec)\n  \n  result &lt;- list(min = a, quartile_1 = b, median = c, mean = d, quartile_3 = e, max = f)\n  \n  return(result)\n}\n\nAlthough I would probably prefer to write it a bit more tersely like this:\n\nmy_summary &lt;- function(values) {\n  result &lt;- list(\n    min = min(vec),\n    quartile_1 = quantile(vec, probs = 0.25),\n    median = median(vec),\n    mean = mean(vec),\n    quartile_3 = quantile(vec, probs = 0.75),\n    max = max(vec)\n  )\n  \n  return(result)\n}\n\nWe can then run our function like this:\n\nmy_summary(vec)\n\n$min\n[1] 0.06\n\n$quartile_1\n   25% \n0.2625 \n\n$median\n[1] 0.665\n\n$mean\n[1] 0.783\n\n$quartile_3\n   75% \n1.2425 \n\n$max\n[1] 1.96\n\n\n\n\n\n\nYes, we had to write the code anyway, we even had to do the extra work of wrapping it inside other code (the function body, name the one input argument values, which could be multiple arguments for more complex function). So, one could argue that we didn’t actually save any time. However, that code is now “encapsulated” in a fully self-contained form and can be called repeatably, without any copy-pasting.\nIn other words, if you now create these three vectors of numeric values:\n\nvec1 &lt;- runif(10)\nvec2 &lt;- runif(10)\nvec3 &lt;- runif(10)\n\nYou can now compute our summary statistics by calling our function my_summary() on these vectors, without any code repetition:\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nmy_summary(vec1)\n\n$min\n[1] 0.06\n\n$quartile_1\n   25% \n0.2625 \n\n$median\n[1] 0.665\n\n$mean\n[1] 0.783\n\n$quartile_3\n   75% \n1.2425 \n\n$max\n[1] 1.96\n\nmy_summary(vec2)\n\n$min\n[1] 0.06\n\n$quartile_1\n   25% \n0.2625 \n\n$median\n[1] 0.665\n\n$mean\n[1] 0.783\n\n$quartile_3\n   75% \n1.2425 \n\n$max\n[1] 1.96\n\nmy_summary(vec3)\n\n$min\n[1] 0.06\n\n$quartile_1\n   25% \n0.2625 \n\n$median\n[1] 0.665\n\n$mean\n[1] 0.783\n\n$quartile_3\n   75% \n1.2425 \n\n$max\n[1] 1.96\n\n\nAnd, surprise! This is what the incredibly useful built-in function summary() provided with every R installation does!\n\nsummary(vec1)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1933  0.3224  0.7321  0.6401  0.9106  0.9834 \n\nsummary(vec2)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.001587 0.350676 0.650365 0.608091 0.891698 0.979559 \n\nsummary(vec3)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0441  0.1117  0.2773  0.3866  0.6929  0.8998 \n\n\n\n\n\n\nThe punchline is this: if we ever need to modify how are summary statistics are computed, we only have to make a single change in the function code instead of having to modify multiple copies of the code in multiple locations in our project. This is what “Don’t Repeat Yoursef (DRY)” means!\n\nIn the penguin section above, you used hist(penguins$body_mass) to plot a histogram of the penguins’ body mass. Write a custom function penguin_hist() which will accept two arguments: 1. the name of the column in the penguins data frame, and 2. the number of histogram breakpoints to use as the breaks = arguments in a hist(&lt;vector&gt;, breaks = ...) call.\nHint: Remember that you can extract a column of a data frame as a vector not just using a symbolic identifier with the $ operator but also as a string name with the [[]] operator.\nHint: A useful new argument of the hist() function is main =. You can specify this as a string and the contents of this argument will be plotted as the figure’s title.\nIf you need further help, feel free to use this template and fill the insides of the function body with your code:\n\npenguin_hist &lt;- function(df, column, breaks) {\n  # ... put your hist() code here\n}\n\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nWe can extract a column column of any data frame df with df[[column]], therefore, we only need to do this:\n\npenguin_hist &lt;- function(df, column, breaks) {\n  title &lt;- paste(\"Histogram of the column\", column)\n  hist(df[[column]], breaks = breaks, main = title)\n}\n\nWe can then use our fancy new function like this:\n\npenguin_hist(penguins, \"bill_length_mm\", breaks = 50)\n\n\n\n\n\n\n\n\n\npenguin_hist(penguins, \"bill_depth_mm\", breaks = 50)\n\n\n\n\n\n\n\n\n\npenguin_hist(penguins, \"flipper_length_mm\", breaks = 50)\n\n\n\n\n\n\n\n\n\npenguin_hist(penguins, \"body_mass_g\", breaks = 50)",
    "crumbs": [
      "R language",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "r-bootcamp.html#exercise-11-conditional-expressions",
    "href": "r-bootcamp.html#exercise-11-conditional-expressions",
    "title": "R bootcamp",
    "section": "Exercise 11: Conditional expressions",
    "text": "Exercise 11: Conditional expressions\nOftentimes, especially when writing custom code, we need to make automated decisions whether something should or shouldn’t happen given a certain value. We can do this using the if expression which has the following form:\n\nif (&lt;condition resulting in TRUE or FALSE&gt;) {\n  ... code which should be executed if condition is TRUE...\n}\n\nAn extension of this is the if-else expression, taking the following form:\n\nif (&lt;condition resulting in TRUE or FALSE&gt;) {\n  ... code which should be executed if condition is TRUE...\n} else {\n  ... code which should be executed if condition is FALSE...\n}\n\n\nGo back to your new penguin_hist() function and add an if expression which will make sure that breaks is greater than 0. In other words, if breaks &lt; 1 (the condition you will be testing against), execute the command stop(\"Incorrect breaks argument given!\").\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nHere is our new modified function:\n\npenguin_hist &lt;- function(df, column, breaks) {\n  if (breaks &lt; 1) {\n    stop(\"Incorrect breaks argument given!\", call. = FALSE) # call. = FALSE makes errors less verbose\n  }\n\n  title &lt;- paste(\"Histogram of the column\", column)\n  hist(df[[column]], breaks = breaks, main = title)\n}\n\nAnd here is how our new modification guards against incorrect use of our function:\n\npenguin_hist(df, \"bill_len\", breaks = -123)\n\nError: Incorrect breaks argument given!\n\n\n\n\n\n\nThis was just a little sneak peak. As you get more comfortable with programming, if and if-else expressions like this will be very useful to make your code more robust. Whenever you’re coding, catching errors as soon as they happen is extremely important!",
    "crumbs": [
      "R language",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "r-bootcamp.html#exercise-12-iteration-and-loops",
    "href": "r-bootcamp.html#exercise-12-iteration-and-loops",
    "title": "R bootcamp",
    "section": "Exercise 12: Iteration and loops",
    "text": "Exercise 12: Iteration and loops\nFunctions help us take pieces of code and generalize them to reduce the amount of code needed to do similar things, repeatedly, multiple times, and avoid code duplication by copy-pasting (nearly the same) chunks of code over and over. You could think of iteration as generalizing those repetitions even further. Instead of manually calling a bit of code repeatedly, we can iterate over that code in an iterative way.\nIn general, there are two types of loops:\n\n1. Loops producing a value for each iteration\nThe loops in this category which we are going to encounter most often are those of the apply family, like lapply() or sapply(). The general pattern like this:\n\nresult &lt;- lapply(&lt;vector/list of values&gt;, &lt;function&gt;)\n\nThe lapply and sapply functions take, at minimum, a vector or a list as their first argument, and then a function which takes a single argument. Then they apply the given function to each element of the vector/list, and return either a list (if we use the lapply() function) or a vector (if we use the sapply() function).\nLet’s consider this more concrete example:\n\ninput_list &lt;- list(\"hello\", \"this\", 123, \"is\", \"a mixed\", 42, \"list\")\n\nresult_list &lt;- lapply(input_list, is.numeric)\n\nresult_list\n\n[[1]]\n[1] FALSE\n\n[[2]]\n[1] FALSE\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] FALSE\n\n[[5]]\n[1] FALSE\n\n[[6]]\n[1] TRUE\n\n[[7]]\n[1] FALSE\n\n\nWe took an input_list and applied a function to each element, automatically, gathering the results in another list!\n\nCreate the following function which, for a given number, returns TRUE if this number is even and FALSE if the number is odd. Then use sapply() to test which of the following numbers in the input_vector is odd or even. Notice that we can do thousands or even millions of operations like this (for very very long input vectors) with a single sapply() command!\n\ninput_vector &lt;- c(23, 11, 8, 36, 47, 6, 66, 94, 20, 2)\n\nis_even &lt;- function(x) {\n  x %% 2 == 0 # this answers the question \"does x divided by 2 give 0?\")\n}\n\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nLet’s first test our custom-made function on a couple of examples. This is always a good idea when we want to do iteration using lapply() / sapply().\n\nis_even(2)\n\n[1] TRUE\n\nis_even(7)\n\n[1] FALSE\n\nis_even(10)\n\n[1] TRUE\n\nis_even(11)\n\n[1] FALSE\n\n\nNow we apply is_even to every element of our input_vector:\n\nresult &lt;- sapply(input_vector, is_even)\nresult\n\n [1] FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\n\n\n\n\nAs a practice of indexing, use the result of the sapply() you just got to filter the input_vector values to only odd numbers.\nHint: Remember that you can negate any TRUE / FALSE value (even vector) using the ! operator.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nAgain, this is how we detected which numbers are even:\n\neven_numbers &lt;- sapply(input_vector, is_even)\neven_numbers\n\n [1] FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\nThis is how we can flip this vector to get odd number indices:\n\nodd_numbers &lt;- !even_numbers\nodd_numbers\n\n [1]  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n\n\nAnd finally, this is how we can filter the original vector to only numbers that are odd:\n\ninput_vector[odd_numbers]\n\n[1] 23 11 47\n\n\nHere’s a secret: there’s a much quicker way to do this, even without looping, utilizing the fact that many operations can be performed on vectors in a “vectorized” way (for every element at the same time):\nThis gives us even numbers right away:\n\ninput_vector %% 2 != 0\n\n [1]  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n\n\nThis is how we can do the filtering in one go:\n\ninput_vector[input_vector %% 2 != 0]\n\n[1] 23 11 47\n\n\n\n\n\n\nThese examples are perhaps too boring to see immediate usefulness of sapply() and lapply() for real-world applications. In later sessions, we will see more complex (and more useful) applications of these functions in daily data analysis tasks.\nThat said, you can hopefully see how automating an action of many things at once can be very useful means to save yourself repeated typing of the same command over and over. Again, consider a situation in which you have thousands or even millions of data points!\n\n2. Loops which don’t necessarily return a value\nThis category of loops most often takes form of a for loop, which generally have the following shape:\n\nfor (&lt;item&gt; in &lt;vector or list&gt;) {\n  ... some commands ...\n}\n\nThe most trivial runnable example I could think of is this:\n\n# \"input\" vector of x values\nxs &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\n# take each x out of all given xs in sequence...\nfor (x in xs) {\n  # ... and print it out on a new line\n  cat(\"The square of\", x, \"is\", x^2, \"\\n\")\n}\n\nThe square of 1 is 1 \nThe square of 2 is 4 \nThe square of 3 is 9 \nThe square of 4 is 16 \nThe square of 5 is 25 \nThe square of 6 is 36 \nThe square of 7 is 49 \nThe square of 8 is 64 \nThe square of 9 is 81 \nThe square of 10 is 100 \n\n\nNote: cat() is a very useful function which prints out a given value (or here, actually, multiple values!). If we append \"\\n\" it will add an (invisible) “new line” character, equivalent of hitting ENTER on your keyboard when writing.\n\nLet’s say you want to automate the plotting of several numeric variables from your penguins data frame to a PDF using your custom-defined function penguin_hist() you created above. Fill in the necessary bits of code in the following template to do this! This is a super common pattern that comes in handy very often in data science work.\nNote: You can see the cat() call in the body of the for loop (we call the insides of the { ... } block the “body” of a loop). When you iterate over many things, it’s very useful to print out this sort of “log” information, particularly if the for loop can take very long.\n\n# create an empty PDF file\npdf(\"penguins_hist.pdf\")\n\n# let's define our variables of interest (columns of a data frame)\n# ... put your code here ...\n\n# let's now \"loop over\" each of those variables\nfor (var in variables) {\n# ... put your plotting code here ...\n}\n\ndev.off() # this closes the PDF\n\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\n# create an empty PDF file\npdf(\"penguins_hist.pdf\")\n\n# let's define our variables of interest (columns of a data frame)\nvariables &lt;- c(\"bill_len\", \"bill_dep\", \"flipper_len\", \"body_mass\")\n\n# let's now \"loop over\" each of those variables\nfor (var in variables) {\n  cat(\"Plotting variable\", var, \"...\\n\")\n  # ... here is where you should put your call to `penguin_hist()` like\n  #     you did above manually ...\n}\n## Plotting variable bill_len ...\n## Plotting variable bill_dep ...\n## Plotting variable flipper_len ...\n## Plotting variable body_mass ...\n\ndev.off() # this closes the PDF\n## quartz_off_screen \n##                 2\n\n\n\n\n\nIf everything works correctly, look at the penguins_hist.pdf file you just created! It was all done in a fully automated way! A hugely important thing for reproducibility.\n\nIf you want, look up ?pdf to see how you could modify the width and height of the figures that will be created.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\npdf(\"penguins_hist.pdf\")\n\nvariables &lt;- c(\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\")\nfor (var in variables) {\n  penguin_hist(penguins, var, 100)  \n}\n\ndev.off()\n## quartz_off_screen \n##                 2",
    "crumbs": [
      "R language",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "r-bootcamp.html#further-resources",
    "href": "r-bootcamp.html#further-resources",
    "title": "R bootcamp",
    "section": "Further resources",
    "text": "Further resources\nIf you have energy and time, take a look at the following chapters of the freely-available Advanced R textbook (the best resource for R programming out there). First work through the quiz at the beginning of each chapter. If you’re not sure about answers (the questions are very hard, so if you can’t answer them, that’s completely OK), work through the material of each chapter and try to solve the exercises.\n\nDebugging features\nNames and values\nVectors\nSubsetting\nControl flow\nFunctions\n\nPick whichever topic seems interesting to you. Don’t try to ingest everything – even isolated little bits and details that stick with you will pay off in the long run! I strongly suggest you start with the debugging topic (particularly the interactive debugging features of R and RStudio) because this will have the biggest immediate impact on your own work.",
    "crumbs": [
      "R language",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "tidy-basics.html",
    "href": "tidy-basics.html",
    "title": "Introduction to tidyverse",
    "section": "",
    "text": "Packages introduced in this session\nBefore we begin, let’s introduce some crucial parts of the tidyverse ecosystem, which we will be using extensively during exercises in this chapter (and throughout the remaining chapters).\nEvery single script you will be writing in this session will therefore begin with these three lines of code, which load functions from these two packages.\nlibrary(dplyr)\nlibrary(readr)",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "tidy-basics.html#packages-introduced-in-this-session",
    "href": "tidy-basics.html#packages-introduced-in-this-session",
    "title": "Introduction to tidyverse",
    "section": "",
    "text": "dplyr is a centerpiece of the entire R data science universe, providing important functions for data manipulation, data summarization, and filtering of tabular data. Many data-frame operations that were annoying or cumbersome to do during the R bootcamp session are easy and natural to do with dplyr.\nreadr provides useful functions for reading (and writing) tabular data. Think of it as a set of better alternatives to base R functions such as read.table(). (Another very useful package is readxl which is useful for working with Excel data).",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "tidy-basics.html#our-example-data-set",
    "href": "tidy-basics.html#our-example-data-set",
    "title": "Introduction to tidyverse",
    "section": "Our example data set",
    "text": "Our example data set\nLet’s also introduce a second star in this session, our example data set. The command below will read a metadata table with information about individuals published in a recent aDNA paper on the history or the Holocene in West Eurasia, dubbed “MesoNeo” (reference). Feel free to take a couple of minutes to skim the paper to get an idea about the sort of data we will be using during our exercises.\nWe will read the metadata of individual sequenced as part of this paper using the read_tsv() function (from the above-mentioned readr package), and store the metadata table in a variable df (it’s a short name and we’ll be typing it a lot in this session).\nYes, this function is quite magical – it can read stuff from a file stored on the internet. If you’re curious about the file itself, just paste the URL address in your browser. Even though R gives us superpowers in analyzing data, it’s never a bad thing to rely on more mundane ways to look at the information we’re dealing with.\n\ndf &lt;- read_tsv(\"https://tinyurl.com/simgen-metadata\")",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "tidy-basics.html#exercise-0-creating-a-new-script",
    "href": "tidy-basics.html#exercise-0-creating-a-new-script",
    "title": "Introduction to tidyverse",
    "section": "Exercise 0: Creating a new script",
    "text": "Exercise 0: Creating a new script\nNow, begin by creating a new R script in RStudio, (File -&gt; New file -&gt; R Script) and save it somewhere on your computer as tidy-basics.R (File -&gt; Save). Put the library() calls and the read_tsv() command above in this script.\nUnlike the previous R Bootcamp session in which it didn’t really matter if you wrote in a script or R console, in this session, whenever you’re done experimenting in the R console, always record your solution in your R script. I recommend separating code for different exercises with comments, perhaps something like this:\n\n########################################\n# Exercise 1 solutions\n########################################\n\nNow let’s get started!",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "tidy-basics.html#a-selection-of-data-frame-inspection-functions",
    "href": "tidy-basics.html#a-selection-of-data-frame-inspection-functions",
    "title": "Introduction to tidyverse",
    "section": "A selection of data-frame inspection functions",
    "text": "A selection of data-frame inspection functions\nWhenever you get a new source of data, like a table from a collaborator, or a data sheet downloaded from supplementary materials of a paper (our situation in this session!), you need to familiarize yourself with it before you do anything else.\nHere is a list of functions that you will be using constantly when doing data science to answer the following basic questions about your data.\n\n“How many observations (rows) and variables (columns) does my data have?” – nrow(), ncol()\n“What variable names (columns) am I going to be working with?” – colnames()\n“What data types (numbers, strings, logicals, etc.) does it contain?” – str(), or better, glimpse()\n“How can I take a quick look at a subset of the data (first/last couple of rows)?” – head(), tail()\n“For a specific variable column, what is the distribution of values I can expect in that column?” – table() for “categorical types” (types which take only a couple of discrete values), summary() for “numeric types”, min(), max(), which.min(), which.max(). Remember that you can get values of a given col in a data frame df by using the named-list syntax of df$col!\n\nNote: Feel free to use this list as another cheatsheet of sorts! Also, don’t forget to use the ?function command in the R console to look up the documentation to see the possibilities for options and additional features, or even just to refresh your memory. Every time I look up the ? help for a function I’ve been using for decade, I always learn new tricks.\n\nBefore we move on, note that when you type df into your R console, you will see a slightly different format of the output than when we worked with plain R data frames in the previous chapter. This format of data frame data is called a “tibble” and represents tidyverse’s more user friendly and modern take on data frames. For almost all practical purposes, from now on, we’ll be talking about tibbles as data frames (they behave the same anyway).",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "tidy-basics.html#exercise-1-exploring-new-data",
    "href": "tidy-basics.html#exercise-1-exploring-new-data",
    "title": "Introduction to tidyverse",
    "section": "Exercise 1: Exploring new data",
    "text": "Exercise 1: Exploring new data\nTry to answer the following questions using the functions from the list above (you should decide which function is appropriate for which question).\nBefore you use one of these functions for the first time, take a moment to skim through its help using the ?FUNCTIONNAME command, just to build that habit of not forgetting that the help pages are always there.\n\nWhat variables (i.e., columns) do we have in our data? Think about what could they mean (some will be obvious, some less so, as is always the case with unknown data from a supplementary material of a paper). What do the first couple of rows of the data look like (i.e., what form does the data take)?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nColumn names function:\n\ncolnames(df)\n\n [1] \"sampleId\"     \"popId\"        \"site\"         \"country\"      \"region\"      \n [6] \"continent\"    \"groupLabel\"   \"groupAge\"     \"flag\"         \"latitude\"    \n[11] \"longitude\"    \"dataSource\"   \"age14C\"       \"ageHigh\"      \"ageLow\"      \n[16] \"ageAverage\"   \"datingSource\" \"coverage\"     \"sex\"          \"hgMT\"        \n[21] \"gpAvg\"        \"ageRaw\"       \"hgYMajor\"     \"hgYMinor\"    \n\n\nA couple of first rows:\n\nhead(df)\n\n# A tibble: 6 × 24\n  sampleId popId site  country region     continent groupLabel groupAge flag \n  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;\n1 NA18486  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n2 NA18488  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n3 NA18489  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n4 NA18498  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n5 NA18499  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n6 NA18501  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n# ℹ 15 more variables: latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, dataSource &lt;chr&gt;,\n#   age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;, ageLow &lt;dbl&gt;, ageAverage &lt;dbl&gt;,\n#   datingSource &lt;chr&gt;, coverage &lt;dbl&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;\n\n\n\n\n\n\nHow many individuals do we have metadata for?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nNumber of rows:\n\nnrow(df)\n\n[1] 4072\n\n\nNumber of unique() sample IDs (this should ideally always give the same number, but there’s never enough sanity checks in data science):\n\nlength(unique(df$sampleId))\n\n[1] 4072\n\n\n\n\n\n\nWhat data types (numbers, strings, logicals, etc.) are our variables of?\nHint: Look at the list of questions and suitable functions above!\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nglimpse(df)\n\nRows: 4,072\nColumns: 24\n$ sampleId     &lt;chr&gt; \"NA18486\", \"NA18488\", \"NA18489\", \"NA18498\", \"NA18499\", \"N…\n$ popId        &lt;chr&gt; \"YRI\", \"YRI\", \"YRI\", \"YRI\", \"YRI\", \"YRI\", \"YRI\", \"YRI\", \"…\n$ site         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ country      &lt;chr&gt; \"Nigeria\", \"Nigeria\", \"Nigeria\", \"Nigeria\", \"Nigeria\", \"N…\n$ region       &lt;chr&gt; \"WestAfrica\", \"WestAfrica\", \"WestAfrica\", \"WestAfrica\", \"…\n$ continent    &lt;chr&gt; \"Africa\", \"Africa\", \"Africa\", \"Africa\", \"Africa\", \"Africa…\n$ groupLabel   &lt;chr&gt; \"YRI\", \"YRI\", \"YRI\", \"YRI\", \"YRI\", \"YRI\", \"YRI\", \"YRI\", \"…\n$ groupAge     &lt;chr&gt; \"Modern\", \"Modern\", \"Modern\", \"Modern\", \"Modern\", \"Modern…\n$ flag         &lt;chr&gt; \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0…\n$ latitude     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ longitude    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ dataSource   &lt;chr&gt; \"1000g\", \"1000g\", \"1000g\", \"1000g\", \"1000g\", \"1000g\", \"10…\n$ age14C       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ageHigh      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ageLow       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ageAverage   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ datingSource &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ coverage     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ sex          &lt;chr&gt; \"XY\", \"XX\", \"XX\", \"XY\", \"XX\", \"XY\", \"XX\", \"XY\", \"XX\", \"XY…\n$ hgMT         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ gpAvg        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ageRaw       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ hgYMajor     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ hgYMinor     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\n\n\n\n\nWhat are the columns we have that describe the ages (maybe look at those which have “age” in their name you detected with the columns() function earlier)? How many missing values (NA) are in the ageAverage column? What information does the groupAge column probably contain?\nHint: Remember that for a column vector df$COLUMN_NAME, the command is.na(df$COLUMN_NAME) gives you the value TRUE for each NA element in that column variable, and sum(is.na(df$COLUMN_NAME)) then counts the number of those NA values (because TRUE counts as 1, FALSE as 0). Alternatively, mean(is.na(df$COLUMN_NAME)) counts the proportion of these NA values (because it computes the proportion of 1s).\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nFrom colnames(df) above we see a number of columns which seem to have something todo with “age”:\n\ncolumns &lt;- colnames(df)\ncolumns\n\n [1] \"sampleId\"     \"popId\"        \"site\"         \"country\"      \"region\"      \n [6] \"continent\"    \"groupLabel\"   \"groupAge\"     \"flag\"         \"latitude\"    \n[11] \"longitude\"    \"dataSource\"   \"age14C\"       \"ageHigh\"      \"ageLow\"      \n[16] \"ageAverage\"   \"datingSource\" \"coverage\"     \"sex\"          \"hgMT\"        \n[21] \"gpAvg\"        \"ageRaw\"       \"hgYMajor\"     \"hgYMinor\"    \n\n\nOur age columns of interest are these:\n\n\n[1] \"groupAge\"   \"age14C\"     \"ageHigh\"    \"ageLow\"     \"ageAverage\"\n[6] \"ageRaw\"    \n\n\nNow let’s take a look at the values available in the column ageAverage:\n\nmean(!is.na(df$ageAverage))\n\n[1] 0.4093811\n\nunique(df$groupAge)\n\n[1] \"Modern\"  \"Ancient\" \"Archaic\"\n\n\nIt looks like the ageAverage variable has the proportion of non-NA values at about 40.94%.\nWe also seem to have another column, groupAge which clusters individuals into three groups. We’ll stick to these two variables whenever we have a question regarding a date of an individual.\n\n\n\n\nHow many “ancient” individuals do we have in our data? How many “modern” (i.e., present-day humans) individuals (column groupAge)?\nHint: Maybe the table() function is most useful here?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntable() is probably the best solution here:\n\ntable(df$groupAge)\n\n\nAncient Archaic  Modern \n   1664       3    2405 \n\n\n\n\n\n\nWho are the mysterious “Archaic” individuals? What are their names (sampleId column) and which publications they come frome (dataSource column)? Use your data-frame row- and column-indexing knowledge from the R bootcamp session here.\nHint: We need to filter our table down to rows which have groupAge == \"Archaic\". This is an indexing operation which you learned about in the R bootcamp session! Remember that data frames can be indexed into along two dimensions: rows and columns. You want to filter by the rows here using a logical vector obtained by df$groupAge == \"Archaic\".\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nAgain, this gets us a logical vector which has TRUE for each element corresponding to an “Archaic” individual (whoever that might be):\n\nwhich_archaic &lt;- df$groupAge == \"Archaic\"\nhead(which_archaic)\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nAnd we can then use this vector as an index into our overall data frame, just like we learned in the bootcamp session:\n\narchaic_df &lt;- df[which_archaic, ]\narchaic_df$sampleId\n\n[1] \"Vindija33.19\"    \"AltaiNeandertal\" \"Denisova\"       \n\n\nOur mysterious “Archaic” individuals are two Neanderthals and a Denisovan!\nHere are the publications:\n\narchaic_df$dataSource\n\n[1] \"Prufer_Science_2017\" \"Prufer_Nature_2014\"  \"Pruefer_2017\"       \n\n\n\n\n\n\nDo we have geographical information in our metadata? Maybe countries or geographical coordinates (or even both)? Which countries do we have individuals from?\nHint: Again, the function table() ran on an appropriate column will help here.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nLooking at colnames(df) we have a number of columns which have something to do with geography: country, region, and also the traditional longitude and latitude coordinates.\nHow can we get an idea about the distribution of countries? table() counts the number of elements in a vector (basically, a histogram without plotting it), sort() then sorts those elements for easier reading:\n\nsort(table(df$country), decreasing = TRUE)\n\n\n         India          China         Russia          Italy        Nigeria \n           394            308            285            256            207 \n       Denmark         Sweden             UK          Spain         Gambia \n           184            174            167            132            114 \n           USA        Vietnam          Japan     PuertoRico        Finland \n           108            108            107            104            102 \n         Kenya       Barbados       Pakistan       Colombia           Peru \n            98             96             96             94             91 \n   SierraLeone         Mexico     Kazakhstan        Estonia        Ukraine \n            85             67             64             60             60 \n        Poland         France         Norway        Hungary        Iceland \n            56             54             42             41             40 \n    Kyrgyzstan         Turkey        Lebanon       Portugal        Ireland \n            39             22             19             18             16 \n        Faroes        Germany      Greenland        Moldova  CzechRepublic \n            15             12             11             10              9 \n  South Africa           Iran         Latvia        Romania        Armenia \n             9              8              7              7              6 \n        Canada         Serbia      Argentina         Brazil Canary Islands \n             6              6              5              5              5 \n      Mongolia          Chile        Georgia           Laos       Malaysia \n             5              4              4              4              4 \n      Thailand   Turkmenistan         Greece       Cameroon      Indonesia \n             4              4              3              2              2 \n       Croatia       Ethiopia      Lithuania     Luxembourg    Philippines \n             1              1              1              1              1 \n      Slovakia    Switzerland \n             1              1 \n\n\n\nsort(table(df$region), decreasing = TRUE)\n\n\n      NorthernEurope            SouthAsia             EastAsia \n                 635                  489                  420 \n      SouthernEurope           WestAfrica         NorthAmerica \n                 409                  406                  382 \nCentralEasternEurope        WesternEurope         SouthAmerica \n                 330                  251                  199 \n           NorthAsia        SouthEastAsia          CentralAsia \n                 146                  124                  107 \n          EastAfrica          WesternAsia          SouthAfrica \n                  99                   59                    9 \n         NorthAfrica        CentralAfrica \n                   5                    2 \n\n\nWe will ignore longitude and latitude for now, because they are most useful in truly geographical data analysis setting (which we will delve into a bit later).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s the distribution of coverage of the samples (coverage column)? Compute the summary() function on the coverage column. Finally, use the hist() function to visualize this information to get a rough idea about the data.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsummary() computes various useful summary statistics, and also reports on the number of NA values which are missing:\n\nsummary(df$coverage)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0124  0.2668  0.7824  1.6238  1.5082 58.8661    2405 \n\n\nWe can see that the coverage information is missing for 2405 individuals, which is the number of individuals in the (present-day) 1000 Genomes Project data. So it makes sense, we only have coverage for the lower-coverage aDNA samples, but for present-day individuals (who have imputed genomes), the coverage does not even make sense here.\nLet’s plot the coverage values using base R hist() function:\n\nhist(df$coverage, breaks = 100)",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "tidy-basics.html#exercise-2-selecting-columns",
    "href": "tidy-basics.html#exercise-2-selecting-columns",
    "title": "Introduction to tidyverse",
    "section": "Exercise 2: Selecting columns",
    "text": "Exercise 2: Selecting columns\nWe have now some basic familiarity with the format of our data and which kinds of variables/columns we have for every individual. We also got a little bit more practice on using base R for basic data-frame operations, mostly indexing (or subsetting). It’s time to learn about techniques for manipulating, modifying, and filtering this data using tidyverse, specifically the dplyr package.\nOften times we end up in a situation in which we don’t want to have a large data frame with a huge number of columns. Not as much for the reasons of the data taking up too much memory, but for convenience. You can see that our df metadata table has 24 columns, which don’t fit on the screen if we just print it out (note the “13 more variables” in the output). Just try this yourself again in your R console and observe what kind of (cluttered) output you get:\n\ndf\n\n# A tibble: 4,072 × 24\n   sampleId popId site  country region     continent groupLabel groupAge flag \n   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;\n 1 NA18486  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n 2 NA18488  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n 3 NA18489  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n 4 NA18498  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n 5 NA18499  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n 6 NA18501  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n 7 NA18502  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n 8 NA18504  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n 9 NA18505  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n10 NA18507  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n# ℹ 4,062 more rows\n# ℹ 15 more variables: latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, dataSource &lt;chr&gt;,\n#   age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;, ageLow &lt;dbl&gt;, ageAverage &lt;dbl&gt;,\n#   datingSource &lt;chr&gt;, coverage &lt;dbl&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;\n\n\nYou can select which columns to pick from a (potentially very large data frame) with the function select(), the simplest and most basic dplyr function. It has the following general format:\nselect(&lt;data frame&gt;, &lt;column 1&gt;, &lt;column 2&gt;, ...)\nAs a reminder, this is how we would select columns using a normal base R subsetting/indexing operation:\n\nsmaller_df &lt;- df[, c(\"sampleId\", \"region\", \"coverage\", \"ageAverage\")]\n\nsmaller_df\n\n# A tibble: 4,072 × 4\n   sampleId region     coverage ageAverage\n   &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n 1 NA18486  WestAfrica       NA         NA\n 2 NA18488  WestAfrica       NA         NA\n 3 NA18489  WestAfrica       NA         NA\n 4 NA18498  WestAfrica       NA         NA\n 5 NA18499  WestAfrica       NA         NA\n 6 NA18501  WestAfrica       NA         NA\n 7 NA18502  WestAfrica       NA         NA\n 8 NA18504  WestAfrica       NA         NA\n 9 NA18505  WestAfrica       NA         NA\n10 NA18507  WestAfrica       NA         NA\n# ℹ 4,062 more rows\n\n\nThis is the tidyverse approach using select():\n\nsmaller_df &lt;- select(df, sampleId, region, coverage, ageAverage)\n\nsmaller_df\n\n# A tibble: 4,072 × 4\n   sampleId region     coverage ageAverage\n   &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n 1 NA18486  WestAfrica       NA         NA\n 2 NA18488  WestAfrica       NA         NA\n 3 NA18489  WestAfrica       NA         NA\n 4 NA18498  WestAfrica       NA         NA\n 5 NA18499  WestAfrica       NA         NA\n 6 NA18501  WestAfrica       NA         NA\n 7 NA18502  WestAfrica       NA         NA\n 8 NA18504  WestAfrica       NA         NA\n 9 NA18505  WestAfrica       NA         NA\n10 NA18507  WestAfrica       NA         NA\n# ℹ 4,062 more rows\n\n\nNote: The most important thing for you to notice here is the absence of “double quotes”. It might not look like much, but saving yourself from having to type double quotes for every data-frame operation (like with base R) is incredibly convenient.\n\nPractice select() by creating three new data frame objects and assigning them into the following two new variables:\n\nData frame df_ages which contains all variables related to sample ages you found above\nData frame df_geo which contains all variables related to geography you found above\n\nAdditionally, the first column of these data frames should always be sampleId, so make sure this column is included in the selection.\nCheck visually by typing out those two data frames into your console, or using the ncol() on them, that you indeed have a smaller number of columns in these two new data frames.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\ndf_ages &lt;- select(df, sampleId, groupAge, age14C, ageHigh, ageLow, ageAverage, ageRaw)\ndf_ages\n\n# A tibble: 4,072 × 7\n   sampleId groupAge age14C ageHigh ageLow ageAverage ageRaw\n   &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; \n 1 NA18486  Modern       NA      NA     NA         NA &lt;NA&gt;  \n 2 NA18488  Modern       NA      NA     NA         NA &lt;NA&gt;  \n 3 NA18489  Modern       NA      NA     NA         NA &lt;NA&gt;  \n 4 NA18498  Modern       NA      NA     NA         NA &lt;NA&gt;  \n 5 NA18499  Modern       NA      NA     NA         NA &lt;NA&gt;  \n 6 NA18501  Modern       NA      NA     NA         NA &lt;NA&gt;  \n 7 NA18502  Modern       NA      NA     NA         NA &lt;NA&gt;  \n 8 NA18504  Modern       NA      NA     NA         NA &lt;NA&gt;  \n 9 NA18505  Modern       NA      NA     NA         NA &lt;NA&gt;  \n10 NA18507  Modern       NA      NA     NA         NA &lt;NA&gt;  \n# ℹ 4,062 more rows\n\n\n\ndf_geo &lt;- select(df, site, country, region, latitude, longitude)\ndf_geo\n\n# A tibble: 4,072 × 5\n   site  country region     latitude longitude\n   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1 &lt;NA&gt;  Nigeria WestAfrica       NA        NA\n 2 &lt;NA&gt;  Nigeria WestAfrica       NA        NA\n 3 &lt;NA&gt;  Nigeria WestAfrica       NA        NA\n 4 &lt;NA&gt;  Nigeria WestAfrica       NA        NA\n 5 &lt;NA&gt;  Nigeria WestAfrica       NA        NA\n 6 &lt;NA&gt;  Nigeria WestAfrica       NA        NA\n 7 &lt;NA&gt;  Nigeria WestAfrica       NA        NA\n 8 &lt;NA&gt;  Nigeria WestAfrica       NA        NA\n 9 &lt;NA&gt;  Nigeria WestAfrica       NA        NA\n10 &lt;NA&gt;  Nigeria WestAfrica       NA        NA\n# ℹ 4,062 more rows\n\n\n\n\n\nNote: select() allows us to see the contents of columns of interest much easier. For instance, in a situation in which we want to analyse the geographical location of samples, we don’t want to see columns unrelated to that (like haplogrous, sex of an individual, etc. which are all part of the huge original table) and select() is the solution to this.\n\nIf your table has many columns of interest you might want to select (and save to a new variable like you did above), typing them all by hand like you did just now can become tiresome real quick. Here are a few helper functions which can be very useful in that situation, and which you can use inside the select() function as an alternative to manually typing out column names:\n\nstarts_with(\"age\") – matches columns starting with the string “age”\nends_with(\"age\") – matches columns ending with the string “age”\ncontains(\"age\") – matches columns containing the string “age”\n\nYou can use them in place of normal column names. If we would modify our select() “template” above, we could do this, for instance:\nselect(df, starts_with(\"text1\"), ends_with(\"text2\"))\nCheck out the ?help belonging to those functions (note that they have ignore.case = TRUE set by default!). Then create the df_ages table again, but this time use the three helper functions listed above.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\ndf_ages1 &lt;- select(df, sampleId, groupAge, age14C, ageHigh, ageLow, ageAverage, ageRaw)\n\ndf_ages2 &lt;- select(df,\n                   sampleId,\n                   starts_with(\"age\", ignore.case = FALSE),\n                   ends_with(\"Age\", ignore.case = FALSE))\n\nThe function contains() unfortunately doesn’t work because of the coverage column:\n\nselect(df, contains(\"age\"))\n\n# A tibble: 4,072 × 7\n   groupAge age14C ageHigh ageLow ageAverage coverage ageRaw\n   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; \n 1 Modern       NA      NA     NA         NA       NA &lt;NA&gt;  \n 2 Modern       NA      NA     NA         NA       NA &lt;NA&gt;  \n 3 Modern       NA      NA     NA         NA       NA &lt;NA&gt;  \n 4 Modern       NA      NA     NA         NA       NA &lt;NA&gt;  \n 5 Modern       NA      NA     NA         NA       NA &lt;NA&gt;  \n 6 Modern       NA      NA     NA         NA       NA &lt;NA&gt;  \n 7 Modern       NA      NA     NA         NA       NA &lt;NA&gt;  \n 8 Modern       NA      NA     NA         NA       NA &lt;NA&gt;  \n 9 Modern       NA      NA     NA         NA       NA &lt;NA&gt;  \n10 Modern       NA      NA     NA         NA       NA &lt;NA&gt;  \n# ℹ 4,062 more rows",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "tidy-basics.html#exercise-3-filtering-rows",
    "href": "tidy-basics.html#exercise-3-filtering-rows",
    "title": "Introduction to tidyverse",
    "section": "Exercise 3: Filtering rows",
    "text": "Exercise 3: Filtering rows\nIn the session on base R, we learned how to filter rows using the indexing operation along the row-based dimension of (two-dimensional) data frames. For instance, in order to find out which individuals in the df metadata are archaics, we can first created a TRUE / FALSE vector of the rows corresponding to those individuals like this:\n\n# get a vector of the indices belonging to archaic individuals\nwhich_archaic &lt;- df$groupAge == \"Archaic\"\n\n# take a peek at the result to make sure we got TRUE / FALSE vector\ntail(which_archaic, 10)\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\n\nAnd then we would use it to index into the data frame (in its first dimension, before the , comma):\n\narchaic_df &lt;- df[which_archaic, ]\narchaic_df\n\n# A tibble: 3 × 24\n  sampleId        popId site  country region continent groupLabel groupAge flag \n  &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;\n1 Vindija33.19    Vind… Vind… Croatia Centr… Europe    Neanderta… Archaic  0    \n2 AltaiNeandertal Alta… Deni… Russia  North… Asia      Neanderta… Archaic  0    \n3 Denisova        Deni… Deni… Russia  North… Asia      Denisova_… Archaic  0    \n# ℹ 15 more variables: latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, dataSource &lt;chr&gt;,\n#   age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;, ageLow &lt;dbl&gt;, ageAverage &lt;dbl&gt;,\n#   datingSource &lt;chr&gt;, coverage &lt;dbl&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;\n\n\nHowever, what if we need to filter on multiple conditions? For instance, what if we want to find all archaic individuals older than 50000 years?\nOne option would be to create multiple TRUE / FALSE vectors, each corresponding to one of those conditions, and then join them into a single logical vector by combining & and | logical operators like you did in the R bootcamp chapter. For example, we could do this:\n\n# who is archaic in our data?\nwhich_archaic &lt;- df$groupAge == \"Archaic\"\n# who is older than 50000 years?\nwhich_old &lt;- df$ageAverage &gt; 50000\n\n# who is BOTH? note the AND logical operator!\nwhich_combined &lt;- which_archaic & which_old\n\nThen we can use that logical vector for row-based indexing (in this case, subsetting of rows) again:\n\ndf[which_combined, c(\"sampleId\", \"ageAverage\")]\n\n# A tibble: 2 × 2\n  sampleId        ageAverage\n  &lt;chr&gt;                &lt;dbl&gt;\n1 AltaiNeandertal     125000\n2 Denisova             80000\n\n\nBut this gets tiring very quickly, requires unnecessary amount of typing, and is very error prone. Imagine having to do this for many more conditions! The filter() function from tidyverse again fixes all of these problems.\nWe can rephrase the example situation with the archaics to use the new function filter() like this (please try it yourself!):\n\nfilter(df, groupAge == \"Archaic\" & ageAverage &gt; 50000)\n\nI hope that, even if you never really programmed much before, you appreciate that this single command involves very little typing and is immediately readable, almost like this English sentence:\n\n“Filter data frame df for individuals in which column groupAge is”Archaic” and who are older than 50000 years”.\n\nOver time you will see that all of tidyverse packages follow these ergonomic principles.\nNote: It’s worth pointing out again that — just like this is the case across the entire tidyverse — we refer to columns of our data frame df (like the columns groupAge or ageAverage right above) as if they were normal variables! Just like we did in select() above. In any tidyverse function we don’t write \"groupAge\", but simply groupAge! When you start, you’ll probably make this kind of mistake quite often. So this is a reminder to keep an eye for this.\n\nPractice filtering with the filter() function by finding out which individual(s) in your df metadata table are from a country with the value \"CzechRepublic\".\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nfilter(df, country == \"CzechRepublic\")\n\n# A tibble: 9 × 24\n  sampleId popId   site       country region continent groupLabel groupAge flag \n  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;\n1 NEO128   NEO128  Vedrovice  CzechR… Centr… Europe    Czech_Neo… Ancient  lowc…\n2 kol6     kol6    Kol√≠n     CzechR… Centr… Europe    Czech_Neo… Ancient  0    \n3 kol2     kol2    Kol√≠n     CzechR… Centr… Europe    Czech_Neo… Ancient  lowG…\n4 RISE566  RISE566 Knezeves   CzechR… Centr… Europe    Czech_Bro… Ancient  0    \n5 RISE586  RISE586 Moravsk√°… CzechR… Centr… Europe    Czech_Bro… Ancient  0    \n6 RISE577  RISE577 Velk√© P≈… CzechR… Centr… Europe    Czech_Bro… Ancient  0    \n7 DA111    DA111   Lovosice … CzechR… Centr… Europe    Czech_Iro… Ancient  0    \n8 DA112    DA112   Lovosice … CzechR… Centr… Europe    Czech_Iro… Ancient  0    \n9 RISE569  RISE569 Brandysek  CzechR… Centr… Europe    Czech_Iro… Ancient  0    \n# ℹ 15 more variables: latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, dataSource &lt;chr&gt;,\n#   age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;, ageLow &lt;dbl&gt;, ageAverage &lt;dbl&gt;,\n#   datingSource &lt;chr&gt;, coverage &lt;dbl&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;\n\n\n\n\n\n\nWhich one of these “Czech” individuals have coverage higher than 1? Which individuals have coverage higher than 10?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nfilter(df, country == \"CzechRepublic\" & coverage &gt; 1)\n\n# A tibble: 3 × 24\n  sampleId popId   site       country region continent groupLabel groupAge flag \n  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;\n1 kol6     kol6    Kol√≠n     CzechR… Centr… Europe    Czech_Neo… Ancient  0    \n2 RISE577  RISE577 Velk√© P≈… CzechR… Centr… Europe    Czech_Bro… Ancient  0    \n3 RISE569  RISE569 Brandysek  CzechR… Centr… Europe    Czech_Iro… Ancient  0    \n# ℹ 15 more variables: latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, dataSource &lt;chr&gt;,\n#   age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;, ageLow &lt;dbl&gt;, ageAverage &lt;dbl&gt;,\n#   datingSource &lt;chr&gt;, coverage &lt;dbl&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;\n\n\n\nfilter(df, country == \"CzechRepublic\" & coverage &gt; 10)\n\n# A tibble: 0 × 24\n# ℹ 24 variables: sampleId &lt;chr&gt;, popId &lt;chr&gt;, site &lt;chr&gt;, country &lt;chr&gt;,\n#   region &lt;chr&gt;, continent &lt;chr&gt;, groupLabel &lt;chr&gt;, groupAge &lt;chr&gt;,\n#   flag &lt;chr&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, dataSource &lt;chr&gt;,\n#   age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;, ageLow &lt;dbl&gt;, ageAverage &lt;dbl&gt;,\n#   datingSource &lt;chr&gt;, coverage &lt;dbl&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "tidy-basics.html#exercise-4-the-pipe",
    "href": "tidy-basics.html#exercise-4-the-pipe",
    "title": "Introduction to tidyverse",
    "section": "Exercise 4: The pipe %>%",
    "text": "Exercise 4: The pipe %&gt;%\nThe pipe operator, %&gt;%, is the rockstar of the tidyverse R ecosystem, and the primary reason what makes tidy data workflow so efficient and easy to read.\nFirst, what is “the pipe”? Whenever you see code like this:\n\nsomething%&gt;%f()\n\nyou can read it as:\n\n“take something and put it as the first argument of f()`”.\n\nWhy would you want to do this? Imagine some complex data processing operation like this:\n\nh(f(g(i(j(input_data)))))\n\nThis means take input_data, compute j(input_data), then compute i() on that, so i(j(input_data)), then compute g(i(j(input_data))), etc. Of course this is an extreme example but is surprisingly not that far off from what we often have to do in data science.\nOne way to make this easier to read would be perhaps this:\n\ntmp1 &lt;- j(input_data)\ntmp2 &lt;- i(tmp1)\ntmp3 &lt;- g(tmp2)\ntmp4 &lt;- f(tmp3)\nresult &lt;- f(tmp4)\n\nBut that’s too much typing when we want to get insights into our data as quickly as possible with as little work as possible.\nThe pipe approach of tidyverse would make the same thing easier to write and read like this:\n\ninput_data %&gt;% j %&gt;% i %&gt;% g %&gt;% f %&gt;% h\n\nThis kind of “data transformation chain” is so frequent that RStudio even provides a built-in shortcut for it:\n\nCMD + Shift + M on macOS\nCTRL + Shift + M on Windows and Linux\n\nWhenever you will pipe something like this in your solutions, always get in the habit of using these shortcuts. Eventually this will allow you to write code as quickly as you can think, trust me! (And take a peek at the cheatsheet) of RStudio shortcuts to refresh your memory on the other useful shortcuts! For instance, Alt + - or Option + - inserts the &lt;- assignment operator!\n\nUse your newly acquired select() and filter() skills, powered by the %&gt;% piping operator, and perform the following transformation on the df metadata table, “chaining” the filter-ing and select-ion operations on the indicated columns:\n\nFirst filter() the df metadata to get only those rows / individuals who are:\n\n\n“Ancient” individuals (column groupAge)\nolder than 10000 years (column ageAverage)\nfrom Italy (column country)\nwith coverage column higher than 3\n\n\nThen pipe the result of step 1. to select() columns: sampleId, site, sex, and hgMT and hgYMajor (mt and Y haplogroups).\n\nAs a practice, try to be as silly as you can and write the entire command with as many uses of filter() and select() function calls in sequence as you can.\nHint: Don’t write the entire pipeline at once! For filter(), start with one condition, evaluate it, then add another one, etc., inspecting the intermediate results as you’re seeing them in the R console after every evaluation from your script (CTRL / CMD + Enter). Alternatively, first build up everything in the console one step at a time, then paste the result into your script to save the command on disk. This is the tidyverse way of doing data science!\nHint: What I mean by this is that the following two commands produce the exact same result:\nnew_df1 &lt;- filter(df, col1 == \"MatchString\" & col2 &gt; 10000 & col3 == TRUE) %&gt;%\n  select(colX, colY, starts_with(\"ColNamePrefix\"))\nlike doing this instead:\nnew_df1 &lt;- df %&gt;%\n  filter(col1 == \"MatchString\") %&gt;%\n  filter(col2 &gt; 10000) %&gt;%\n  filter(col3 == TRUE) %&gt;%\n  select(colX, colY, starts_with(\"ColNamePrefix\"))\n(Just take a moment to read both of these versions and compare them to convince youself they do the same.)\nThis works because “the thing on the left” (which is always a data frame) is placed by the %&gt;% pipe operator as “the first argument of a function on the right” (which again expects always a data frame)!\nHopefully you now see what this idea of “build a more complex %&gt;% pipeline one step a time” can mean in practice. Now apply these ideas to solve the filter() and select() exercise above.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\ndf %&gt;%\n  filter(groupAge == \"Ancient\") %&gt;%\n  filter(ageAverage &gt; 10000) %&gt;%\n  filter(country == \"Italy\") %&gt;%\n  filter(coverage &gt; 3) %&gt;%\n  select(sampleId, site, sex, hgMT, hgYMajor)\n\n# A tibble: 1 × 5\n  sampleId site              sex   hgMT              hgYMajor\n  &lt;chr&gt;    &lt;chr&gt;             &lt;chr&gt; &lt;chr&gt;             &lt;chr&gt;   \n1 R7       Grotta Continenza XY    U5b1+16189+@16192 I2a     \n\n\nWe could also write the same thing more concisely (not the single filter() call):\n\ndf %&gt;%\n  filter(groupAge == \"Ancient\" & ageAverage &gt; 10000 & country == \"Italy\" & coverage &gt; 1) %&gt;%\n  select(sampleId, site, sex, hgMT, hgYMajor)\n\n# A tibble: 1 × 5\n  sampleId site              sex   hgMT              hgYMajor\n  &lt;chr&gt;    &lt;chr&gt;             &lt;chr&gt; &lt;chr&gt;             &lt;chr&gt;   \n1 R7       Grotta Continenza XY    U5b1+16189+@16192 I2a     \n\n\n\n\n\nNote: It is always a good practice to split long chains of %&gt;% piping commands into multiple lines, and indenting them neatly one after the other. Readability matters to avoid bugs in your code!",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "tidy-basics.html#exercise-5-more-complex-conditions",
    "href": "tidy-basics.html#exercise-5-more-complex-conditions",
    "title": "Introduction to tidyverse",
    "section": "Exercise 5: More complex conditions",
    "text": "Exercise 5: More complex conditions\nRecall our exercises about logical conditional expressions (&, |, !, etc.).\nWhenever you need to do a more complex operation, such as saying that a variable columnX should have a value \"ABC\" or \"ZXC\", you can already guess that you can do this by writing filter(df, columnX == \"ABC\" | column == \"ZXC\").\nSimilarly, you can condition on numerical variables, just as we did in the exercises on TRUE / FALSE expressions. For instance, if you want to condition on a column varX being varX &lt; 1 or varX &gt; 10, you could write filter(df, varX &lt; 1 | var X &gt; 10).\n\nPractice combining multiple filtering conditions into a tidyverse %&gt;% “piping chain” by filtering our metadata table to find individuals for which the following conditions hold:\n\nThey are “Ancient” (groupAge column)\nThey are from “France” or “Canary Islands” (country column)\nTheir coverage less than 0.1 or more than 3 (coverage column)\n\nHint: The easiest solution is to write three filter() function calls, one for each condition above:\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\ndf %&gt;%\n  filter(groupAge == \"Ancient\") %&gt;%\n  filter(country == \"France\" | country == \"Canary Islands\") %&gt;%\n  filter(coverage &lt; 0.1 | coverage &gt; 3)\n\n# A tibble: 3 × 24\n  sampleId popId  site        country region continent groupLabel groupAge flag \n  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;\n1 gun011   gun011 Tenerife    Canary… North… Africa    CanaryIsl… Ancient  0    \n2 NEO812   NEO812 Grotte du … France  Weste… Europe    France_Ne… Ancient  0    \n3 NEO813   NEO813 Grotte du … France  Weste… Europe    France_Ne… Ancient  1d_r…\n# ℹ 15 more variables: latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, dataSource &lt;chr&gt;,\n#   age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;, ageLow &lt;dbl&gt;, ageAverage &lt;dbl&gt;,\n#   datingSource &lt;chr&gt;, coverage &lt;dbl&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;\n\n\nWhenever you want to test whether a variable is of a set of multiple possible values (like you wanted here for the country filter), you can use the %in% operator:\n\ndf %&gt;%\n  filter(groupAge == \"Ancient\") %&gt;%\n  filter(country %in% c(\"France\", \"Canary Islands\")) %&gt;%\n  filter(coverage &lt; 0.1 | coverage &gt; 3)\n\n# A tibble: 3 × 24\n  sampleId popId  site        country region continent groupLabel groupAge flag \n  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;\n1 gun011   gun011 Tenerife    Canary… North… Africa    CanaryIsl… Ancient  0    \n2 NEO812   NEO812 Grotte du … France  Weste… Europe    France_Ne… Ancient  0    \n3 NEO813   NEO813 Grotte du … France  Weste… Europe    France_Ne… Ancient  1d_r…\n# ℹ 15 more variables: latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, dataSource &lt;chr&gt;,\n#   age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;, ageLow &lt;dbl&gt;, ageAverage &lt;dbl&gt;,\n#   datingSource &lt;chr&gt;, coverage &lt;dbl&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;\n\n\n\n\n\n\nNow select individuals who are from Germany and have coverage higher than 3 or individuals who are from Estonia with coverage less or equal to 1. Save your result to df_subset variable and print everything in this table by executing print(df_subset, n = Inf).\nSupplementary question: Why do we need to print() the result like this? What happens when you just type df_subset into your R console (or execute this from your script using CTRL / CMD + Enter on its own?)\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nWe can of course compose (infinitely) complex combinations of & and | and !:\n\ndf_subset &lt;- filter(df,\n                    (country == \"Germany\" & coverage &gt; 3) |\n                    (country == \"Estonia\" & coverage &lt;= 1))\n\nprint(df_subset, n = Inf)\n\n# A tibble: 34 × 24\n   sampleId  popId     site   country region continent groupLabel groupAge flag \n   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;\n 1 Stuttgart Stuttgart Stutt… Germany Weste… Europe    Germany_N… Ancient  0    \n 2 FN2       FN2       Bavar… Germany Weste… Europe    Germany_R… Ancient  0    \n 3 Alh1      Alh1      Bavar… Germany Weste… Europe    Germany_M… Ancient  0    \n 4 Alh10     Alh10     Bavar… Germany Weste… Europe    Germany_M… Ancient  0    \n 5 Ardu1     Ardu1     Ardu,… Estonia North… Europe    Estonia_N… Ancient  0    \n 6 Ardu2     Ardu2     Ardu,… Estonia North… Europe    Estonia_N… Ancient  0    \n 7 RISE00    RISE00    J√§ba… Estonia North… Europe    Estonia_N… Ancient  0    \n 8 Kunila1   Kunila1   Kursi… Estonia North… Europe    Estonia_N… Ancient  0    \n 9 V14       V14       Harju… Estonia North… Europe    Estonia_B… Ancient  0    \n10 X10       X10       Harju… Estonia North… Europe    Estonia_B… Ancient  0    \n11 V9        V9        Harju… Estonia North… Europe    Estonia_B… Ancient  0    \n12 0LS11     0LS11     Harju… Estonia North… Europe    Estonia_B… Ancient  0    \n13 X17       X17       Harju… Estonia North… Europe    Estonia_B… Ancient  0    \n14 X08       X08       Harju… Estonia North… Europe    Estonia_B… Ancient  0    \n15 X14       X14       Harju… Estonia North… Europe    Estonia_B… Ancient  0    \n16 V16       V16       Harju… Estonia North… Europe    Estonia_B… Ancient  0    \n17 X15       X15       Tartu… Estonia North… Europe    Estonia_B… Ancient  0    \n18 X11       X11       Ida-V… Estonia North… Europe    Estonia_B… Ancient  0    \n19 V10       V10       Laane… Estonia North… Europe    Estonia_I… Ancient  0    \n20 0LS10     0LS10     Laane… Estonia North… Europe    Estonia_I… Ancient  0    \n21 VII4      VII4      Laane… Estonia North… Europe    Estonia_I… Ancient  0    \n22 V11       V11       Saare… Estonia North… Europe    Estonia_I… Ancient  0    \n23 V12       V12       Saare… Estonia North… Europe    Estonia_I… Ancient  0    \n24 X04       X04       Saare… Estonia North… Europe    Estonia_I… Ancient  0    \n25 VK480     VK480     Salme  Estonia North… Europe    Estonia_V… Ancient  0    \n26 VK485     VK485     Salme  Estonia North… Europe    Estonia_V… Ancient  1d_r…\n27 VK488     VK488     Salme  Estonia North… Europe    Estonia_V… Ancient  0    \n28 VK490     VK490     Salme  Estonia North… Europe    Estonia_V… Ancient  1d_r…\n29 VK504     VK504     Salme  Estonia North… Europe    Estonia_V… Ancient  0    \n30 VK507     VK507     Salme  Estonia North… Europe    Estonia_V… Ancient  0    \n31 VK554     VK554     Salme  Estonia North… Europe    Estonia_V… Ancient  0    \n32 IIa       IIa       Saare… Estonia North… Europe    Estonia_M… Ancient  0    \n33 IIf       IIf       Valga… Estonia North… Europe    Estonia_M… Ancient  0    \n34 IVLS09KT  IVLS09KT  Tartu… Estonia North… Europe    Estonia_M… Ancient  0    \n# ℹ 15 more variables: latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, dataSource &lt;chr&gt;,\n#   age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;, ageLow &lt;dbl&gt;, ageAverage &lt;dbl&gt;,\n#   datingSource &lt;chr&gt;, coverage &lt;dbl&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;\n\n\n\n\n\n\nHow is “chaining” several filter() command one after another different from using a single filter() command with multiple conditional expressions joined by &? How about the difference from joining them with |?",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "tidy-basics.html#exercise-6-dropping-columns",
    "href": "tidy-basics.html#exercise-6-dropping-columns",
    "title": "Introduction to tidyverse",
    "section": "Exercise 6: Dropping columns",
    "text": "Exercise 6: Dropping columns\nThis one will be easy. If you want to drop a column from a table, just prefix its name with a minus sign (-) in a select() function.\nNote: Yes, this also works with starts_with() and its friends above, just put - in front of them!\nTo demonstrate the dropping of columns in practice, here’s our df table again (just one row for brevity). Observe the columns of the resulting table:\n\ndf %&gt;% head(1)\n\n# A tibble: 1 × 24\n  sampleId popId site  country region     continent groupLabel groupAge flag \n  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;\n1 NA18486  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n# ℹ 15 more variables: latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, dataSource &lt;chr&gt;,\n#   age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;, ageLow &lt;dbl&gt;, ageAverage &lt;dbl&gt;,\n#   datingSource &lt;chr&gt;, coverage &lt;dbl&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;\n\n\nObserve what happens when we do this (and compare to the above):\n\ndf %&gt;% select(-sampleId) %&gt;% head(1)\n\n# A tibble: 1 × 23\n  popId site  country region     continent groupLabel groupAge flag  latitude\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;\n1 YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0           NA\n# ℹ 14 more variables: longitude &lt;dbl&gt;, dataSource &lt;chr&gt;, age14C &lt;dbl&gt;,\n#   ageHigh &lt;dbl&gt;, ageLow &lt;dbl&gt;, ageAverage &lt;dbl&gt;, datingSource &lt;chr&gt;,\n#   coverage &lt;dbl&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;, ageRaw &lt;chr&gt;,\n#   hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;\n\n\nAnd this:\n\ndf %&gt;% select(-sampleId, -site) %&gt;% head(1)\n\n# A tibble: 1 × 22\n  popId country region    continent groupLabel groupAge flag  latitude longitude\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 YRI   Nigeria WestAfri… Africa    YRI        Modern   0           NA        NA\n# ℹ 13 more variables: dataSource &lt;chr&gt;, age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;,\n#   ageLow &lt;dbl&gt;, ageAverage &lt;dbl&gt;, datingSource &lt;chr&gt;, coverage &lt;dbl&gt;,\n#   sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;, ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;,\n#   hgYMinor &lt;chr&gt;\n\n\nAnd this:\n\ndf %&gt;% select(-sampleId, -site, -popId) %&gt;% head(1)\n\n# A tibble: 1 × 21\n  country region     continent groupLabel groupAge flag  latitude longitude\n  &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 Nigeria WestAfrica Africa    YRI        Modern   0           NA        NA\n# ℹ 13 more variables: dataSource &lt;chr&gt;, age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;,\n#   ageLow &lt;dbl&gt;, ageAverage &lt;dbl&gt;, datingSource &lt;chr&gt;, coverage &lt;dbl&gt;,\n#   sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;, ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;,\n#   hgYMinor &lt;chr&gt;\n\n\nThe columns prefixed with - are dropped from the resulting table!\nRather than typing out a long list of columns to drop, we can also do this to specify the range of consecutive columns (notice the minus - sign):\n\ndf %&gt;% select(-(sampleId:popId)) %&gt;% head(1)\n\n# A tibble: 1 × 22\n  site  country region    continent groupLabel groupAge flag  latitude longitude\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 &lt;NA&gt;  Nigeria WestAfri… Africa    YRI        Modern   0           NA        NA\n# ℹ 13 more variables: dataSource &lt;chr&gt;, age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;,\n#   ageLow &lt;dbl&gt;, ageAverage &lt;dbl&gt;, datingSource &lt;chr&gt;, coverage &lt;dbl&gt;,\n#   sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;, ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;,\n#   hgYMinor &lt;chr&gt;\n\n\nAlternatively, we can also use our well-known c() function, which is very useful whenever we want to drop a non-consecutive set of columns (again notice the minus - sign):\n\ndf %&gt;% select(-c(sampleId, site, popId)) %&gt;% head(1)\n\n# A tibble: 1 × 21\n  country region     continent groupLabel groupAge flag  latitude longitude\n  &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 Nigeria WestAfrica Africa    YRI        Modern   0           NA        NA\n# ℹ 13 more variables: dataSource &lt;chr&gt;, age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;,\n#   ageLow &lt;dbl&gt;, ageAverage &lt;dbl&gt;, datingSource &lt;chr&gt;, coverage &lt;dbl&gt;,\n#   sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;, ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;,\n#   hgYMinor &lt;chr&gt;\n\n\nNote: The same “range syntax” of using : and and listing columns with c() applies also to selecting which columns to choose, not just for dropping them.\n\nUse the : range in select() to drop every column after country (i.e., all the way to the last column in your table, whichever column this is). Do not save the result back to the df variable though! Just run the select() command on its own.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\ndf %&gt;% select(-(region:hgYMinor))\n\n# A tibble: 4,072 × 4\n   sampleId popId site  country\n   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  \n 1 NA18486  YRI   &lt;NA&gt;  Nigeria\n 2 NA18488  YRI   &lt;NA&gt;  Nigeria\n 3 NA18489  YRI   &lt;NA&gt;  Nigeria\n 4 NA18498  YRI   &lt;NA&gt;  Nigeria\n 5 NA18499  YRI   &lt;NA&gt;  Nigeria\n 6 NA18501  YRI   &lt;NA&gt;  Nigeria\n 7 NA18502  YRI   &lt;NA&gt;  Nigeria\n 8 NA18504  YRI   &lt;NA&gt;  Nigeria\n 9 NA18505  YRI   &lt;NA&gt;  Nigeria\n10 NA18507  YRI   &lt;NA&gt;  Nigeria\n# ℹ 4,062 more rows\n\n\nI was personally a bit surprised that this also works without the parentheses. R is smart!\n\ndf %&gt;% select(-region:-hgYMinor)\n\n# A tibble: 4,072 × 4\n   sampleId popId site  country\n   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  \n 1 NA18486  YRI   &lt;NA&gt;  Nigeria\n 2 NA18488  YRI   &lt;NA&gt;  Nigeria\n 3 NA18489  YRI   &lt;NA&gt;  Nigeria\n 4 NA18498  YRI   &lt;NA&gt;  Nigeria\n 5 NA18499  YRI   &lt;NA&gt;  Nigeria\n 6 NA18501  YRI   &lt;NA&gt;  Nigeria\n 7 NA18502  YRI   &lt;NA&gt;  Nigeria\n 8 NA18504  YRI   &lt;NA&gt;  Nigeria\n 9 NA18505  YRI   &lt;NA&gt;  Nigeria\n10 NA18507  YRI   &lt;NA&gt;  Nigeria\n# ℹ 4,062 more rows",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "tidy-basics.html#exercise-7-renaming-columns",
    "href": "tidy-basics.html#exercise-7-renaming-columns",
    "title": "Introduction to tidyverse",
    "section": "Exercise 7: Renaming columns",
    "text": "Exercise 7: Renaming columns\nVery often you read data frames in which columns have names which are either very long, containing characters which are not allowed, or generally inconvenient. Imagine a situation, in which you refer to a particular column very often in your workflow, but it takes too much effort to type it out, or it uses awkward characters.\nAfter discussing select() and filter(), let’s introduce another member of the tidyverse—the function rename().\nThe template for using it is again very easy (again, you would replace the text in &lt;pointy brackets&gt; with appropriate symbols):\nrename(&lt;data frame&gt;, &lt;new name&gt; = &lt;old name&gt;)\n\nCreate a new data frame df_subset by doing the following:\n\nFirst select() the columns sampleId, popId, country, continent, groupAge, ageAverage, and coverage.\nPipe the result of the filter() operation using %&gt;% into:\nrename() function to give some columns a shorter name: sampleId -&gt; sample, popId -&gt; population, groupAge -&gt; set, ageAverage -&gt; age. Leave the country and coverage columns as they are (i.e., don’t rename those).\n\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\ndf_subset &lt;-\n  df %&gt;%\n  select(sampleId, popId, country, continent, groupAge, ageAverage, coverage) %&gt;%\n  rename(sample = sampleId, population = popId, set = groupAge, age = ageAverage)\n\ndf_subset\n\n# A tibble: 4,072 × 7\n   sample  population country continent set      age coverage\n   &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 NA18486 YRI        Nigeria Africa    Modern    NA       NA\n 2 NA18488 YRI        Nigeria Africa    Modern    NA       NA\n 3 NA18489 YRI        Nigeria Africa    Modern    NA       NA\n 4 NA18498 YRI        Nigeria Africa    Modern    NA       NA\n 5 NA18499 YRI        Nigeria Africa    Modern    NA       NA\n 6 NA18501 YRI        Nigeria Africa    Modern    NA       NA\n 7 NA18502 YRI        Nigeria Africa    Modern    NA       NA\n 8 NA18504 YRI        Nigeria Africa    Modern    NA       NA\n 9 NA18505 YRI        Nigeria Africa    Modern    NA       NA\n10 NA18507 YRI        Nigeria Africa    Modern    NA       NA\n# ℹ 4,062 more rows\n\n\nWe now have a much cleaner table which is much easier to work with!\n\n\n\n\nA shortcut which can be quite useful sometimes is that select() also accepts the new_name = old_name renaming pattern used by the rename() function, which allows you to both select columns (and rename some of them) all at once. To practice this, create the df_subset data frame again, but this time using just select().\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\ndf_subset &lt;- df %&gt;%\n  select(sample = sampleId, population = popId,\n         country,\n         continent,\n         set = groupAge, age = ageAverage,\n         coverage)\n\ndf_subset\n\n# A tibble: 4,072 × 7\n   sample  population country continent set      age coverage\n   &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 NA18486 YRI        Nigeria Africa    Modern    NA       NA\n 2 NA18488 YRI        Nigeria Africa    Modern    NA       NA\n 3 NA18489 YRI        Nigeria Africa    Modern    NA       NA\n 4 NA18498 YRI        Nigeria Africa    Modern    NA       NA\n 5 NA18499 YRI        Nigeria Africa    Modern    NA       NA\n 6 NA18501 YRI        Nigeria Africa    Modern    NA       NA\n 7 NA18502 YRI        Nigeria Africa    Modern    NA       NA\n 8 NA18504 YRI        Nigeria Africa    Modern    NA       NA\n 9 NA18505 YRI        Nigeria Africa    Modern    NA       NA\n10 NA18507 YRI        Nigeria Africa    Modern    NA       NA\n# ℹ 4,062 more rows\n\n\n\n\n\n\nWhen would you use one or the other (select() vs rename())`?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nAnswer: select() always drops the columns which are not explicitly listed. rename() only renames the columns which are listed, but retains all other columns even though they are not listed.",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "tidy-basics.html#exercise-8-reorganizing-columns",
    "href": "tidy-basics.html#exercise-8-reorganizing-columns",
    "title": "Introduction to tidyverse",
    "section": "Exercise 8: Reorganizing columns",
    "text": "Exercise 8: Reorganizing columns\nLet’s look at another useful application of the select() function and that is reordering columns. Our df metadata table has 24 columns. When we print it out, we only see a couple of them:\n\nhead(df, 3)\n\n# A tibble: 3 × 24\n  sampleId popId site  country region     continent groupLabel groupAge flag \n  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;\n1 NA18486  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n2 NA18488  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n3 NA18489  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n# ℹ 15 more variables: latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, dataSource &lt;chr&gt;,\n#   age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;, ageLow &lt;dbl&gt;, ageAverage &lt;dbl&gt;,\n#   datingSource &lt;chr&gt;, coverage &lt;dbl&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;\n\n\nOftentimes when doing data analysis, we often work interactively in the console, focusing on a specific subset of columns, and need to immediately see the values of our columns of interest, rather than having them buried in the rest of the (non-visible) output — how can we do this?\nWe already know that we can use select() to pick those columns of interest, but this removes the non-selected columns from the data frame we get. Whenever we want to retain them, we can add the call to everything(), like this:\nselect(&lt;data frame&gt;, &lt;column 1&gt;, &lt;column 2&gt;, ..., everything())\nWhich effectively moves &lt;column 1&gt;, &lt;column 2&gt;, … to the “front” of our table, and adds everything else at the end.\nSelect the subset of columns you selected in the previous exercise on renaming in exactly the same way, but this time add a call to everything() at the end to keep the entire data set intact (with just columns rearranged). Save the result to df again.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nOur data frame before:\n\nhead(df, 3)\n\n# A tibble: 3 × 24\n  sampleId popId site  country region     continent groupLabel groupAge flag \n  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;\n1 NA18486  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n2 NA18488  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n3 NA18489  YRI   &lt;NA&gt;  Nigeria WestAfrica Africa    YRI        Modern   0    \n# ℹ 15 more variables: latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, dataSource &lt;chr&gt;,\n#   age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;, ageLow &lt;dbl&gt;, ageAverage &lt;dbl&gt;,\n#   datingSource &lt;chr&gt;, coverage &lt;dbl&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;\n\n\n\ndf &lt;- select(df, sample = sampleId, population = popId, country, continent,\n             set = groupAge, age = ageAverage, coverage,\n             everything())\n\nOur data frame after:\n\nhead(df, 3)\n\n# A tibble: 3 × 24\n  sample  population country continent set      age coverage site  region    \n  &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     \n1 NA18486 YRI        Nigeria Africa    Modern    NA       NA &lt;NA&gt;  WestAfrica\n2 NA18488 YRI        Nigeria Africa    Modern    NA       NA &lt;NA&gt;  WestAfrica\n3 NA18489 YRI        Nigeria Africa    Modern    NA       NA &lt;NA&gt;  WestAfrica\n# ℹ 15 more variables: groupLabel &lt;chr&gt;, flag &lt;chr&gt;, latitude &lt;dbl&gt;,\n#   longitude &lt;dbl&gt;, dataSource &lt;chr&gt;, age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;,\n#   ageLow &lt;dbl&gt;, datingSource &lt;chr&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;\n\n\nNotice that we prioritized the selected columns of interest (and also renamed some for more readability), but we still have all the other columns available!\n\n\n\n\nExperiment with the function relocate() (it uses the same format as select()). Try it with our df table and with giving it a names of a couple of columns, similarly to what you did with select() above. What result do you get?\nWhat happens when you do the same with select() (just use select() instead of relocate()) and add everything() after the last column?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nrelocate(df, flag, ageLow, continent)\n\n# A tibble: 4,072 × 24\n   flag  ageLow continent sample  population country set      age coverage site \n   &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;\n 1 0         NA Africa    NA18486 YRI        Nigeria Modern    NA       NA &lt;NA&gt; \n 2 0         NA Africa    NA18488 YRI        Nigeria Modern    NA       NA &lt;NA&gt; \n 3 0         NA Africa    NA18489 YRI        Nigeria Modern    NA       NA &lt;NA&gt; \n 4 0         NA Africa    NA18498 YRI        Nigeria Modern    NA       NA &lt;NA&gt; \n 5 0         NA Africa    NA18499 YRI        Nigeria Modern    NA       NA &lt;NA&gt; \n 6 0         NA Africa    NA18501 YRI        Nigeria Modern    NA       NA &lt;NA&gt; \n 7 0         NA Africa    NA18502 YRI        Nigeria Modern    NA       NA &lt;NA&gt; \n 8 0         NA Africa    NA18504 YRI        Nigeria Modern    NA       NA &lt;NA&gt; \n 9 0         NA Africa    NA18505 YRI        Nigeria Modern    NA       NA &lt;NA&gt; \n10 0         NA Africa    NA18507 YRI        Nigeria Modern    NA       NA &lt;NA&gt; \n# ℹ 4,062 more rows\n# ℹ 14 more variables: region &lt;chr&gt;, groupLabel &lt;chr&gt;, latitude &lt;dbl&gt;,\n#   longitude &lt;dbl&gt;, dataSource &lt;chr&gt;, age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;,\n#   datingSource &lt;chr&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;, ageRaw &lt;chr&gt;,\n#   hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;\n\n\n\nselect(df, flag, ageLow, continent, everything())\n\n# A tibble: 4,072 × 24\n   flag  ageLow continent sample  population country set      age coverage site \n   &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;\n 1 0         NA Africa    NA18486 YRI        Nigeria Modern    NA       NA &lt;NA&gt; \n 2 0         NA Africa    NA18488 YRI        Nigeria Modern    NA       NA &lt;NA&gt; \n 3 0         NA Africa    NA18489 YRI        Nigeria Modern    NA       NA &lt;NA&gt; \n 4 0         NA Africa    NA18498 YRI        Nigeria Modern    NA       NA &lt;NA&gt; \n 5 0         NA Africa    NA18499 YRI        Nigeria Modern    NA       NA &lt;NA&gt; \n 6 0         NA Africa    NA18501 YRI        Nigeria Modern    NA       NA &lt;NA&gt; \n 7 0         NA Africa    NA18502 YRI        Nigeria Modern    NA       NA &lt;NA&gt; \n 8 0         NA Africa    NA18504 YRI        Nigeria Modern    NA       NA &lt;NA&gt; \n 9 0         NA Africa    NA18505 YRI        Nigeria Modern    NA       NA &lt;NA&gt; \n10 0         NA Africa    NA18507 YRI        Nigeria Modern    NA       NA &lt;NA&gt; \n# ℹ 4,062 more rows\n# ℹ 14 more variables: region &lt;chr&gt;, groupLabel &lt;chr&gt;, latitude &lt;dbl&gt;,\n#   longitude &lt;dbl&gt;, dataSource &lt;chr&gt;, age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;,\n#   datingSource &lt;chr&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;, ageRaw &lt;chr&gt;,\n#   hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;\n\n\neverything() is another useful helper function for select() operations, like starts_with() et al. above.\n\n\n\n\nIs the select() & everything() combination needed when we have relocate()?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nI don’t think so — I only learned about relocate() two days ago, after using R for ten years professionally. But there’s a lot of code out there using select() for this, so it’s good to be aware of this technique.",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "tidy-basics.html#exercise-10-sorting-rows-based-on-column-values",
    "href": "tidy-basics.html#exercise-10-sorting-rows-based-on-column-values",
    "title": "Introduction to tidyverse",
    "section": "Exercise 10: Sorting rows based on column values",
    "text": "Exercise 10: Sorting rows based on column values\nWhen you need to sort the rows of a data frame based on a value of one or multiple columns, you will need to use the function arrange(). Again, in the spirit of the consistency across the tidyverse ecosystem, it follows exactly the same format of first giving the function a data frame to operate on, followed by a list of columns to sort by.\narrange(&lt;data frame&gt;, &lt;column 1&gt;, &lt;column 2&gt;, ...)\nNote: When you want to reverse the order of the sort, you can surround the column name in a helper function desc() (standing for “descending”).\narrange(&lt;data frame&gt;, desc(&lt;column 1&gt;), desc(&lt;column 2&gt;), ...)\n\nWho is the oldest individual in your data who is not an archaic individual?\nHint: Remember that you can filter() out rows corresponding to archaic individuals with the condition set != \"Ancient\" and then pipe %&gt;% the result into arrange() for sorting based on the column age.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nIt’s the famous Ust’-Ishim individual!\n\ndf %&gt;% filter(set != \"Archaic\") %&gt;% arrange(desc(age))\n\n# A tibble: 4,069 × 24\n   sample   population country continent set        age coverage site     region\n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; \n 1 UstIshim UstIshim   Russia  Asia      Ancient 45020    35.2   Ust'-Is… North…\n 2 Kostenki Kostenki   Russia  Europe    Ancient 37470     2.53  Kostenki Centr…\n 3 SII      SII        Russia  Europe    Ancient 34234     4.25  Sunghir  Centr…\n 4 SIII     SIII       Russia  Europe    Ancient 34092.   11.2   Sunghir  Centr…\n 5 SIV      SIV        Russia  Europe    Ancient 33992     4.04  Sunghir  Centr…\n 6 SI       SI         Russia  Europe    Ancient 32822.    1.16  Sunghir  Centr…\n 7 Yana     Yana       Russia  Asia      Ancient 31950    26.5   Yana RHS North…\n 8 Yana2    Yana2      Russia  Asia      Ancient 31950     6.81  Yana RHS North…\n 9 NEO283   NEO283     Georgia Asia      Ancient 25635     0.786 Kotias … Weste…\n10 MA1      MA1        Russia  Asia      Ancient 24305     1.09  Mal'ta   North…\n# ℹ 4,059 more rows\n# ℹ 15 more variables: groupLabel &lt;chr&gt;, flag &lt;chr&gt;, latitude &lt;dbl&gt;,\n#   longitude &lt;dbl&gt;, dataSource &lt;chr&gt;, age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;,\n#   ageLow &lt;dbl&gt;, datingSource &lt;chr&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;\n\n\n\n\n\n\nSimilarly, who is the youngest ancient individual in your data in terms of its dating?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nApparently it’s an individual from Andaman islands from 90 years ago:\n\ndf %&gt;% filter(set == \"Ancient\") %&gt;% arrange(age)\n\n# A tibble: 1,664 × 24\n   sample  population country   continent set       age coverage site     region\n   &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; \n 1 Andaman Andaman    India     Asia      Ancient   90    16.6   Great A… South…\n 2 890     890        Argentina America   Ancient  100     0.508 Beagle … South…\n 3 894     894        Argentina America   Ancient  100     0.983 Beagle … South…\n 4 895     895        Argentina America   Ancient  100     1.32  Beagle … South…\n 5 MA577   MA577      Argentina America   Ancient  100     1.82  Tierra … South…\n 6 Nr74    Nr74       Chile     America   Ancient  100     0.438 Strait … South…\n 7 AM71    AM71       Chile     America   Ancient  100     0.103 Strait … South…\n 8 523a_C  523a_C     USA       America   Ancient  125     0.956 Palm Si… North…\n 9 Vt719   Vt719      Vietnam   Asia      Ancient  154.    0.257 Northea… South…\n10 KOV-A-2 KOV-A-2    Iceland   Europe    Ancient  236     0.685 Kopavog… North…\n# ℹ 1,654 more rows\n# ℹ 15 more variables: groupLabel &lt;chr&gt;, flag &lt;chr&gt;, latitude &lt;dbl&gt;,\n#   longitude &lt;dbl&gt;, dataSource &lt;chr&gt;, age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;,\n#   ageLow &lt;dbl&gt;, datingSource &lt;chr&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;\n\n\n\n\n\n\nDoes the same approach work for sorting text? What do you get when you try sorting based on the country column?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nYes, this gives us standard alphabetical sort!\n\ndf %&gt;% arrange(country)\n\n# A tibble: 4,072 × 24\n   sample    population country   continent set       age coverage site   region\n   &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n 1 Aconcagua Aconcagua  Argentina America   Ancient  500    2.67   Cerro… South…\n 2 890       890        Argentina America   Ancient  100    0.508  Beagl… South…\n 3 894       894        Argentina America   Ancient  100    0.983  Beagl… South…\n 4 895       895        Argentina America   Ancient  100    1.32   Beagl… South…\n 5 MA577     MA577      Argentina America   Ancient  100    1.82   Tierr… South…\n 6 NEO110    NEO110     Armenia   Asia      Ancient 7594    0.0605 Aknas… Weste…\n 7 RISE423   RISE423    Armenia   Asia      Ancient 3256.   0.366  Nerqu… Weste…\n 8 DA31      DA31       Armenia   Asia      Ancient 3314.   0.345  Lchas… Weste…\n 9 DA35      DA35       Armenia   Asia      Ancient 3314.   1.62   Lchas… Weste…\n10 RISE407   RISE407    Armenia   Asia      Ancient 2955    0.187  Norab… Weste…\n# ℹ 4,062 more rows\n# ℹ 15 more variables: groupLabel &lt;chr&gt;, flag &lt;chr&gt;, latitude &lt;dbl&gt;,\n#   longitude &lt;dbl&gt;, dataSource &lt;chr&gt;, age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;,\n#   ageLow &lt;dbl&gt;, datingSource &lt;chr&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;\n\n\n\n\n\n\nWhat do you get when you try sorting based on country and then coverage? What happens when you sort based on coverage and then country? Why is there a difference?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\ndf %&gt;% arrange(country, coverage)\n\n# A tibble: 4,072 × 24\n   sample    population country   continent set       age coverage site   region\n   &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n 1 890       890        Argentina America   Ancient  100    0.508  Beagl… South…\n 2 894       894        Argentina America   Ancient  100    0.983  Beagl… South…\n 3 895       895        Argentina America   Ancient  100    1.32   Beagl… South…\n 4 MA577     MA577      Argentina America   Ancient  100    1.82   Tierr… South…\n 5 Aconcagua Aconcagua  Argentina America   Ancient  500    2.67   Cerro… South…\n 6 NEO110    NEO110     Armenia   Asia      Ancient 7594    0.0605 Aknas… Weste…\n 7 RISE407   RISE407    Armenia   Asia      Ancient 2955    0.187  Norab… Weste…\n 8 DA31      DA31       Armenia   Asia      Ancient 3314.   0.345  Lchas… Weste…\n 9 RISE423   RISE423    Armenia   Asia      Ancient 3256.   0.366  Nerqu… Weste…\n10 RISE397   RISE397    Armenia   Asia      Ancient 2902.   0.392  Kapan  Weste…\n# ℹ 4,062 more rows\n# ℹ 15 more variables: groupLabel &lt;chr&gt;, flag &lt;chr&gt;, latitude &lt;dbl&gt;,\n#   longitude &lt;dbl&gt;, dataSource &lt;chr&gt;, age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;,\n#   ageLow &lt;dbl&gt;, datingSource &lt;chr&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;\n\ndf %&gt;% arrange(coverage, country)\n\n# A tibble: 4,072 × 24\n   sample population country continent set       age coverage site        region\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt; \n 1 NEO496 NEO496     Ukraine Europe    Ancient 10655   0.0124 Vasilevka-I Centr…\n 2 NEO7   NEO7       Denmark Europe    Ancient  5240   0.0127 Sigersdal … North…\n 3 NEO13  NEO13      Denmark Europe    Ancient  9511   0.0134 Hedegaard … North…\n 4 NEO580 NEO580     Denmark Europe    Ancient  4612   0.0134 Klokkehøj   North…\n 5 NEO1   NEO1       Denmark Europe    Ancient  6594   0.0150 Holmegård-… North…\n 6 NEO916 NEO916     Russia  Asia      Ancient  5727   0.0175 Vengerovo-2 North…\n 7 NEO41  NEO41      Denmark Europe    Ancient  5531   0.0195 Rude        North…\n 8 NEO915 NEO915     Russia  Asia      Ancient  5727   0.0202 Vengerovo-2 North…\n 9 NEO566 NEO566     Denmark Europe    Ancient  5135   0.0206 Døjringe    North…\n10 NEO961 NEO961     Denmark Europe    Ancient  5149   0.0229 Avlebjerg … North…\n# ℹ 4,062 more rows\n# ℹ 15 more variables: groupLabel &lt;chr&gt;, flag &lt;chr&gt;, latitude &lt;dbl&gt;,\n#   longitude &lt;dbl&gt;, dataSource &lt;chr&gt;, age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;,\n#   ageLow &lt;dbl&gt;, datingSource &lt;chr&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;\n\n\n\n\n\n\nDoes it matter what order you use filter() (on some column) and arrange() (on another column) if you’re using both functions? If yes, why? If not, why not? Think about the amount of work these functions have to do in either of those two scenarios.",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "tidy-basics.html#exercise-11-mutating-tables",
    "href": "tidy-basics.html#exercise-11-mutating-tables",
    "title": "Introduction to tidyverse",
    "section": "Exercise 11: Mutating tables",
    "text": "Exercise 11: Mutating tables\nMutating a table means adding a column for a new variable. Again, as with the previously introduced functions select(), filter(), and rename() and arrange(), it follows a consistent tidyverse pattern:\ndf %&gt;% mutate(&lt;new column name&gt; = &lt;vector of the required length&gt;)\nNot surprisingly, the new column is assigned a vector of the same length as the number of rows in a data frame. What we often do is this:\ndf %&gt;% mutate(&lt;new column name&gt; = &lt;expression involving other columns&gt;)\nbecause mutate() allows us to refer to other columns in the data frame already present.\nPlease note that you can also modify an existing column using the same command. In this case, &lt;new column name&gt; above could simply be the name of an already existing column.\n\nTo demonstrate this on a couple of exercises, let’s actually remove some columns from our data frame first. They were originally created by mutate() in the first place and this gives as a useful opportunity for practice (because we’ll soon add them back again ourselves):\n\ndf &lt;- select(df, -set)\n\nThe important aspect of mutate is that it is a vectorized operation. We can’t create a column and give values to only some rows. Here are several ways how we could do this, starting with a simple example of assignment of sex description based on sex chromosomes:\n\n1. if_else()\nif_else() accepts a logical vector (i.e., a vector of TRUE / FALSE values), and produces another vector which contains one value for each TRUE, and another value for each FALSE. Here is an example:\n\nv &lt;- c(TRUE, TRUE, FALSE, TRUE, FALSE)\n\nif_else(v, 123, -123)\n\n[1]  123  123 -123  123 -123\n\n\nNotice that we get 123 for each TRUE and -123 for each FALSE.\n\nThe above can be a bit confusing, so spend a bit of time playing around with if_else(). For instance, create a different logical vector variable (containing an arbitrary TRUE or FALSE values, up to you), and have the function produce values “hello” and “bye” depending on TRUE / FALSE state of each element of the logical vector variable.\nJust get familiar with this vectorized thinking because whenever we do data science, it almost always happens in this vectorized fashion (such as operating on every single value in a column of a table at once, like we’ll do soon).\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nv &lt;- c(FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, TRUE, FALSE)\n\nif_else(v, \"hello\", \"bye\")\n\n[1] \"bye\"   \"bye\"   \"bye\"   \"hello\" \"hello\" \"bye\"   \"hello\" \"bye\"  \n\n\n\n\n\n\nAs mentioned, this function is extremely useful whenever we need to generate values of a new column based on values of another column(s). The general pattern for using it inside a mutate() call is this:\ndf %&gt;%\n  mutate(&lt;new_column&gt; = if_else(&lt;logical vector&gt;, value_for_true, value_for_false))\n\nYou can see that the column df$sex contains chromosome sex determination of each individual as either XY or XX. Use the if_else() function in a mutate() call to create a new column sex_desc which will contain a character vector of either “male” or “female” depending on the sex chromosome determination.\nHint: Again, you might want to build an intuition first. Create a small vector of a mix of “XY” and “XX” values and store it in a variable sex. Then experiment with if_else()-based assignment of “female” and “male”. When you’re sure you got it, apply it to the data frame in the mutate() call to solve this exercise.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\ndf &lt;- mutate(df, sex_desc = if_else(sex == \"XY\", \"male\", \"female\"))\n\n\n\n\n\nRun table(df$sex, df$sex_value) to make sure the assignment of sex description worked as expected. How do you interpret the results? Did we miss something? If we did, what is wrong?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\ntable(df$sex, df$sex_desc)\n\n     \n      female male\n  XX    1862    0\n  XXY      1    0\n  XY       0 2209\n\n\nOh, we can see that there’s an “XXY” individual who – because of our improperly specified if_else() condition (which assigns individuals as male only when they are “XY”) – got assigned in to the “female” category.\nWe would’ve noticed this in our exploratory phase, had we checked the range of possible values of the sex column like this. This is why getting familiar with all data is so crucial the first time we look at it. We could’ve (and should’ve) ran this at the beginning:\n\ntable(df$sex)\n\n\n  XX  XXY   XY \n1862    1 2209 \n\nunique(df$sex)\n\n[1] \"XY\"  \"XX\"  \"XXY\"\n\n\n\n\n\n\nIt turns out there is an individual with a Klinefelter syndrome. A male carrying an extra X chromosome. Clearly the logic of our if_else() is incorrect because this individual was incorrectly assigned as “female”. Take a look at your mutate() and if_else() code again to make sure you see why we did it wrong. How would you fix the mutate() (or rather the if_else()) command to work correctly and correctly assign the XXY individual as “male”?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nThere are a number of options. This is simplest, we can just flip the condition:\n\ndf &lt;- mutate(df, sex_desc = if_else(sex == \"XX\", \"female\", \"male\"))\n\nBut it may be better to be more explicit?\n\ndf &lt;- mutate(df, sex_desc = if_else(sex %in% c(\"XY\", \"XXY\"), \"male\", \"female\"))\n\nWe can verify that the assignment of sex description worked correctly now:\n\ntable(df$sex, df$sex_desc)\n\n     \n      female male\n  XX    1862    0\n  XXY      0    1\n  XY       0 2209\n\n\n\n\n\nThis is a lesson to always remember to check assumptions in your data, even the ones you consider trivial. Functions such as table() and unique() are extremely useful for this.\nNote: I got this originally wrong when I was preparing these materials. So this is a real-world cautionary tale.\n\nLet’s practice if_else() a bit more. First, use filter() to look at the values of the age column for present-day individuals in the 1000 Genomes Project data (dataSource == \"1000g\"). What ages do you see?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nThe individuals have NA age. This could be annoying if we ever want to plot some population genetic statistics as functions of age:\n\ndf %&gt;% filter(dataSource == \"1000g\")\n\n# A tibble: 2,405 × 24\n   sample  population country continent   age coverage site  region   groupLabel\n   &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;     \n 1 NA18486 YRI        Nigeria Africa       NA       NA &lt;NA&gt;  WestAfr… YRI       \n 2 NA18488 YRI        Nigeria Africa       NA       NA &lt;NA&gt;  WestAfr… YRI       \n 3 NA18489 YRI        Nigeria Africa       NA       NA &lt;NA&gt;  WestAfr… YRI       \n 4 NA18498 YRI        Nigeria Africa       NA       NA &lt;NA&gt;  WestAfr… YRI       \n 5 NA18499 YRI        Nigeria Africa       NA       NA &lt;NA&gt;  WestAfr… YRI       \n 6 NA18501 YRI        Nigeria Africa       NA       NA &lt;NA&gt;  WestAfr… YRI       \n 7 NA18502 YRI        Nigeria Africa       NA       NA &lt;NA&gt;  WestAfr… YRI       \n 8 NA18504 YRI        Nigeria Africa       NA       NA &lt;NA&gt;  WestAfr… YRI       \n 9 NA18505 YRI        Nigeria Africa       NA       NA &lt;NA&gt;  WestAfr… YRI       \n10 NA18507 YRI        Nigeria Africa       NA       NA &lt;NA&gt;  WestAfr… YRI       \n# ℹ 2,395 more rows\n# ℹ 15 more variables: flag &lt;chr&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;,\n#   dataSource &lt;chr&gt;, age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;, ageLow &lt;dbl&gt;,\n#   datingSource &lt;chr&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;, ageRaw &lt;chr&gt;,\n#   hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;, sex_desc &lt;chr&gt;\n\n\nIs there anyone in the 1000 GP data who has an age specified?\n\ndf %&gt;% filter(dataSource == \"1000g\") %&gt;% filter(!is.na(age))\n\n# A tibble: 0 × 24\n# ℹ 24 variables: sample &lt;chr&gt;, population &lt;chr&gt;, country &lt;chr&gt;,\n#   continent &lt;chr&gt;, age &lt;dbl&gt;, coverage &lt;dbl&gt;, site &lt;chr&gt;, region &lt;chr&gt;,\n#   groupLabel &lt;chr&gt;, flag &lt;chr&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;,\n#   dataSource &lt;chr&gt;, age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;, ageLow &lt;dbl&gt;,\n#   datingSource &lt;chr&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;, ageRaw &lt;chr&gt;,\n#   hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;, sex_desc &lt;chr&gt;\n\n\n\n\n\n\nThe 1000 GP individuals are missing an age value. It should be set to 0. Let’s fix that now and correct the metadata table.\nYou already know that you can get a logical vector indicating whether a certain element of another vector (here, the column age) is NA or not via the function is.na(). Use is.na(age) in the if_else() to set the age column with the mutate() function so that:\n\nRows where is.na(age) is TRUE will be set to 0, and\nRows where is.na(age) is FALSE will be set to age (because those values don’t need to be replaced).\n\nNote: Using mutate() to replace values of an existing column (rather than creating a new column) is done very often in data science, particularly in “clean up” operations like this one. Your data will never be “perfect” (especially if you get it from some other source) and you will need to do a lot of so-called “table munging” to prepare it for analysis and plotting, just like we’re doing here.\nHint: Again, if you need help, you can start slowly by creating a toy example variable age which will have a vector of a mix of numbers, including some NA values. Then practice creating an if_else() expression which will return 0 in place of NA values in this vector, and keep every other value intact.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nThe individuals have NA age. This could be annoying if we ever want to plot some population genetic statistics as functions of age:\n\ndf &lt;- df %&gt;% mutate(age = if_else(is.na(age), 0, age))\n\n\n\n\n\n\n2. case_when()\nif_else() does only work on binary TRUE or FALSE conditions. But what if we want to create a new column with values of multiple categories, not just two. Recall our original column set (which we dropped from our df table earlier), which had values either “Archaic”, “Ancient”, or “Modern”.\nFor this purpose, case_when() is perfect. It works in a very similar manner to if_else(), but allows not-just-binary categorization. Consider this vector of numbers between 1 and 30:\n\nv &lt;- 1:20\n\nIf we wanted to assign a value “less_than_10” or “10_or_more” to each element, we could use the function if_else() like this:\n\nif_else(v &lt; 10, \"less than 10\", \"10 or more\")\n\n [1] \"less than 10\" \"less than 10\" \"less than 10\" \"less than 10\" \"less than 10\"\n [6] \"less than 10\" \"less than 10\" \"less than 10\" \"less than 10\" \"10 or more\"  \n[11] \"10 or more\"   \"10 or more\"   \"10 or more\"   \"10 or more\"   \"10 or more\"  \n[16] \"10 or more\"   \"10 or more\"   \"10 or more\"   \"10 or more\"   \"10 or more\"  \n\n\nWhat if we want to introduce three or more categories? case_when() to the rescue! Try running this yourself on the vector v created above.\n\ncase_when(\n  v &lt; 10  ~ \"less than 5\",\n  v == 10 ~ \"exactly 10\",\n  v &gt; 10 ~ \"more than 10\"\n)\n\n [1] \"less than 5\"  \"less than 5\"  \"less than 5\"  \"less than 5\"  \"less than 5\" \n [6] \"less than 5\"  \"less than 5\"  \"less than 5\"  \"less than 5\"  \"exactly 10\"  \n[11] \"more than 10\" \"more than 10\" \"more than 10\" \"more than 10\" \"more than 10\"\n[16] \"more than 10\" \"more than 10\" \"more than 10\" \"more than 10\" \"more than 10\"\n\n\nRemember how we had a slightly annoying time with doing vectorized conditional logical expressions on TRUE / FALSE vectors in our Bootcamp session? Now it’s all paying off!\nEvery one of the three conditions actually result in a logical vector, and case_when() decides which value to produced for each element of that vector based on whichever results in TRUE.\nThe case_when() function has a very useful optional argument called .default =, which determines the value it should return whenever a particular location in the logical vector either results in all FALSE (so none of the conditions would apply), or whenever it produces a NA value (so none of the conditions can apply even in principle).\n\nFirst, let’s start simple and reimplement the mutate() operation to add the sex_desc column again (just to practice on something we already know) this time using case_when() and implementing the three conditions (“XX”, “XY”, “XXY”) individually. Additionally, use the .default = argument of case_when() to assign “unknown” as a value of the sex_desc column.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\ndf &lt;-\n  df %&gt;%\n  mutate(sex_desc = case_when(\n    sex == \"XX\" ~ \"female\",\n    sex == \"XY\" ~ \"male\",\n    sex == \"XXY\" ~ \"male\",\n    .default = \"unknown\"\n  ))\n\nLet’s check that this worked correctly:\n\ntable(df$sex, df$sex_desc)\n\n     \n      female male\n  XX    1862    0\n  XXY      0    1\n  XY       0 2209\n\n\n\n\n\n\nIn the exercise on conditional expressions, you learned about & and | operators which make it possible to combine multiple conditions into a single TRUE / FALSE vector. Of course, this means that the conditions inside the case_when() function can also utilize those operators.\nCreate a new column sex_set which will contain the following values:\n\nsex == \"XX\" & age == 0\" should produce \"female (present-day)\",\n(sex == \"XY\" | sex == \"XXY\") & age == 0 should produce \"male (present-day)\",\nsex == \"XX\" & age &gt; 0 should produce \"female (ancient)\",\n(sex == \"XY\" | sex == \"XXY\") & age &gt; 0 should produce \"male (ancient)\",\nany other combination should default to “other”\n\nI provided the code of the logical conditions for you, you just have to put them in a case_when() call appropriately, modifying the solution to the previous exercise. Verify the outcome by doing table(df$sex_set) again.\nWhy did I put parenthesis around the sub-clause involving the | OR operator? What happens if we don’t do this?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\ndf &lt;-\n  df %&gt;%\n  mutate(sex_set = case_when(\n    sex == \"XX\" & age == 0                   ~ \"female (present-day)\",\n    sex == \"XX\" & age &gt; 0                    ~ \"female (ancient)\",\n    (sex == \"XY\" | sex == \"XXY\") & age == 0  ~ \"male (present-day)\",\n    (sex == \"XY\" | sex == \"XXY\") & age &gt; 0   ~ \"male (ancient)\",\n    .default = \"other\"\n  ))\n\nLet’s check that this worked correctly:\n\ntable(df$sex_set)\n\n\n    female (ancient) female (present-day)       male (ancient) \n                 641                 1221                 1026 \n  male (present-day) \n                1184 \n\n\n\n\n\nNote: Again, admittedly this might seem arbitrary to you — why would we need something like this for data analysis and statistics? But this kind of “arbitrary categorization” is extremely useful to generate factor categories for plotting, especially for plotting with ggplot2 later. So keep this in mind for the time being.\n\nRemember how we removed the set column, which originally contained values “Modern”, “Ancient”, or “Archaic”, depending on whether the individual was a present-day modern human, ancient modern human, or an archaic individual, respectively? Use what you learned in this exercise on if_else() or case_when() to reconstruct this column again based on information in the remaining columns. (There is many possible solutions, so don’t be afraid to think creatively!)\nHint: Using information in age and the names of the three archaic individuals (combined with the .default argument of case_when() is probably the easiest solution).\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\n# let's pick out individual names of the archaics\narchaics &lt;- c(\"Vindija33.19\", \"AltaiNeandertal\", \"Denisova\")\n\ndf &lt;-\n  df %&gt;%\n  mutate(set = case_when(\n    age == 0             ~ \"Modern\",  # whoever has sampling date 0 is \"Modern\"\n    sample %in% archaics ~ \"Archaic\", # whoever is among archaics is \"Archaic\"\n    .default             = \"Ancient\"  # and only the third category remains\n  ),\n  .after = coverage)\n\n\n\n\n\nNote the use of .after = in my solution to the previous exercise. Look up the help of mutate() in ?mutate to see what it does and why I used it.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nUsually, when we add a new column using mutate() we don’t want to add it to the very end of the table (where we cannot see it), which is what the function does by default. .after and .before are options which allow us to specify where among the already present columns in the data frame do we want to add the new column",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "tidy-basics.html#exercise-12-summarizing-tables",
    "href": "tidy-basics.html#exercise-12-summarizing-tables",
    "title": "Introduction to tidyverse",
    "section": "Exercise 12: Summarizing tables",
    "text": "Exercise 12: Summarizing tables\nYou have already learned how to operate on rows (filter(), arrange()), columns (select(), rename()), and both rows and columns (mutate()). We have one remaining piece of the puzzle and that is operating on groups of rows. This takes the tidyverse functionality to an entire whole level and allows you to do many more powerful things with tabular data and compute summary statistics.\nIn this section, we will will cover the functions group_by(), summarize(), and various associated “slice functions”.\n\n1. group_by()\nTake a look at the first couple of rows of our metadata table again:\n\ndf\n\n# A tibble: 4,072 × 26\n   sample  population country continent   age coverage set    site  region    \n   &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;     \n 1 NA18486 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 2 NA18488 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 3 NA18489 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 4 NA18498 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 5 NA18499 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 6 NA18501 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 7 NA18502 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 8 NA18504 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 9 NA18505 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n10 NA18507 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n# ℹ 4,062 more rows\n# ℹ 17 more variables: groupLabel &lt;chr&gt;, flag &lt;chr&gt;, latitude &lt;dbl&gt;,\n#   longitude &lt;dbl&gt;, dataSource &lt;chr&gt;, age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;,\n#   ageLow &lt;dbl&gt;, datingSource &lt;chr&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;, sex_desc &lt;chr&gt;, sex_set &lt;chr&gt;\n\n\nNow run the following code in your R console and carefully inspect the output (and compare it to the result of the previous command):\n\ngroup_by(df, continent)\n\n# A tibble: 4,072 × 26\n# Groups:   continent [4]\n   sample  population country continent   age coverage set    site  region    \n   &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;     \n 1 NA18486 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 2 NA18488 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 3 NA18489 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 4 NA18498 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 5 NA18499 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 6 NA18501 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 7 NA18502 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 8 NA18504 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 9 NA18505 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n10 NA18507 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n# ℹ 4,062 more rows\n# ℹ 17 more variables: groupLabel &lt;chr&gt;, flag &lt;chr&gt;, latitude &lt;dbl&gt;,\n#   longitude &lt;dbl&gt;, dataSource &lt;chr&gt;, age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;,\n#   ageLow &lt;dbl&gt;, datingSource &lt;chr&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;, sex_desc &lt;chr&gt;, sex_set &lt;chr&gt;\n\n\nYou can probably see that the data hasn’t changed at all (same number of columns, same number of rows, all remains the same), but you can notice a new piece of information in the output:\n[...]\n# Groups:   continent [4]\n[...]\nThe output says that the data has been grouped by the column/variable continent. This means that any subsequent operation will be applied not to individual rows, like it was with our mutate() operations earlier, but they will now work “per continent”. You can imagine this as the group_by() function adding a tiny bit of invisible annotation data which instructs downstream functions to work per group.\nBefore we move on to computing summary statistics, let’s also note that we can, of course, group based on multiple columns. For instance, in our data, we should probably not summarize based on continent but also on age, splitting individuals based on the broad age sets like this:\n\ngroup_by(df, continent, set)\n\n# A tibble: 4,072 × 26\n# Groups:   continent, set [10]\n   sample  population country continent   age coverage set    site  region    \n   &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;     \n 1 NA18486 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 2 NA18488 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 3 NA18489 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 4 NA18498 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 5 NA18499 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 6 NA18501 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 7 NA18502 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 8 NA18504 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n 9 NA18505 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n10 NA18507 YRI        Nigeria Africa        0       NA Modern &lt;NA&gt;  WestAfrica\n# ℹ 4,062 more rows\n# ℹ 17 more variables: groupLabel &lt;chr&gt;, flag &lt;chr&gt;, latitude &lt;dbl&gt;,\n#   longitude &lt;dbl&gt;, dataSource &lt;chr&gt;, age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;,\n#   ageLow &lt;dbl&gt;, datingSource &lt;chr&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;, sex_desc &lt;chr&gt;, sex_set &lt;chr&gt;\n\n\nGrouping is just the first step though, and doesn’t do anything on its own…\n\n\n2. summarize()\nWhy is grouping introduced above even useful? The most simple use case is computing summary statistics on the data on a “per group” basis using the summarize() function. This function works a little bit like mutate(), because it creates new columns, but it only creates one row for each group.\nFor instance, we could compute the mean coverage of each individual in a group like this, and arrange the groups by the coverage available (try running this in your R console to see this in practice!):\n\ndf %&gt;%\n  group_by(continent, set) %&gt;%                   # first create groups\n  summarize(mean_coverage = mean(coverage)) %&gt;%  # then create summaries for each group\n  arrange(desc(mean_coverage))\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 10 × 3\n# Groups:   continent [4]\n   continent set     mean_coverage\n   &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;\n 1 Asia      Archaic         35.3 \n 2 Europe    Archaic         24.4 \n 3 Africa    Ancient          4.99\n 4 America   Ancient          2.67\n 5 Asia      Ancient          1.71\n 6 Europe    Ancient          1.41\n 7 Africa    Modern          NA   \n 8 America   Modern          NA   \n 9 Asia      Modern          NA   \n10 Europe    Modern          NA   \n\n\nNotice that the result has only three columns, unlike the many columns in the original df table! We have a column for each variable we grouped over, and we have the new column with our summary statistic. Additionally, we only have 10 rows, one row for each combination of continent and set. So, this is kind of similar to mutate() (it creates new variables/columns), but different.\n\nWhat does the NA value for “Modern” individuals in the output of the previous code chunk? Take a look at the original data df to answer this question.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nThe reason is simple – the present-day individuals are high quality imputed genotypes, which is why the sequencing coverage column is full of NA (not available, i.e., missing data) values. Computing a mean of such values is, itself, an NA value too:\n\nmean(c(1, NA, 2, NA, 42, NA))\n\n[1] NA\n\n\n\n\n\n\nsummarize() is even more powerful than that because we can compute many different things at the same time, for each group! This next exercise is an example of that.\nAs you know, the mean (such as the mean_coverage we computed right above) is computed as a sum of a set of values divided by their count. Use the group_by() and summarize() code exactly like you did above, but instead of computing the mean() directly, compute it manually by doing (in a summarize() call):\n\nsum_coverage = sum(coverage) (the sum of coverages for each group),\nn = n() (the number of values in each group),\nmean_coverage = sum_coverage / n (using the two quantities just computed to calculate the mean)\n\nTo make things a little tidier, arrange() the results by the column mean_coverage.\nOnce you have your results, save this new table in a variable called df_summary.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nHere’s our entire pipeline to compute the mean coverage “manually” instead of the built-in function mean() like we did in the previous exercise:\n\ndf_summary &lt;-\n  df %&gt;%                               # take the data frame df...\n  group_by(continent, set) %&gt;%         # ... group rows on continent and set...\n  summarize(                           # ... and compute for each group:\n    sum_coverage = sum(coverage),      #       1. the sum of all coverage\n    n = n(),                           #       2. the number of coverages\n    mean_coverage = sum_coverage / n   #       3. the mean using 1. and 2.above\n  ) %&gt;%\n  arrange(mean_coverage)               # ... then sort by average coverage\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\ndf_summary\n\n# A tibble: 10 × 5\n# Groups:   continent [4]\n   continent set     sum_coverage     n mean_coverage\n   &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;         &lt;dbl&gt;\n 1 Europe    Ancient       1722.   1220          1.41\n 2 Asia      Ancient        600.    350          1.71\n 3 America   Ancient        205.     77          2.67\n 4 Africa    Ancient         84.8    17          4.99\n 5 Europe    Archaic         24.4     1         24.4 \n 6 Asia      Archaic         70.6     2         35.3 \n 7 Africa    Modern          NA     504         NA   \n 8 America   Modern          NA     504         NA   \n 9 Asia      Modern          NA     993         NA   \n10 Europe    Modern          NA     404         NA   \n\n\n\n\n\n\nLook up the function ?ungroup. Why is it useful?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nAlthough grouping is very useful for computing summary statistics across groups of values, we often need to discard the grouping because we might want to continue working with the summarized table we obtained with other tidyverse functions without working on the pre-defined groups. For instance, we might want to follow up a summarize() operation with a mutate() operation, without working with groups.\n\n\n\n\nHow would you compute 95% confidence interval for the mean coverage using the group_by() and summarize()? Remember that you will need standard deviation (R function sd()) and also the number of observations (function n()). Look up confidence interval equation on Wikipedia.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nIf you’re here this means that you needed extra challenge, so I’m sure you figured it out. 😜 Reach out to me if you want to discuss the solution!\n\n\n\n\n\n\nslice_ functions\nFinally, in addition to computing various summaries on groups using summarize(), we have five “slicing” functions at our disposal, each of which extracts specific row(s) from each defined group. These functions are:\n\nslice_head(n = 1) takes the first row,\nslice_tail(n = 1) takes the last row,\nslice_min(&lt;column&gt;, n = 1) takes the row with the smallest value of ,\nslice_max(&lt;column&gt;, n = 1) takes the row with the largest value of ,\nslice_sample(&lt;col&gt; = 1) takes one random row.\n\n\nWhat is the lowest coverage individual in each continent and set group? While you’re doing this, filter() out the set of “Modern” individuals because they are not meaningful anyway (they have NA coverages), composing the entire command as a series of steps in a %&gt;% pipeline.\nAt the end of your %&gt;% pipeline, pipe your results into relocate(continent, set) to make everything even tidier.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\ndf %&gt;%\n  filter(set != \"Modern\") %&gt;%\n  group_by(continent, set) %&gt;%\n  slice_min(coverage, n = 1) %&gt;%\n  relocate(continent, set)\n\n# A tibble: 6 × 26\n# Groups:   continent, set [6]\n  continent set     sample       population country    age coverage site  region\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Africa    Ancient gun002       gun002     Canary…   860.   0.213  Tene… North…\n2 America   Ancient AM71         AM71       Chile     100    0.103  Stra… South…\n3 Asia      Ancient NEO916       NEO916     Russia   5727    0.0175 Veng… North…\n4 Asia      Archaic Denisova     Denisova   Russia  80000   25.8    Deni… North…\n5 Europe    Ancient NEO496       NEO496     Ukraine 10655    0.0124 Vasi… Centr…\n6 Europe    Archaic Vindija33.19 Vindija33… Croatia 41950   24.4    Vind… Centr…\n# ℹ 17 more variables: groupLabel &lt;chr&gt;, flag &lt;chr&gt;, latitude &lt;dbl&gt;,\n#   longitude &lt;dbl&gt;, dataSource &lt;chr&gt;, age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;,\n#   ageLow &lt;dbl&gt;, datingSource &lt;chr&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;, sex_desc &lt;chr&gt;, sex_set &lt;chr&gt;\n\n\n\n\n\n\nModify your %&gt;% pipeline for the exercise above to answer who is the oldest individual we have available on each continent?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\ndf %&gt;%\n  filter(set != \"Modern\") %&gt;%\n  group_by(continent) %&gt;%\n  slice_max(age) %&gt;%\n  relocate(continent, set, age)\n\n# A tibble: 4 × 26\n# Groups:   continent [4]\n  continent set        age sample       population country coverage site  region\n  &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Africa    Ancient   7885 I10871       I10871     Camero…     15.2 Shum… Centr…\n2 America   Ancient  12649 Clovis       Clovis     USA         15.1 Mont… North…\n3 Asia      Archaic 125000 AltaiNeande… AltaiNean… Russia      44.8 Deni… North…\n4 Europe    Archaic  41950 Vindija33.19 Vindija33… Croatia     24.4 Vind… Centr…\n# ℹ 17 more variables: groupLabel &lt;chr&gt;, flag &lt;chr&gt;, latitude &lt;dbl&gt;,\n#   longitude &lt;dbl&gt;, dataSource &lt;chr&gt;, age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;,\n#   ageLow &lt;dbl&gt;, datingSource &lt;chr&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;, sex_desc &lt;chr&gt;, sex_set &lt;chr&gt;",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "tidy-basics.html#exercise-13-writing-and-reading-tables",
    "href": "tidy-basics.html#exercise-13-writing-and-reading-tables",
    "title": "Introduction to tidyverse",
    "section": "Exercise 13: Writing (and reading) tables",
    "text": "Exercise 13: Writing (and reading) tables\nAbove you create a summary table df_summary. Let’s try writing it to disk using write_tsv(). The .tsv stands for “tab-separated values” (TSV) which is a file format similar to another file you might have heard about, the “comma-separated values” (CSV) file. They can be opened with Excel too, in case this is useful at some point, but they are very useful for computational workflows.\n\nLook up the help in ?write_tsv. What are the parameters you need to specify at minimum to save the file? This function has all of the useful options saved as defaults, so there’s not much you have to do. Still, look up the options!\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nThe only thing we really want to do is specify the data frame to write to disk and the filename:\n\nwrite_tsv(df_summary, \"~/Desktop/df_summary.tsv\")\n\n\n\n\n\nWhat are other means of writing files you found in ?write_tsv? How would you read a CSV file into R? What about other file formats?\nHint: Look up ?read_csv.\n\nA very useful R package is readxl, for reading and writing Excel files. Google this package, and install it with install.packages(\"readxl\"). If you want and have some Excel file on your computer, try reading it into R with library(readxl); read_excel(\"path/to/your/excel/file\").",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "tidy-basics.html#take-home-exercises",
    "href": "tidy-basics.html#take-home-exercises",
    "title": "Introduction to tidyverse",
    "section": "“Take home exercises”",
    "text": "“Take home exercises”\nWhich country has the highest number of aDNA samples (i.e., samples for which set == \"Ancient“). How many samples from Estonia, Lithuania, or Latvia do we have?\nHint: Use the group_by(country) and summarize() combo again, together with the n() helper function.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nCount the number of samples from each country, and order them by this count:\n\ncountries_df &lt;-\n  df %&gt;%\n  filter(set == \"Ancient\") %&gt;%\n  group_by(country) %&gt;%\n  summarise(n = n()) %&gt;%\n  arrange(desc(n))\n\ncountries_df\n\n# A tibble: 58 × 2\n   country        n\n   &lt;chr&gt;      &lt;int&gt;\n 1 Russia       283\n 2 Denmark      184\n 3 Sweden       174\n 4 Italy        149\n 5 UK            76\n 6 Kazakhstan    64\n 7 Estonia       60\n 8 Ukraine       60\n 9 Poland        56\n10 France        54\n# ℹ 48 more rows\n\n\n\ncountries_df %&gt;% filter(country %in% c(\"Estonia\", \"Lithuania\", \"Latvia\"))\n\n# A tibble: 3 × 2\n  country       n\n  &lt;chr&gt;     &lt;int&gt;\n1 Estonia      60\n2 Latvia        7\n3 Lithuania     1\n\n\n\n\n\n\nIs there any evidence (real or not — just out of curiousity!) of a potential bias in terms of how many set == \"Ancient\" samples do we have from a country and the geographical location of that country, such as the average longitude or latitude of samples from there? Compute a linear model using the lm function of the count the n count of samples from each country as a function of avg_lat or avg_lon (average latitude and longitude) of samples in each country (all of these quantities computed with group_by() and summarize() like you did above).\nHint: If you’re not familiar with this, look up the linear model function ?lm or perhaps this tutorial to see how you can build a linear regression between a response variable (like our count n) and an explanatory variable (such as average longitude or latitude of a country), as computed by the group_by() and summarize() combination (the result of which you should save to a new data frame variable, to be used in the lm() function).\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nLet’s first compute our summary statistics:\n\ncountries_df &lt;-\n  df %&gt;%\n  filter(set == \"Ancient\") %&gt;%\n  group_by(country) %&gt;%\n  summarise(n = n(),\n            avg_lat = mean(latitude, na.rm = TRUE),\n            avg_lon = mean(longitude, na.rm = TRUE))\n\ncountries_df\n\n# A tibble: 58 × 4\n   country            n avg_lat avg_lon\n   &lt;chr&gt;          &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Argentina          5  -50.2    -69.5\n 2 Armenia            6   40.1     45.3\n 3 Brazil             5  -19.5    -43.9\n 4 Cameroon           2    5.86    10.1\n 5 Canada             6   47.0    -94.1\n 6 Canary Islands     5   28.2    -16.2\n 7 Chile              4  -52.7    -72.9\n 8 China              7   43.6     93.2\n 9 CzechRepublic      9   49.9     15.0\n10 Denmark          184   55.6     11.0\n# ℹ 48 more rows\n\n\nCompute the linear regression of n as a function of avg_lon – there doesn’t appear to be any significant relationship between the two variables:\n\nlm_res &lt;- lm(n ~ avg_lon, data = countries_df)\nsummary(lm_res)\n\n\nCall:\nlm(formula = n ~ avg_lon, data = countries_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.785 -24.543 -21.474   7.008 254.543 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 28.822477   7.431781   3.878 0.000279 ***\navg_lon     -0.005886   0.123753  -0.048 0.962235    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52.45 on 56 degrees of freedom\nMultiple R-squared:  4.039e-05, Adjusted R-squared:  -0.01782 \nF-statistic: 0.002262 on 1 and 56 DF,  p-value: 0.9622\n\nplot(countries_df$avg_lon, countries_df$n)\nabline(lm_res)\n\n\n\n\n\n\n\n\nCompute the linear regression of n as a function of avg_lat – it looks like there’s a significant relationship between the number of samples from a country and geographical latitude? Is this a signal of aDNA surviving in more colder climates? :)\n\nlm_res &lt;- lm(n ~ avg_lat, data = countries_df)\nsummary(lm_res)\n\n\nCall:\nlm(formula = n ~ avg_lat, data = countries_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-41.085 -28.042 -13.834   7.972 242.851 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   8.2158    10.9508   0.750    0.456  \navg_lat       0.5848     0.2501   2.338    0.023 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 50.07 on 56 degrees of freedom\nMultiple R-squared:  0.08891,   Adjusted R-squared:  0.07264 \nF-statistic: 5.465 on 1 and 56 DF,  p-value: 0.023\n\nplot(countries_df$avg_lat, countries_df$n)\nabline(lm_res)\n\n\n\n\n\n\n\n\nDoes it mean anything? Hard to tell, but it does make for a fun exercise. :)\n\n\n\n\nTake your own data and play around with it using the concepts you learned above. If the data isn’t in a form that’s readily readable as a table with something like read_tsv(), please talk to me! tidyverse is huge and there are packages for munging all kinds of data. I’m happy to help you out.\nDon’t worry about “getting somewhere”. Playing and experimenting (and doing silly things) is the best way to learn.\nFor more inspiration on other things you could do with your data, take a look at the dplyr cheatsheet.\nIn each following session, you’ll have the opportunity to do the same. The next session will focus on more real-world insights from data, and the section after that will focus on creating beautiful visualizations using the ggplot2 package.",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "tidy-basics.html#closing-remarks",
    "href": "tidy-basics.html#closing-remarks",
    "title": "Introduction to tidyverse",
    "section": "Closing remarks",
    "text": "Closing remarks\nHopefully you’re now learning that really integrating these couple of tidyverse “verbs” gives you an entirely new natural way of not just working and modifying and filtering data, but “thinking in data” (as pretentious and hippy this must seem to you right now).\nWhen you watch experts doing data science using tidyverse in real time, like an experienced colleague helping you out, you will see that they can “think about the data” at the same time as they are typing tidyverse commands. Over time, with practice, you will get there too.\nAnd this is actually just the beginning! The true power of data science arrives when we learn ggplot2 visualization later. ggplot2 uses the same philosophical principles ad the dplyr commands we practiced above, but allows you to “visualize information at the same time as you’re thinking about it”.",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "tidy-advanced.html",
    "href": "tidy-advanced.html",
    "title": "More tidyverse practice",
    "section": "",
    "text": "Tip on doing interactive data science\nIn this chapter we will introduce new (and reinforce already familiar) tidyverse concepts on another real-world data set, related to the metadata we looked previously. We will build the components of a processing and filtering (and in the next session, visualization) pipeline for a new Identity-by-Descent (IBD) data set that I recently got my hands on. We will proceed step by step, developing new components to the processing and analytical pipeline and learning how tidyverse can be useful in a very practical way, before we learn how to wrap it all up in a nice, structured, reproducible way in later parts of our workshop.\nThis is a real world example! You will be retracing the steps I had to work through in my job in August 2025 where I got sent a completely unfamiliar new data set of IBD segments and asked to do some science with it.\nWhy do this tidyverse stuff again? We just did this with the metadata table.\nA few reasons:\nAgain, here is how the start of our solutions script for this session will look like. Same as before:\nRemember that the best way can solve any R problem is by building up a solution one little step at a time. First work in the R console, try bits and pieces of code interactively, and only when you’re sure you understand the problem (and solution) you can finalize it by writing it in your solution script (often saving the result in a variable for later use).\nThis helps a huge deal to avoid feeling overwhelmed by what can initially seem like a problem that’s too hard!\nLet’s also read in data which we will be using in these exercises. These are coordinates of identity-by-descent (IBD) segments between pairs of individuals in a huge aDNA study we’ve already talked about. This data is big and quite complex — I prefiltered it to contain only IBD segments for chromosome 21. The IBD data should correspond to individuals in the metadata we worked with in the previous session.\nibd_segments &lt;- read_tsv(\"https://tinyurl.com/simgen-ibd-segments\")\n\nRows: 461858 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (3): sample1, sample2, rel\ndbl (3): chrom, start, end\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nAnd we also need to read the corresponding metadata, with which you are already very closely familiar with:\nmetadata_all &lt;- read_tsv(\"https://tinyurl.com/simgen-metadata\")\n\nRows: 4072 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (16): sampleId, popId, site, country, region, continent, groupLabel, gro...\ndbl  (8): latitude, longitude, age14C, ageHigh, ageLow, ageAverage, coverage...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nNote: I call it metadata_all because later we will be working with a smaller section of it, for more clarity. Therefore, metadata_all will be the original, unprocessed data set. It’s often very useful to keep raw data in our session like this.\nCreate a new R script in RStudio, (File -&gt; New file -&gt; R Script) and save it somewhere on your computer as tidy-advanced.R (File -&gt; Save). Copy the chunks of code above into it (the library() and read_tsv() commands and let’s get started)!",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More _tidyverse_ practice</span>"
    ]
  },
  {
    "objectID": "tidy-advanced.html#real-world-story",
    "href": "tidy-advanced.html#real-world-story",
    "title": "More tidyverse practice",
    "section": "Real world story",
    "text": "Real world story\nSo, the story is that you just got a new data set from a bioinformatics software, in this case the IBDseq software for detecting IBD segments between pairs of individuals. Before you proceed with doing any kind of statistical analysis you need to do two things:\n\nExploring the data to see what is it that you just got. Never trust the data you’ve been given blindly and always do many sanity checks! In the process, you will gain more confidence in it, and also learn its ins and outs, which will be useful either way.\nFiltering and processing it in a way which will make your work easier.\n\nLet’s tackle step 1 first.",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More _tidyverse_ practice</span>"
    ]
  },
  {
    "objectID": "tidy-advanced.html#exercise-1-exploring-new-data",
    "href": "tidy-advanced.html#exercise-1-exploring-new-data",
    "title": "More tidyverse practice",
    "section": "Exercise 1: Exploring new data",
    "text": "Exercise 1: Exploring new data\nAgain, the first step is always to familiarize yourself with the basic structure of the data. Which columns does the IBD data have? What’s the format of the data? What ranges or distributions of columns’ values do you have, and with what data types? Do you have information for the entire genome (all chromosomes)?\nHint: head(), colnames(), glimpse(), str(), table(), summary() which are either applicable to an entire data frame, or a specific column (column vectors can be extracted with the $ operator, of course).\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nhead(ibd_segments)\n\n# A tibble: 6 × 6\n  sample1 sample2 chrom start   end rel  \n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1 VK548   NA20502    21  58.3  62.1 none \n2 VK528   HG01527    21  24.8  27.9 none \n3 PL_N28  NEO36      21  14.0  16.5 none \n4 NEO556  HG02230    21  13.1  14.0 none \n5 HG02399 HG04062    21  18.8  21.0 none \n6 EKA1    NEO220     21  30.1  35.8 none \n\n\n\ncolnames(ibd_segments)\n\n[1] \"sample1\" \"sample2\" \"chrom\"   \"start\"   \"end\"     \"rel\"    \n\n\n\nglimpse(ibd_segments)\n\nRows: 461,858\nColumns: 6\n$ sample1 &lt;chr&gt; \"VK548\", \"VK528\", \"PL_N28\", \"NEO556\", \"HG02399\", \"EKA1\", \"1965…\n$ sample2 &lt;chr&gt; \"NA20502\", \"HG01527\", \"NEO36\", \"HG02230\", \"HG04062\", \"NEO220\",…\n$ chrom   &lt;dbl&gt; 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21…\n$ start   &lt;dbl&gt; 58.325139, 24.809766, 14.011766, 13.051847, 18.777295, 30.1228…\n$ end     &lt;dbl&gt; 62.077452, 27.851017, 16.454424, 14.014276, 20.985679, 35.8404…\n$ rel     &lt;chr&gt; \"none\", \"none\", \"none\", \"none\", \"none\", \"none\", \"none\", \"none\"…\n\n\nWe have information just for chromosome 1 to make the data set a little smaller and easier for your laptops to handle.\n\ntable(ibd_segments$chrom)\n\n\n    21 \n461858 \n\n\n\n\n\n\nAs a sanity check, find out if you have metadata information for every individual in the sample1 and sample2 columns of the IBD table. What about the other way around – do all individuals in the metadata have some IBD relationship to another individual in the IBD segments table? If not, find out which individuals are these.\nThis is another sort of sanity checking you will be doing all the time. We can only analyze data for which we have metadata information (population assignment, geographical location, dating information), so let’s make sure we have what we need.\nHint: Another way to phrase this question is this: does every sample that appears in either sample1 or sample2 column of ibd_segments have a corresponding row in the sampleId column of metadata_all? Note that you can use the function unique() to get all values unique in a given vector (i.e., unique(ibd_segments$sample1) gives all individuals in the sample1 column, which has otherwise many duplicated entries). And remember the existence of the all() function, and the %in% operator, which can check if unique(ibd_segments$sample1) is in metadata_all$sampleId.\nHint: If this still doesn’t make sense, please work through my solution directly and try to understand it bit by bit! This is a bit more advanced than our previous tidyverse exercises.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nWe can get unique values across a combination of vectors by first binding everything together using c(ibd$sample1, ibd$sample2) which gets us a single vector, then calling unique() on that:\n\nibd_samples &lt;- unique(c(ibd_segments$sample1, ibd_segments$sample2))\n\nmetadata_samples &lt;- metadata_all$sampleId\n\nThen we can use our well-known operator %in% to check that every sample in the IBD data has a representative in the metadata:\n\nall(ibd_samples %in% metadata_samples)\n\n[1] TRUE\n\n\nThis is a good sign! We’re not missing any information about anyone we have an IBD segment for! Otherwise, we would have trouble analyzing such person’s IBD patterns across time, geography, populations, etc.\nHow about the other way around? Note that this is not the same operation, although it might look similar superficially:\n\nall(metadata_samples %in% ibd_samples)\n\n[1] FALSE\n\n\nThe FALSE answer tells us that there are some individuals in our metadata who are not participating in any pairwise IBD sharing. Who are these?\n\nmetadata_all[!metadata_samples %in% ibd_samples, ]\n\n# A tibble: 2 × 24\n  sampleId popId    site      country region continent groupLabel groupAge flag \n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;\n1 NEO962   NEO962   Dragsholm Denmark North… Europe    Denmark_M… Ancient  0    \n2 Denisova Denisova Denisova… Russia  North… Asia      Denisova_… Archaic  0    \n# ℹ 15 more variables: latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, dataSource &lt;chr&gt;,\n#   age14C &lt;dbl&gt;, ageHigh &lt;dbl&gt;, ageLow &lt;dbl&gt;, ageAverage &lt;dbl&gt;,\n#   datingSource &lt;chr&gt;, coverage &lt;dbl&gt;, sex &lt;chr&gt;, hgMT &lt;chr&gt;, gpAvg &lt;dbl&gt;,\n#   ageRaw &lt;chr&gt;, hgYMajor &lt;chr&gt;, hgYMinor &lt;chr&gt;\n\n\n\n\n\nWith the basic sanity check above, we can be sure that our IBD segments table can be co-analyzed with the information in the metadata and that we won’t be missing anything important.",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More _tidyverse_ practice</span>"
    ]
  },
  {
    "objectID": "tidy-advanced.html#exercise-2-ibd-processing",
    "href": "tidy-advanced.html#exercise-2-ibd-processing",
    "title": "More tidyverse practice",
    "section": "Exercise 2: IBD processing",
    "text": "Exercise 2: IBD processing\nAdd a new column length to the ibd_segments data frame using the mutate() function, which will contain the length of each IBD segment in centimorgans (end - start). Save the data frame that mutate() returns back to the variable ibd_segments.\nNote: Sometimes saving new things to already-defined variables leads to very messy code and its cleaner to create new variables for nex results. In this instance, we’re basically building up a processing pipeline whose purpose is to filter / mutate / clean a data frame for downstream use. In fact, later we will add individual steps of this pipeline name in several subsequent steps.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nibd_segments &lt;- ibd_segments %&gt;% mutate(length = end - start)\n\nibd_segments\n\n# A tibble: 461,858 × 7\n   sample1 sample2 chrom start   end rel   length\n   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1 VK548   NA20502    21  58.3  62.1 none   3.75 \n 2 VK528   HG01527    21  24.8  27.9 none   3.04 \n 3 PL_N28  NEO36      21  14.0  16.5 none   2.44 \n 4 NEO556  HG02230    21  13.1  14.0 none   0.962\n 5 HG02399 HG04062    21  18.8  21.0 none   2.21 \n 6 EKA1    NEO220     21  30.1  35.8 none   5.72 \n 7 19651   HG01941    21  49.0  51.3 none   2.24 \n 8 HG02181 HG04033    21  13.1  13.9 none   0.868\n 9 VK328   HG01438    21  60.7  62.8 none   2.04 \n10 F004    HG03616    21  25.4  27.6 none   2.15 \n# ℹ 461,848 more rows\n\n\nNote that you don’t always use a tidyverse approach. Sometimes doing a simple thing using a simple method is just… simpler:\n\n# we first make a copy of the original data\nibd_segments &lt;- ibd_segments\n# and then add the new column\nibd_segments$length &lt;- ibd_segments$end - ibd_segments$start\n\nibd_segments\n\n# A tibble: 461,858 × 7\n   sample1 sample2 chrom start   end rel   length\n   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1 VK548   NA20502    21  58.3  62.1 none   3.75 \n 2 VK528   HG01527    21  24.8  27.9 none   3.04 \n 3 PL_N28  NEO36      21  14.0  16.5 none   2.44 \n 4 NEO556  HG02230    21  13.1  14.0 none   0.962\n 5 HG02399 HG04062    21  18.8  21.0 none   2.21 \n 6 EKA1    NEO220     21  30.1  35.8 none   5.72 \n 7 19651   HG01941    21  49.0  51.3 none   2.24 \n 8 HG02181 HG04033    21  13.1  13.9 none   0.868\n 9 VK328   HG01438    21  60.7  62.8 none   2.04 \n10 F004    HG03616    21  25.4  27.6 none   2.15 \n# ℹ 461,848 more rows",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More _tidyverse_ practice</span>"
    ]
  },
  {
    "objectID": "tidy-advanced.html#exercise-3-metadata-processing",
    "href": "tidy-advanced.html#exercise-3-metadata-processing",
    "title": "More tidyverse practice",
    "section": "Exercise 3: Metadata processing",
    "text": "Exercise 3: Metadata processing\nWe have read our IBD table and added a new useful column length to it, so let’s proceed with the metadata. You already became familiar with it in the previous session—the goal here now is to transform it to a form more useful for downstream data co-analysis with the genomic IBD data.\nThink of it this way: metadata contains all relevant variables for each individual in our data (age, location, population, country, etc.). IBD contains only genomic information. We eventually need to merge those two sets of information together to ask questions about population history and biology.\n\nFirst, there’s much more information than we need for now, just take a look again:\n\ncolnames(metadata_all)\n\n [1] \"sampleId\"     \"popId\"        \"site\"         \"country\"      \"region\"      \n [6] \"continent\"    \"groupLabel\"   \"groupAge\"     \"flag\"         \"latitude\"    \n[11] \"longitude\"    \"dataSource\"   \"age14C\"       \"ageHigh\"      \"ageLow\"      \n[16] \"ageAverage\"   \"datingSource\" \"coverage\"     \"sex\"          \"hgMT\"        \n[21] \"gpAvg\"        \"ageRaw\"       \"hgYMajor\"     \"hgYMinor\"    \n\n\nRealistically, given that our interest here is pairwise IBD segments, a lot of this metadata information is quite useless at this stage. Let’s make the data a bit smaller and easier to look at at a glance.\n\nUse select() a subset of the metadata with the following columns and store it in a new variable metadata (not back to metadata_all! we don’t want to overwrite the original big table metadata_all in case we need to refer to it a bit later): sampleId, country, continent, ageAverage. Then rename sampleId to sample, popId to pop, and ageAverage to age just to save ourselves some typing later.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nmetadata &lt;- select(metadata_all, sample = sampleId, country, continent, age = ageAverage)\n\nmetadata\n\n# A tibble: 4,072 × 4\n   sample  country continent   age\n   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n 1 NA18486 Nigeria Africa       NA\n 2 NA18488 Nigeria Africa       NA\n 3 NA18489 Nigeria Africa       NA\n 4 NA18498 Nigeria Africa       NA\n 5 NA18499 Nigeria Africa       NA\n 6 NA18501 Nigeria Africa       NA\n 7 NA18502 Nigeria Africa       NA\n 8 NA18504 Nigeria Africa       NA\n 9 NA18505 Nigeria Africa       NA\n10 NA18507 Nigeria Africa       NA\n# ℹ 4,062 more rows\n\n\n\n\n\n\nJust as you did in the previous chapter, use mutate() and if_else() inside the mutate() call to make sure that the “modern” individuals have the age set to 0, instead of NA (and everyone else’s age stays the same). In other words, for rows where age is NA, replace that NA with 0. Yes, you did this before, but try not to copy-paste your solution. Try doing this on your own first.\nHint: Remember the is.na() bit we used before! Try it on the age column vector if you need a reminder: is.na(metadata$age).\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nmetadata &lt;- mutate(metadata, age = if_else(is.na(age), 0, age))\n\nLet’s make sure we haven’t missed anything else:\n\nall(!is.na(metadata$age))\n\n[1] TRUE\n\n\n\n\n\n\nOur analyses will exclusively focus on modern humans. Filter out the three archaics in the metadata, saving the results into the same metadata variable again. As a reminder, these are individuals whose sample name is among c(\"Vindija33.19\", \"AltaiNeandertal\", \"Denisova\") which you can test in a filter() command using the %in% operator.\nHint: Remember that you can get a TRUE / FALSE indexing vector (remember our R bootcamp session!) by not only column %in% c(... some values...) but you can also do the opposite test as !column %in% c(... some values...) (notice the ! operator in the second bit of code).\nExperiment with the filter() command in your R console first if you want to check that your filtering result is correct.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nmetadata &lt;- filter(metadata, !sample %in% c(\"Vindija33.19\", \"AltaiNeandertal\", \"Denisova\"))",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More _tidyverse_ practice</span>"
    ]
  },
  {
    "objectID": "tidy-advanced.html#status-of-our-data-so-far",
    "href": "tidy-advanced.html#status-of-our-data-so-far",
    "title": "More tidyverse practice",
    "section": "Status of our data so far",
    "text": "Status of our data so far\nWe now have a cleaner IBD table looking like this:\n\nhead(ibd_segments)\n\n# A tibble: 6 × 7\n  sample1 sample2 chrom start   end rel   length\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 VK548   NA20502    21  58.3  62.1 none   3.75 \n2 VK528   HG01527    21  24.8  27.9 none   3.04 \n3 PL_N28  NEO36      21  14.0  16.5 none   2.44 \n4 NEO556  HG02230    21  13.1  14.0 none   0.962\n5 HG02399 HG04062    21  18.8  21.0 none   2.21 \n6 EKA1    NEO220     21  30.1  35.8 none   5.72 \n\n\nAnd here’s our metadata information:\n\nhead(metadata)\n\n# A tibble: 6 × 4\n  sample  country continent   age\n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n1 NA18486 Nigeria Africa        0\n2 NA18488 Nigeria Africa        0\n3 NA18489 Nigeria Africa        0\n4 NA18498 Nigeria Africa        0\n5 NA18499 Nigeria Africa        0\n6 NA18501 Nigeria Africa        0\n\n\n\nIn our original metadata_all table we have the groupAge column with these values:\n\ntable(metadata_all$groupAge)\n\n\nAncient Archaic  Modern \n   1664       3    2405 \n\n\nNote that in the new metadata we removed it because it’s not actually useful for most data analysis purposes (it has only three values). For instance, later we might want to do some fancier analyses, looking at IBD as a time series, not just across basically two temporal categories, “young” and “old”.\nTo this end, let’s create more useful time-bin categories! This will require a bit more code than above solutions. Don’t feel overwhelmed! I will first introduce a useful function using a couple of examples for you to play around with in your R console. Only then we will move on to an exercise in which you will try to implement this on the full metadata table! For now, keep on reading and experimenting in your R console!\nIt will be worth it, because this kind of binning is something we do all the time in computational biology, in every project.\n\nLet’s introduce an incredibly useful function called cut(). Take a look at ?cut help page and skim through it to figure out what it does. As a bit of a hint, we will want to add a new metadata column which will indicate in which age bin (maybe, split in steps of 5000 years) do our individuals belong to.\nHere’s a small example to help us get started. Let’s pretend for a moment that the df variable created below is a example representative of our full-scale big metadata table. Copy this into your script, so that you can experiment with the cut() technique in this section.\n\n# a toy example data frame mimicking the age column in our huge metadata table\ndf &lt;- data.frame(\n  sample = c(\"a\", \"b\", \"c\", \"d\", \"x\", \"y\", \"z\", \"q\", \"w\", \"e\", \"r\", \"t\"),\n  age = c(0, 0, 1000, 0, 5000, 10000, 7000, 13000, 18000, 21000, 27000, 30000)\n)\ndf\n\n   sample   age\n1       a     0\n2       b     0\n3       c  1000\n4       d     0\n5       x  5000\n6       y 10000\n7       z  7000\n8       q 13000\n9       w 18000\n10      e 21000\n11      r 27000\n12      t 30000\n\n\nLet’s do the time-binning now! Think about what the cut() function does here based on the result it gives you on the little example code below. You can pretend for now that the ages variable corresponds to age of our samples in the huge metadata table. Again, try running this on your own to see what happens when you run this code in your R console. Particularly, take a look at the format of the new column age_bin:\n\n# let's first generate the breakpoints for our bins (check out `?seq` if\n# you're confused by this!)\nbreakpoints &lt;- seq(0, 50000, by = 5000)\n# take a look at the breakpoints defined:\nbreakpoints\n##  [1]     0  5000 10000 15000 20000 25000 30000 35000 40000 45000 50000\n\n# create a new column of the data frame, containing bins of age using our breakpoints\ndf$age_bin &lt;- cut(df$age, breaks = breakpoints)\n# take a look at the data frame (with the new column `age_bin` added)\ndf\n##    sample   age         age_bin\n## 1       a     0            &lt;NA&gt;\n## 2       b     0            &lt;NA&gt;\n## 3       c  1000       (0,5e+03]\n## 4       d     0            &lt;NA&gt;\n## 5       x  5000       (0,5e+03]\n## 6       y 10000   (5e+03,1e+04]\n## 7       z  7000   (5e+03,1e+04]\n## 8       q 13000 (1e+04,1.5e+04]\n## 9       w 18000 (1.5e+04,2e+04]\n## 10      e 21000 (2e+04,2.5e+04]\n## 11      r 27000 (2.5e+04,3e+04]\n## 12      t 30000 (2.5e+04,3e+04]\n\nThe function cut() is extremely useful whenever you want to discretize some continuous variable in bins (basically, a little similar to what a histogram does in context of plotting). Doing statistics on this kind of binned data is something we do very often. So never forget that cut() is there to help you! For example, you could also use the same concept to partition samples into categories based on “low coverage”, “medium coverage”, “high coverage”, etc.\n\nIn the example of the cut() function right above, what is the data type of the column age_bin created by the cut() function? Use glimpse(df) to see this data type, then skim through the documentation of ?factor. What is a “factor” according to this documentation?\nNote: This is a challenging topic. Please don’t stress. When we’re done I’ll walk through everything myself, interactively, and will explain things in detail. Take this information on “factors” as something to be aware of and a concept to be introduced to, but not something to necessarily be an expert on!\n\nHaving learned about ?factor above, consider the two following vectors and use them for experimentation when coming up with answers. What do you see when you print them out in your R console (by typing x1 and x2)? And what happens when you apply the typeof() function on both of them? x2 gives you a strange result — why? What do you get when you run the following command levels(x2)? What do you get when you run as.character(x2)?\n\nx1 &lt;- c(\"hello\", \"hello\", \"these\", \"are\", \"characters/strings\")\nx2 &lt;- factor(x1)\n\nIn applying cut() to the toy data frame df, why is the age_bin value equal to NA for some of the rows?\nNote: The discussion of “factors” should’ve been technically part of our R bootcamp chapter, on the topic of “data types”. However, that section was already too technical, so I decided to move it here to the data analysis section, because I think it makes more sense to explain it in a wider context.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nYou can see that both vectors look the same, but the second has additional information about “levels”:\n\nx1\n\n[1] \"hello\"              \"hello\"              \"these\"             \n[4] \"are\"                \"characters/strings\"\n\nx2\n\n[1] hello              hello              these              are               \n[5] characters/strings\nLevels: are characters/strings hello these\n\n\nThe reason you get the typeof(x1) as “character” but typeof(x2) as “integer” although both are “basically strings” is because factors are a special data type for encoding categorical variables which can only have a set of fixed possible values. Internally, for efficiency, levels of a factor variables are stored as integers, and their values or just “labels” for those integers.\nImportantly, factor levels are actually ordered, which is very useful for plotting, as we will see later, because the plots maintain order of factors when visualized.\nFor now, don’t worry about this too much. We needed to introduce the concept of factors because they are very frequently used whenever we need to bin continuous data, like we will now for assigning samples into bins based on their age value.\n\nOh, and the answer to why cut() gave us the NA for every 0 value is because of its include.lowest argument is set to FALSE. Compare these two:\n\ncut(df$age, breaks = seq(0, 50000, by = 10000))\n\n [1] &lt;NA&gt;          &lt;NA&gt;          (0,1e+04]     &lt;NA&gt;          (0,1e+04]    \n [6] (0,1e+04]     (0,1e+04]     (1e+04,2e+04] (1e+04,2e+04] (2e+04,3e+04]\n[11] (2e+04,3e+04] (2e+04,3e+04]\n5 Levels: (0,1e+04] (1e+04,2e+04] (2e+04,3e+04] ... (4e+04,5e+04]\n\n\n\ncut(df$age, breaks = seq(0, 50000, by = 10000), include.lowest = TRUE)\n\n [1] [0,1e+04]     [0,1e+04]     [0,1e+04]     [0,1e+04]     [0,1e+04]    \n [6] [0,1e+04]     [0,1e+04]     (1e+04,2e+04] (1e+04,2e+04] (2e+04,3e+04]\n[11] (2e+04,3e+04] (2e+04,3e+04]\n5 Levels: [0,1e+04] (1e+04,2e+04] (2e+04,3e+04] ... (4e+04,5e+04]\n\n\nHowever, this isn’t what we want, because we want to treat present-day individuals separately as their own category, not include them with other ancient people less than 5000 years old.\n\nThe levels() function is useful for getting the “labels” of the categories as they are presented to you in various printouts in the console (rather than their internal numerical encoding).\nConsider the original vector x2 (note the duplicated “hello” value):\n\nx2\n\n[1] hello              hello              these              are               \n[5] characters/strings\nLevels: are characters/strings hello these\n\n\nAnd the levels() output:\n\nlevels(x2)\n\n[1] \"are\"                \"characters/strings\" \"hello\"             \n[4] \"these\"             \n\n\nWhen we run as.character(), we effectively convert the (categorical) factor values into normal strings (i.e., basically convert those values into their plain labels). This is very useful whenever we want to manipulate factors, as we’ll se below:\n\nas.character(x2)\n\n[1] \"hello\"              \"hello\"              \"these\"             \n[4] \"are\"                \"characters/strings\"\n\n\n\n\n\n\nThe scientific notation format of bin labels with 5+e3 etc. is very annoying to look at. How can you use the dig.lab = argument of the cut() functions to make this prettier? Experiment in the R console to figure this out, then modify the df$age_bin &lt;- cut(df$age, breaks = breakpoints) command in your script accordingly.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nLet’s experiment first:\n\n# this is where we started\ncut(df$age, breaks = breakpoints)\n\n [1] &lt;NA&gt;            &lt;NA&gt;            (0,5e+03]       &lt;NA&gt;           \n [5] (0,5e+03]       (5e+03,1e+04]   (5e+03,1e+04]   (1e+04,1.5e+04]\n [9] (1.5e+04,2e+04] (2e+04,2.5e+04] (2.5e+04,3e+04] (2.5e+04,3e+04]\n10 Levels: (0,5e+03] (5e+03,1e+04] (1e+04,1.5e+04] ... (4.5e+04,5e+04]\n\n\n\n# this doesn't do it\ncut(df$age, breaks = breakpoints, dig.lab = 3)\n\n [1] &lt;NA&gt;            &lt;NA&gt;            (0,5e+03]       &lt;NA&gt;           \n [5] (0,5e+03]       (5e+03,1e+04]   (5e+03,1e+04]   (1e+04,1.5e+04]\n [9] (1.5e+04,2e+04] (2e+04,2.5e+04] (2.5e+04,3e+04] (2.5e+04,3e+04]\n10 Levels: (0,5e+03] (5e+03,1e+04] (1e+04,1.5e+04] ... (4.5e+04,5e+04]\n\n\n\n# but this does!\ncut(df$age, breaks = breakpoints, dig.lab = 10)\n\n [1] &lt;NA&gt;          &lt;NA&gt;          (0,5000]      &lt;NA&gt;          (0,5000]     \n [6] (5000,10000]  (5000,10000]  (10000,15000] (15000,20000] (20000,25000]\n[11] (25000,30000] (25000,30000]\n10 Levels: (0,5000] (5000,10000] (10000,15000] (15000,20000] ... (45000,50000]\n\n\nOur solution to get rid of the ugly scientific notation in our labels is therefore:\n\ndf$age_bin &lt;- cut(df$age, breaks = breakpoints, dig.lab = 10)\n\ndf\n\n   sample   age       age_bin\n1       a     0          &lt;NA&gt;\n2       b     0          &lt;NA&gt;\n3       c  1000      (0,5000]\n4       d     0          &lt;NA&gt;\n5       x  5000      (0,5000]\n6       y 10000  (5000,10000]\n7       z  7000  (5000,10000]\n8       q 13000 (10000,15000]\n9       w 18000 (15000,20000]\n10      e 21000 (20000,25000]\n11      r 27000 (25000,30000]\n12      t 30000 (25000,30000]\n\n\nMuch nicer to look at and immediately readable!\n\n\n\n\nYou have now learned that cut() has an optional argument called include.lowest =, which includes the lowest value of 0 (representing the “present-day” age of our samples) in the lowest bin [0, 5]. However, in the case of our assignment of samples from present-day, this is not what we want. We want present-day individuals to have their own category called “present-day”.\nHere’s a useful bit of code I use often for this exact purpose, represented as a complete self-contained chunk of the time-binning code. If we start from the original toy example data frame (with the NA values assigned to present-day ages of 0):\n\ndf\n\n   sample   age       age_bin\n1       a     0          &lt;NA&gt;\n2       b     0          &lt;NA&gt;\n3       c  1000      (0,5000]\n4       d     0          &lt;NA&gt;\n5       x  5000      (0,5000]\n6       y 10000  (5000,10000]\n7       z  7000  (5000,10000]\n8       q 13000 (10000,15000]\n9       w 18000 (15000,20000]\n10      e 21000 (20000,25000]\n11      r 27000 (25000,30000]\n12      t 30000 (25000,30000]\n\n\nWe can fix this by the following three-step process, described in the comments. Don’t stress about any of this! Just run this code on your own and try to relate the # text description of each step in comments to the code itself. Every thing that’s being done here are bits and pieces introduced above.\n\n# 0. assign each row to a bin based on given breakpoints\ndf$age_bin &lt;- cut(df$age, breaks = breakpoints, dig.lab = 10)\n\n# 1. extract labels (no \"present-day\" category yet)\nbin_levels &lt;- levels(df$age_bin)\n\ndf &lt;-\n  df %&gt;%\n  mutate(\n    age_bin = as.character(age_bin),  # 2. convert factors back to \"plain strings\"\n    age_bin = if_else(is.na(age_bin), \"present-day\", age_bin), # 3. replace NA with \"present-day\",\n    age_bin = factor(age_bin, levels = c(\"present-day\", bin_levels)) # 4. convert to factor (to maintain order of levels)\n  )\n\nPlease run the code above bit by bit, inspecting the variables you create (or modify) after each step. Remember the CTRL / CMD + Enter shortcut for stepping through code!\nWhen we print the modified df table, we see all bins properly assigned now, including the present-day samples with ages at 0:\n\ndf\n\n   sample   age       age_bin\n1       a     0   present-day\n2       b     0   present-day\n3       c  1000      (0,5000]\n4       d     0   present-day\n5       x  5000      (0,5000]\n6       y 10000  (5000,10000]\n7       z  7000  (5000,10000]\n8       q 13000 (10000,15000]\n9       w 18000 (15000,20000]\n10      e 21000 (20000,25000]\n11      r 27000 (25000,30000]\n12      t 30000 (25000,30000]\n\n\nThis is a very useful pattern which you will get to practice now on the full metadata table, by literally applying the same code on the big table.\n\nNow that you’re familiar with the cut() technique for binning values, use the functions mutate() and cut() again (in exactly the same way as we did on the df table in the code chunk above!) to create a new column age_bin this time on the whole metadata table. The new column should carry a category age of each individual in steps of 10000 years again.\nHint: As we did above, first assign the age_bin column using the cut() function, then modify the column accordingly with the mutate() snippet to set “present-day” in the age_bin for all present-day individuals.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nYou can match the step numbers to our example code applied on the smaller table df above. They do the same thing!\n\n# step 0. we first bin samples according to their age\nbin_step &lt;- 10000\nmetadata$age_bin &lt;- cut(metadata$age, breaks = seq(0, 50000, by = bin_step), dig.lab = 10)\n\n# step 1. get the assigned bin labels\nbin_levels &lt;- levels(metadata$age_bin)\n\n# then we modify the bins to include \"present-day\" labels:\nmetadata &lt;- metadata %&gt;%\n  mutate(\n    age_bin = as.character(age_bin), # step 2.\n    age_bin = if_else(is.na(age_bin), \"present-day\", age_bin), # step 3.\n    age_bin = factor(age_bin, levels = c(\"present-day\", bin_levels)) # step 4.\n  )\n\nWe can check the binning result to see that we’ve been successful!\n\ntable(metadata$age_bin)\n\n\n  present-day     (0,10000] (10000,20000] (20000,30000] (30000,40000] \n         2405          1623            31             2             7 \n(40000,50000] \n            1 \n\n\nThe reason we converted the age_bin back to factor in the end is that we want to maintain a temporal order of our labels (first “present-day”, then all other subsequent time bins), regardless of how mixed our rows are.\n\n\n\n\nHow many individuals did we end up with in each age_bin? Use table() to answer this question (and also to sanity check your final results), to make sure everything worked correctly.",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More _tidyverse_ practice</span>"
    ]
  },
  {
    "objectID": "tidy-advanced.html#recipe-for-merging-metadata",
    "href": "tidy-advanced.html#recipe-for-merging-metadata",
    "title": "More tidyverse practice",
    "section": "Recipe for merging metadata",
    "text": "Recipe for merging metadata\nAfter basic processing and filtering of the primary data (in our case, the genomic coordinates of pairwise IBD segments), we often need to annotate this data with some meta information… which is what we already processed too! What does this mean? Take a look at our IBD data in its current state again:\n\nhead(ibd_segments)\n\n# A tibble: 6 × 7\n  sample1 sample2 chrom start   end rel   length\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 VK548   NA20502    21  58.3  62.1 none   3.75 \n2 VK528   HG01527    21  24.8  27.9 none   3.04 \n3 PL_N28  NEO36      21  14.0  16.5 none   2.44 \n4 NEO556  HG02230    21  13.1  14.0 none   0.962\n5 HG02399 HG04062    21  18.8  21.0 none   2.21 \n6 EKA1    NEO220     21  30.1  35.8 none   5.72 \n\n\nWhen we analyze things down the line, we might want to look at IBD patterns over space and time, look at some temporal changes, etc. However, our IBD data has none of the information in it! Besides names of individuals (from which population, at which time, which geographical location?) we have nothing, so even if we were to do run our friends group_by() and summarize() to get some summary statistics to get some insights into the history or biological relationships between our samples, we have no idea to correlate them to other variables… precisely because those variables are elsewhere in a completely different table we called metadata.\nWe now need to put both sources of information into a single joint format. I call this “metadata merging”, and it is something we always want to do regardless of what kinds of statistics or measures we work with.\nThe technique below is again a very useful recipe for your future work.\nAgain, the annotation that we need is in the metadata table — it has information about ages and geography:\n\nhead(metadata)\n\n# A tibble: 6 × 5\n  sample  country continent   age age_bin    \n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;fct&gt;      \n1 NA18486 Nigeria Africa        0 present-day\n2 NA18488 Nigeria Africa        0 present-day\n3 NA18489 Nigeria Africa        0 present-day\n4 NA18498 Nigeria Africa        0 present-day\n5 NA18499 Nigeria Africa        0 present-day\n6 NA18501 Nigeria Africa        0 present-day\n\n\nWhat we need to do is join our IBD data with a selection of the most important variables in our metadata, which is what we’re going to do now.\nThis bit might be a bit frustrating and repetitive if I made this into an exercise for you, so just inspect the following code, run it yourself (and put it into your own script) and try to make sense of it. Take this as a recipe that you can modify and reuse in your own projects later!\nI’m happy to answer questions!\n\nStep 1.\nFor each IBD segment (a row in our table ibd_segments) we have two individuals on each row, sample1 and sample2. Therefore, we will need to annotate each row of the ibd_segments table with metadata information for both individuals. Let’s start by making a copy of the metadata table, caling those copies metadata1 and metadata2:\n\nmetadata1 &lt;- metadata\nmetadata2 &lt;- metadata\n\n\n\nStep 2.\nThe columns of these metadata copies are the same, which you can verify by running this:\n\ncolnames(metadata1)\n## [1] \"sample\"    \"country\"   \"continent\" \"age\"       \"age_bin\"\nhead(metadata1, 1)\n## # A tibble: 1 × 5\n##   sample  country continent   age age_bin    \n##   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;fct&gt;      \n## 1 NA18486 Nigeria Africa        0 present-day\n\n\ncolnames(metadata2)\n## [1] \"sample\"    \"country\"   \"continent\" \"age\"       \"age_bin\"\nhead(metadata2, 1)\n## # A tibble: 1 × 5\n##   sample  country continent   age age_bin    \n##   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;fct&gt;      \n## 1 NA18486 Nigeria Africa        0 present-day\n\nObviously this doesn’t work if we want to refer to a location of sample1 or sample2, respectively. Here’s another useful trick for this. Observe what paste0() function does in this situation:\n\n# original columns\ncolnames(metadata1)\n\n[1] \"sample\"    \"country\"   \"continent\" \"age\"       \"age_bin\"  \n\n# columns with a number added after\npaste0(colnames(metadata1), \"1\")\n\n[1] \"sample1\"    \"country1\"   \"continent1\" \"age1\"       \"age_bin1\"  \n\n\nSo using paste0() we can append “1” or “2” to each column name of both tables. We can use the same approach to assign new columns to both of our metadata copies:\n\ncolnames(metadata1) &lt;- paste0(colnames(metadata1), \"1\")\ncolnames(metadata2) &lt;- paste0(colnames(metadata2), \"2\")\n\nNow we have both metadata tables with columns renamed. Let’s check this to make sure we did this correctly:\n\ncolnames(metadata1)\n\n[1] \"sample1\"    \"country1\"   \"continent1\" \"age1\"       \"age_bin1\"  \n\ncolnames(metadata2)\n\n[1] \"sample2\"    \"country2\"   \"continent2\" \"age2\"       \"age_bin2\"  \n\n\n\n\nStep 3.\nNow we are ready to join the table of IBD segments.\nThe final piece of the puzzle is a JOIN operation from the realm of computer databases. A rather complex topic, which can be interpreted very easily in our situation, and can be schematically described by the following diagram of a so-called “inner join”, implemented in dplyr by a function inner_join():\nNote: If you’re more familiar with tidyverse from before, I still think joins might be a relatively new topic. If so, please take a bit of time to read the dplyr documentation on join operations. Joins are a superpower of data science.\n\n\n\n What this operation does is that it creates a “merged” table, consisting of the columns of both “left” and “right” tables, where the “merged” table rows consist of such rows from “left” and “right”, where the two tables have a matching “key” column (here highlighted in green). Just take a moment and try to trace the one row of the “merged” table at the bottom of the diagram to the row(s) of the “left” and “right” tables based on the value of the green key column in both of them.\nNow, consider the columns of our three tables now:\n\nibd_segments columns:\n\n\n\n[1] \"sample1\" \"sample2\" \"chrom\"   \"start\"   \"end\"     \"rel\"     \"length\" \n\n\n\nmetadata1 and metadata2 columns:\n\n\n\n[1] \"sample1\"    \"country1\"   \"continent1\" \"age1\"       \"age_bin1\"  \n\n\n\n\n[1] \"sample2\"    \"country2\"   \"continent2\" \"age2\"       \"age_bin2\"  \n\n\nHere’s what happens when you perform an inner join like this (note that we specify by what column should we be merging, effectively saying which column is the key corresponding to the green column in the diagram above):\n\nibd_merged &lt;- inner_join(ibd_segments, metadata1, by = \"sample1\")\n\nTake a look at the results, saved in a new variable ibd_merged, by typing it into your R console. Our IBD table now has added metadata information for individuals in column sample1, awesome! We can verify this by listing all column names:\n\n# original IBD table:\nhead(ibd_segments, n = 3)\n\n# A tibble: 3 × 7\n  sample1 sample2 chrom start   end rel   length\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 VK548   NA20502    21  58.3  62.1 none    3.75\n2 VK528   HG01527    21  24.8  27.9 none    3.04\n3 PL_N28  NEO36      21  14.0  16.5 none    2.44\n\n# IBD table with metadata1 merged in:\nhead(ibd_merged, n = 3)\n\n# A tibble: 3 × 11\n  sample1 sample2 chrom start   end rel   length country1 continent1  age1\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;\n1 VK548   NA20502    21  58.3  62.1 none    3.75 Norway   Europe     1000 \n2 VK528   HG01527    21  24.8  27.9 none    3.04 Norway   Europe     1050 \n3 PL_N28  NEO36      21  14.0  16.5 none    2.44 Poland   Europe     6074.\n# ℹ 1 more variable: age_bin1 &lt;fct&gt;\n\n\nHow do we add metadata for the second sample? We do it in the same way, except now we join in the second table (and we save the results back to the new variable ibd_merged, to be able to compare the differences):\n\nibd_merged &lt;- inner_join(ibd_merged, metadata2, by = \"sample2\")\n\nIn fact, I do this so often for basically every computational biology project, that I like to use this concise bit of code to do it all in one go (this is what you should add to your script!):\n\nibd_merged &lt;-\n  ibd_segments %&gt;%                          # 1. primary data without metadata\n  inner_join(metadata1, by = \"sample1\") %&gt;% # 2. add metadata for sample1\n  inner_join(metadata2, by = \"sample2\")     # 3. add metadata for sample2\n\nAgain, we can verify that our new table has metadata columns for both sample1 and sample2:\n\nhead(ibd_segments, n = 3)\n\n# A tibble: 3 × 7\n  sample1 sample2 chrom start   end rel   length\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 VK548   NA20502    21  58.3  62.1 none    3.75\n2 VK528   HG01527    21  24.8  27.9 none    3.04\n3 PL_N28  NEO36      21  14.0  16.5 none    2.44\n\nhead(ibd_merged, n = 3)\n\n# A tibble: 3 × 15\n  sample1 sample2 chrom start   end rel   length country1 continent1  age1\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;\n1 VK548   NA20502    21  58.3  62.1 none    3.75 Norway   Europe     1000 \n2 VK528   HG01527    21  24.8  27.9 none    3.04 Norway   Europe     1050 \n3 PL_N28  NEO36      21  14.0  16.5 none    2.44 Poland   Europe     6074.\n# ℹ 5 more variables: age_bin1 &lt;fct&gt;, country2 &lt;chr&gt;, continent2 &lt;chr&gt;,\n#   age2 &lt;dbl&gt;, age_bin2 &lt;fct&gt;\n\n\n\nAnd this concludes merging of the primary IBD data with all necessary meta information! Every computational project has something like this somewhere. Of course, in the case of your primary data which could be observations at particular ecological sampling sites or excavation sites, etc., this annotation might involve metadata with more specific geographical information, environmental data, etc. But the idea of using join for the annotation remains the same.",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More _tidyverse_ practice</span>"
    ]
  },
  {
    "objectID": "tidy-advanced.html#exercise-4.-final-data-processing",
    "href": "tidy-advanced.html#exercise-4.-final-data-processing",
    "title": "More tidyverse practice",
    "section": "Exercise 4. Final data processing",
    "text": "Exercise 4. Final data processing\nYou’ve already seen the super useful function paste() (and also paste0()) which useful whenever we need to join the elements of two (or more) character vectors together. The difference between them is that paste0() doesn’t add a space between the individual values and paste() does (the latter also allows you to customize the string which should be used for joining instead of the space).\nTake a look at your (now almost finalized) table ibd_merged:\n\nhead(ibd_merged)\n\n# A tibble: 6 × 15\n  sample1 sample2 chrom start   end rel   length country1 continent1  age1\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;\n1 VK548   NA20502    21  58.3  62.1 none   3.75  Norway   Europe     1000 \n2 VK528   HG01527    21  24.8  27.9 none   3.04  Norway   Europe     1050 \n3 PL_N28  NEO36      21  14.0  16.5 none   2.44  Poland   Europe     6074.\n4 NEO556  HG02230    21  13.1  14.0 none   0.962 Russia   Europe     7030 \n5 HG02399 HG04062    21  18.8  21.0 none   2.21  China    Asia          0 \n6 EKA1    NEO220     21  30.1  35.8 none   5.72  Estonia  Europe     4240 \n# ℹ 5 more variables: age_bin1 &lt;fct&gt;, country2 &lt;chr&gt;, continent2 &lt;chr&gt;,\n#   age2 &lt;dbl&gt;, age_bin2 &lt;fct&gt;\n\n\nWhen we get to comparing IBD patterns between countries, time bins, etc., it might be useful to have metadata columns specific for that purpose. What do I mean by this? For instance, consider these two vectors and the result of pasting them together. Having this combined pair information makes it very easy to cross-compare multiple sample categories together, rather than having to keep track of two variables v1 or v2, we have just the merged pairs:\n\nv1 &lt;- c(\"Denmark\", \"Czech Republic\", \"Estonia\")\nv2 &lt;- c(\"Finland\", \"Italy\", \"Estonia\")\n\npairs_v &lt;- paste(v1, v2, sep = \":\")\npairs_v\n\n[1] \"Denmark:Finland\"      \"Czech Republic:Italy\" \"Estonia:Estonia\"     \n\n\n\nLet’s create this information for country1/2, continent1/2, and age_bin1/2 columns.\nSpecifically, use the same paste() trick in a mutate() function to add a new column to your ibd_merged table named country_pair, which will contain a vector of values created by joining of the columns country1 and country2. Add this column .before the chrom column for clearer visibility (check out ?mutate if you need a refresher on how does its .before = argument work). Then do the same to create a region_pair based on continent1 and continent2. Save the result back to the variable named ibd_merged.\nHint: Note that you can create multiple new columns with mutate(), but you can also pipe %&gt;% one mutate() into another mutate() into another mutate(), etc.!\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nibd_merged &lt;- mutate(\n  ibd_merged,\n  country_pair = paste(country1, country2, sep = \":\"),\n  region_pair = paste(continent1, continent2, sep = \":\"),\n  .before = chrom\n)\n\n# let's verify the results\nibd_merged\n\n# A tibble: 461,847 × 17\n   sample1 sample2 country_pair     region_pair   chrom start   end rel   length\n   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;            &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1 VK548   NA20502 Norway:Italy     Europe:Europe    21  58.3  62.1 none   3.75 \n 2 VK528   HG01527 Norway:Spain     Europe:Europe    21  24.8  27.9 none   3.04 \n 3 PL_N28  NEO36   Poland:Sweden    Europe:Europe    21  14.0  16.5 none   2.44 \n 4 NEO556  HG02230 Russia:Spain     Europe:Europe    21  13.1  14.0 none   0.962\n 5 HG02399 HG04062 China:India      Asia:Asia        21  18.8  21.0 none   2.21 \n 6 EKA1    NEO220  Estonia:Sweden   Europe:Europe    21  30.1  35.8 none   5.72 \n 7 19651   HG01941 Canada:Peru      America:Amer…    21  49.0  51.3 none   2.24 \n 8 HG02181 HG04033 China:India      Asia:Asia        21  13.1  13.9 none   0.868\n 9 VK328   HG01438 Denmark:Colombia Europe:Ameri…    21  60.7  62.8 none   2.04 \n10 F004    HG03616 China:India      Asia:Asia        21  25.4  27.6 none   2.15 \n# ℹ 461,837 more rows\n# ℹ 8 more variables: country1 &lt;chr&gt;, continent1 &lt;chr&gt;, age1 &lt;dbl&gt;,\n#   age_bin1 &lt;fct&gt;, country2 &lt;chr&gt;, continent2 &lt;chr&gt;, age2 &lt;dbl&gt;,\n#   age_bin2 &lt;fct&gt;\n\n# values in the new `region_pair` column\ntable(ibd_merged$region_pair)\n\n\n  Africa:Africa  Africa:America     Africa:Asia   Africa:Europe  America:Africa \n           8853            1337              29             137            2968 \nAmerica:America    America:Asia  America:Europe     Asia:Africa    Asia:America \n          42917           11270            3821              35           11558 \n      Asia:Asia     Asia:Europe   Europe:Africa  Europe:America     Europe:Asia \n         109360           24806             229           25586           25891 \n  Europe:Europe \n         193050 \n\n\n\n\n\n\nIn addition to computing statistics across pairs of geographical regions, we will also probably want to look at temporal patterns. Create a new column time_pair, which will in this case contains a paste() combination of age_bin1 and age_bin2, added .after the region_pair column. Save the result back to the ibd_merged variable.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nibd_merged &lt;- mutate(\n  ibd_merged,\n  time_pair = paste(age_bin1, age_bin2, sep = \":\"), .after = region_pair\n)\n\nibd_merged\n\n# A tibble: 461,847 × 18\n   sample1 sample2 country_pair    region_pair time_pair chrom start   end rel  \n   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;           &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n 1 VK548   NA20502 Norway:Italy    Europe:Eur… (0,10000…    21  58.3  62.1 none \n 2 VK528   HG01527 Norway:Spain    Europe:Eur… (0,10000…    21  24.8  27.9 none \n 3 PL_N28  NEO36   Poland:Sweden   Europe:Eur… (0,10000…    21  14.0  16.5 none \n 4 NEO556  HG02230 Russia:Spain    Europe:Eur… (0,10000…    21  13.1  14.0 none \n 5 HG02399 HG04062 China:India     Asia:Asia   present-…    21  18.8  21.0 none \n 6 EKA1    NEO220  Estonia:Sweden  Europe:Eur… (0,10000…    21  30.1  35.8 none \n 7 19651   HG01941 Canada:Peru     America:Am… (0,10000…    21  49.0  51.3 none \n 8 HG02181 HG04033 China:India     Asia:Asia   present-…    21  13.1  13.9 none \n 9 VK328   HG01438 Denmark:Colomb… Europe:Ame… (0,10000…    21  60.7  62.8 none \n10 F004    HG03616 China:India     Asia:Asia   (0,10000…    21  25.4  27.6 none \n# ℹ 461,837 more rows\n# ℹ 9 more variables: length &lt;dbl&gt;, country1 &lt;chr&gt;, continent1 &lt;chr&gt;,\n#   age1 &lt;dbl&gt;, age_bin1 &lt;fct&gt;, country2 &lt;chr&gt;, continent2 &lt;chr&gt;, age2 &lt;dbl&gt;,\n#   age_bin2 &lt;fct&gt;\n\n# values in the new `time_pair` column\ntable(ibd_merged$time_pair)\n\n\n        (0,10000]:(0,10000]     (0,10000]:(10000,20000] \n                     161680                        4205 \n    (0,10000]:(20000,30000]     (0,10000]:(30000,40000] \n                        158                         188 \n    (0,10000]:(40000,50000]       (0,10000]:present-day \n                         16                      153476 \n    (10000,20000]:(0,10000] (10000,20000]:(10000,20000] \n                       4873                         326 \n(10000,20000]:(20000,30000] (10000,20000]:(30000,40000] \n                          6                           5 \n  (10000,20000]:present-day     (20000,30000]:(0,10000] \n                       4835                         105 \n(20000,30000]:(10000,20000] (20000,30000]:(30000,40000] \n                          3                           3 \n  (20000,30000]:present-day     (30000,40000]:(0,10000] \n                        141                         514 \n(30000,40000]:(10000,20000] (30000,40000]:(20000,30000] \n                         14                           3 \n(30000,40000]:(30000,40000]   (30000,40000]:present-day \n                         51                         441 \n    (40000,50000]:(0,10000] (40000,50000]:(10000,20000] \n                         53                           3 \n(40000,50000]:(20000,30000] (40000,50000]:(30000,40000] \n                          2                           4 \n  (40000,50000]:present-day     present-day:present-day \n                        146                      130596",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More _tidyverse_ practice</span>"
    ]
  },
  {
    "objectID": "tidy-advanced.html#exercise-5-reproducibility-intermezzo",
    "href": "tidy-advanced.html#exercise-5-reproducibility-intermezzo",
    "title": "More tidyverse practice",
    "section": "Exercise 5: Reproducibility intermezzo",
    "text": "Exercise 5: Reproducibility intermezzo\nYou have written a lot of code in this chapter, both for filtering and processing of the IBD data and the metadata. Your script is already very long (and maybe messy—I organized things like this on purpose, so this isn’t on you!).\nIf you need to ever update or change something (which you always will during a project, usually many times over) it can be hard to figure out which line of your script should be modified. If you forget to update something somewhere in a long script, you’re in big trouble (and sometimes you won’t even find out where’s the problem until its too late).\nOur session on reproducibility is in the following chapter, and will introduce various techniques towards making a project codebase tidyly structured and reproducible. But for now, let’s work towards making our current IBD pipeline a little bit more reusable (which we will leverage in the next session on ggplot2).\n\nCreate a new script called ibd_utils.R in your RStudio and create the following functions in that script (see the list below). Then copy-paste bits and pieces from your solutions above and add them to the bodies of these functions accordingly, so that each function contains all the code we used above for that particular step. (There are three functions, for each part of the pipeline code we developed in this session.)\n\nIf you’ve never written R functions before and are confused, don’t worry. You can skip ahead and use my solution directly (just paste that solution to your ibd_utils.R script)!\n\n\nYes, this is the first time I specifically told you that you can copy-paste something directly. :)\nIf you do, however, examine the code in each of the three functions, and try to make a connection between each function — line by line — and the code you ran earlier in our session! Because these functions really do contain all your code, just “modularized” in a reusable form. Refer back to the exercise on writing custom functions in our R basecamp session if you need a refresher on the R syntax used for this.\n\nHere are the functions you should create:\n\nFirst, don’t forget to load all packages needed using library() calls at the top of the ibd_utils.R script.\nFunction process_ibd &lt;- function() { ... } which will:\n\n\nread the IBD data from the internet like you did above using read_tsv()\nprocess it in the same way you did above, adding the length column\nreturn the processed table with IBD segments in the same way like you produced the ibd_segments data frame with return(ibd_segments) at the last line of that function\n\n\nFunction process_metadata &lt;- function(bin_step) { ... }, which will (accepting a single bin_step argument):\n\n\ndownload the metadata from the internet like you did above,\nprocess and filter it in the same way like above (replacing NA in age column, filtering out unwanted individuals, adding the age_bin column by binning according to the bin_step function parameter, etc.)\nreturn the processed table with this processed metadata in the same way like you produced the metadata data frame as return(metadata)\n\n\nFunction join_metadata &lt;- function(ibd_segments, metdata) { ... }, which will accept two arguments ibd_segments and metadata (produced by the two functions above) and then:\n\n\njoin the ibd_segments table with the metadata table\nadd the three country_pair, region_pair, and time_pair columns\nreturn the finalized table with all information, just like you created your ibd_merged table above, as return(ibd_merged)\n\nDo not write all the code at once! Start with the first function, test it by executing it in your R console independently, checking that your results make sense before you move on to the other function! Building up more complex pipelines from simple components is absolutely critical to minimize bugs!\nMake sure to add comments to your code! Reproducibility is only half of the story – you (and other people) need to be able to understand your code a long time after you wrote it.\nHint: If this looks overwhelming and scary, then remember that function is a block of code (optionally accepting arguments) which simply wraps in { and } the code already written by you above, and calls return on the result.\nHint: An extremely useful shortcut when writing R functions is “Run function definition” under Code -&gt; Run region. Remember that keyboard shortcut! (On macOS it’s Option+CMD+F).\nTo summarize, basically fill in the following template:\n\n\n\n\n\n\nClick to see the “template” for ibd_utils.R\n\n\n\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\n\nprocess_ibd &lt;- function() {\n  cat(\"Downloading and processing IBD data...\\n\")\n\n  # ADD ALL YOUR CODE HERE\n\n  return(ibd)\n}\n\nprocess_metadata &lt;- function(bin_step) {\n  cat(\"Downloading and processing metadata...\\n\")\n\n  # ADD ALL YOUR CODE HERE\n\n  return(metadata)\n}\n\njoin_metadata &lt;- function(ibd_segments, metadata) {\n  cat(\"Joining IBD data and metadata...\\n\")\n\n  # ADD ALL YOUR CODE HERE\n\n  return(ibd_merged)\n}\n\n\n\n\nAfter you’re done with the previous exercise, and are confident that your functions do what they should, create a new script (you can name it something like ibd_pipeline.R), save it to the same directory as your script ibd_utils.R, and add the following code to it, which, when you run it, will execute your entire pipeline and create the final ibd_merged table in a fully automated way. As a final test, restart your R session (Session -&gt; Restart R) and run the script ibd_pipeline.R (you can use the =&gt; Run button in the top right corner of your editor panel).\nYou just build your (maybe first?) reproducible, automated data processing pipeline! 🎉\nNote: The source() command executes all the code in your utility script and therefore makes your custom-built functions available in your R session. This is an incredibly useful trick which you should use all the time whenever you modularize your code into reproducible functions in their own scripts.\n\n# you might need to change the path if your ibd_utils.R is in a different location\nsource(\"ibd_utils.R\")\n\nmetadata &lt;- process_metadata()\nibd_segments &lt;- process_ibd(bin_step = 10000)\n\nibd_merged &lt;- join_metadata(ibd_segments, metadata)\n\n\n\nDownloading and processing metadata...\n\n\nDownloading and processing IBD data...\n\n\nJoining IBD data and metadata...\n\n\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nYou can find my solution here.\n\n\n\n\n\n\nCongratulations! You’ve just passed an important milestone of learning how to structure and modularize your code in a reproducible, efficient way!\n\n\n\nThis is not a joke and I really, really mean it. Most scientists are unfortunately never taught this important concept. Over the next sessions you will see how much difference does (admittedly very annoying!) setup makes, and even learn other techniques how to make your computational research more robust and reliable.\nIf this was too much programming and you struggled to push through, please don’t worry. As I said at the beginning, this session was mostly about showing you a useful recipe for processing data and to demonstrate why tidyverse is so powerful.\nTaking a look at my solution again (now copied to your on ibd_utils.R script), and being able to understand the general process by reading the code line by line is enough at this stage.\n\nPoint your cursor somewhere inside the process_ibd() call in your new script ibd_pipeline.R and press CTRL + .. This is how easy it is to navigate code, even when we modularized it across multiple files!",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More _tidyverse_ practice</span>"
    ]
  },
  {
    "objectID": "tidy-advanced.html#take-home-exercise",
    "href": "tidy-advanced.html#take-home-exercise",
    "title": "More tidyverse practice",
    "section": "“Take home exercise”",
    "text": "“Take home exercise”\nTake your own data and play around with it using the concepts you learned above. If the data isn’t in a form that’s readily readable as a table with something like read_tsv(), please talk to me! tidyverse is huge and there are packages for munging all kinds of data. I’m happy to help you out.\nDon’t worry about “getting somewhere”. Playing and experimenting (and doing silly things) is the best way to learn.\nFor more inspiration on other things you could do with your data, take a look at the dplyr cheatsheet.\nIn each following session, you’ll have the opportunity to create beautiful publication-quality visualizations on your data using the ggplot2 package. So stay tuned!",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More _tidyverse_ practice</span>"
    ]
  },
  {
    "objectID": "tidy-advanced.html#additional-resources",
    "href": "tidy-advanced.html#additional-resources",
    "title": "More tidyverse practice",
    "section": "Additional resources",
    "text": "Additional resources\nIf you’re more comfortable with tidyverse and breezed through this content quickly, here are some resources on topics I think you should study (and do exercises in them) to take your data science skills to the next level:\n\nA huge topic I wish I had a chance to get into here are transformations between long and wide table formats – link to a chapter of R4DS book, tidyr cheatsheet.\n\nAs an exercise, try to convert the various “age-related” columns in our metadata table (encoded in the “wide format”) to a long format after studying this chapter. Then learn how to transform the long format back into the wide format!\n\nRegular expression are another critical topic which every professional data scientist should be familiar with – link to a chapter of R4DS book, regex cheatsheet.\nStudy the chapter on joins in more detail – link.\nLook up some guidelines for modern R coding style – link.\nNot entirely related to tidyverse but a critical component of large scale data analysis pipelines in R (all of which feature dplyr and others internally), particularly if they involve parallelized computing are futures. Especially the package furrr is an absolute pleasure to use – link.\nA companion to furrr which focuses on elegant interface for non-paralellized iteration is called purrr – here is a very nice cheatsheet.",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>More _tidyverse_ practice</span>"
    ]
  },
  {
    "objectID": "tidy-viz.html",
    "href": "tidy-viz.html",
    "title": "Data visualization",
    "section": "",
    "text": "Introduction\nIn this chapter, we will be delving into the data visualization R package called ggplot2, which is possibly the most famous piece of the tidyverse ecosystem. So much so that people who otherwise don’t use any tidyverse functions (or even who don’t even use R for data analysis itself) still use ggplot2 for making figures. It really is that good.",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "tidy-viz.html#part-1-visualizing-patterns-in-our-ibd-data",
    "href": "tidy-viz.html#part-1-visualizing-patterns-in-our-ibd-data",
    "title": "Data visualization",
    "section": "Part 1: Visualizing patterns in our IBD data",
    "text": "Part 1: Visualizing patterns in our IBD data\nFirst, let’s start from a clean slate, and leverage the modularized pipeline we created for our IBD data at the very end of our second tidyverse session. Create a new R script in RStudio (File -&gt; New file -&gt; R Script), and save it somewhere on your computer as tidy-viz.R (File -&gt; Save). Copy the following bit of code which will execute your IBD and metadata processing pipeline into that script.\nThe following exercises will focus on basics of visualization of various types of statistical summaries, using your IBD data set as a representative of typical data we deal with when doing data science.\n\nlibrary(ggplot2) # the star of the day!\n\nlibrary(dplyr)\nlibrary(readr)\n\n# source our utility functions so that we have them available\nsource(\"ibd_utils.R\")\n\n# download and process the metadata and IBD data set\nmetadata &lt;- process_metadata(bin_step = 10000)\nibd_segments &lt;- process_ibd()\n\n# combine the IBD table with metadata information\nibd_merged &lt;- join_metadata(ibd_segments, metadata)\n\nLet’s appreciate the modularization aspect of the work we did earlier. Practically speaking, all of that work is now hidden behind three custom functions: process_metadata(), process_ibd() and join_metadata(). We can reuse this code anywhere and if we ever need to update something, we just have to change it in a single place: the ibd_utils.R script. This is a very important example of good programming practice.\n\nExercise 1: Concept of layers\nAn important (and even elegant) concept of building visualizations using ggplot2 is the idea of “layering”. We will begin introducing this idea from the simplest possible kind of visualization, which is plotting the counts of observations of a categorical (i.e. discrete) variable, but first, what actually happens when you call a function ggplot() on its own? How about when you call ggplot(metadata), i.e. when we tell ggplot() we want to be plotting something from out metadata table?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nWe get an empty “canvas”! Almost as if we were an artist starting to paint something, to use a slightly pretentious metapho.\n\nggplot()\n\n\n\n\n\n\n\n\nSaying that we want to plot metadata doesn’t actually change anything. We get the same empty canvas, with ggplot2 basically waiting for us to give it more instructions.\n\nggplot(metadata)\n\n\n\n\n\n\n\n\n\n\n\nAs mentioned in my solution, calling ggplot() on its own basically only sets up an empty “canvas”, as if we were about to start painting something.\nThis might sound like a silly metaphor, but when it comes to ggplot2 plotting, this is actually remarkably accurate, because it is exactly how the above-mentioned idea of the “layering grammar” of ggplot2 works in practice.\nThere are two important components of every ggplot2 figure:\n\n1. “Aesthetics”\nAesthetics are a specification of the mapping of columns of a data frame to some kind of visual property of the plot we want to create (position along the x, or y axes, color, shape, etc.). We specify these mapping aesthetics with a function called aes() which you will see later many times.\n\n\n2. “Geoms”\nGeoms are graphical elements to be plotted (histograms, points, lines, etc.). We specify those using functions such as geom_bar(), geom_histogram(), geom_point(), and many many others.\n\n\nHow do “aesthetics” and “geoms” fit together?\nWe can think about it this way:\n\n“geoms” specify what our data should be plotted as in terms of “geometrical elements” (that’s where the name “geom” comes from)\n“aesthetics” specify how those geoms should look like (so, the aesthetic visuals features of geoms, literally)\n\nThis is all very abstract, but don’t worry. Refer to this as a guideline when you’re implementing the exercises in practice!\n\n\n\nExercise 2: Distribution of a categorical variable\nLet’s start very simply, and introduce the layering aspect of ggplot2 visualization package step by step. Our data frame has a column age_bin, which contains the assignment of each individual into a respective time period. Let’s start by visualizing the count of individuals in each time bin.\nRun the following code in your R console:\n\nggplot(metadata, aes(x = age_bin))\n\nIt specifies the aesthetic mapping which will map our first variable of interest (column age_bin in our data frame) to the x-axis of our eventual figure. What do you get when you run it? How is the result different from just calling ggplot(metadata) without anything else, like you did above?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nLet’s take it step by step:\n\nAgain, saying what data set we want to plot doesn’t do anything (ggplot2 doesn’t know what to plot yet!):\n\n\nggplot(metadata)\n\n\n\n\n\n\n\n\n\nmapping an aesthetic still doesn’t do anything, because we didn’t instruct ggplot2 to plot anything even now:\n\n\nggplot(metadata, aes(x = age_bin))\n\n\n\n\n\n\n\n\nA-ha! We see a difference! ggplot2 now mapped the x axis against values of the age_bin variable. Let’s verify this by looking at all possible values of that variable:\n\ntable(metadata$age_bin)\n\n\n  present-day     (0,10000] (10000,20000] (20000,30000] (30000,40000] \n         2405          1623            31             2             7 \n(40000,50000] \n            1 \n\n\nBut even then, the plot still actually doesn’t show anything! Remember, we only instruct ggplot2 to map a variable age_bin to an x-axis of a figure… but we didn’t say what we want to visualize (i.e. a “geom”).\nTo make a visualization, both aesthetic mapping and some kind of geom is needed! We will do this in the next step.\n\n\n\n\ngeom_bar()\nIn the previous exercise, you instructed ggplot() to work with the metadata table and you mapped a variable of interest age_bin to the x-axis (and therefore set up a “mapping aesthetic”). To make a complete figure, you need to add a layer with a “geom”. In ggplot2, we add layers with the + operator, mimicking the “painter metaphor” of adding layers one by one, composing a more complex figure with each additional layer.\nAdd the + geom_bar() layer on the same line after the ggplot(metadata, ...) command from the previous exercise, and run the whole line. What happens then? How do you interpret the result? Compare then to what you get by running table(metadata$age_bin).\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nWe get a full figure, yay!\n\nggplot(metadata, aes(x = age_bin)) + geom_bar()\n\n\n\n\n\n\n\n\nThis bar plot indeed matches the counts in our data:\n\ntable(metadata$age_bin)\n\n\n  present-day     (0,10000] (10000,20000] (20000,30000] (30000,40000] \n         2405          1623            31             2             7 \n(40000,50000] \n            1 \n\n\n\n\n\nIt might be hard to believe now but you now know almost everything about making ggplot2 figures. 🙂 Everything else, all the amazing complexity of the possible ggplot2 figures (really, look at that website!) comes down from combining these two simple elements: aesthetics and geoms.\n\nLet’s demonstrate the layering aspect a little bit more by adding more layers to our figure! Use a xlab() function to add an x-axis label element to your bar plot code above, again using the + operator. Look at ?xlab to see how to use this function in practice. What changed about your figure now?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nJust as we did with tidyverse %&gt;% data transformation pipelines, as a ggplot2 visualization pipeline gets more complex, it’s a good idea to introduce indentation so that each visualization steps is on its own line.\n\nggplot(metadata, aes(x = age_bin)) +\n  geom_bar() +\n  xlab(\"Time period [years before present]\")\n\n\n\n\n\n\n\n\nBy default, x and y axes are labeled by the data-frame column name specified for that x or y axis in the aes() mapping. xlab() overrides that default.\n\n\n\n\nNow continue adding another layer with the ylab() function, giving the plot proper y-axis label too.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nggplot(metadata, aes(x = age_bin)) +\n  geom_bar() +\n  xlab(\"Time period [years before present]\") +\n  ylab(\"Number of individuals\")\n\n\n\n\n\n\n\n\n\n\n\n\nGive your figure a proper main title using the function ggtitle() (again, use help ?ggtitle if you need to).\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nggplot(metadata, aes(x = age_bin)) +\n  geom_bar() +\n  xlab(\"Time period [years before present]\") +\n  ylab(\"Number of individuals\") +\n  ggtitle(\"Distribution of sample counts in each time period\")\n\n\n\n\n\n\n\n\n\n\n\n\nAlthough individual functions xlab(), ylab(), ggtitle() are useful, oftentimes it’s better to use a general function labs(). Look up its documentation under ?lab, then rewrite the code for your figure to use only this function, replacing your uses of xlab(), ylab(), and ggtitle() just with labs(). Note that the function has other useful arguments—go ahead and use as many of them as you want.\nNote: Don’t worry about making your figure cluttered with too much text, this is just for practice. In a real paper, you wouldn’t use title or caption directly in a ggplot2 figure, but it’s definitely useful for work-in-progress reports at meetings, etc.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nggplot(metadata, aes(x = age_bin)) +\n  geom_bar() +\n  labs(\n    y = \"Number of individuals\",\n    x = \"Time period [years before present]\",\n    title = \"Distribution of sample counts in each time period\",\n    subtitle = \"Counts for individuals in 1000 Genomes Project and MesoNeo data\",\n    caption = \"Here is an optional caption for the figure, similarly to what\n      you might do in a real scientific article as a more detailed description of\n      of what is going on in the plot.\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nYou can see that the “present-day” bin completely overwhelms the number of individuals in the oldest bins. This often happens with data which follow a more or less exponential scale. A useful trick is adding a + scale_y_log() layer. You can probably guess what it does, so try adding it to your code!\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nOf course, this function transforms the y-axis on the logarithmic scale! Very useful!\n\nggplot(metadata, aes(x = age_bin)) +\n  geom_bar() +\n  labs(\n    y = \"Number of individuals\",\n    x = \"Time period [years before present]\",\n    title = \"Distribution of sample counts in each time period\",\n    subtitle = \"Counts for individuals in 1000 Genomes Project and MesoNeo data\",\n    caption = \"Here is an optional caption for the figure, similarly to what\n      you might do in a real scientific article as a more detailed description of\n      of what is going on in the plot.\"\n  ) +\n  scale_y_log10()\n\n\n\n\n\n\n\n\n\n\n\n\nDoes the order of adding layers with the + operator matter? Do a little experiment to figure it out by switching the order of the plotting code and then running each version in your R console!\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nNo, generally, it doesn’t. This is another very useful feature of ggplot2. Remember that a huge important part of an R data science workflow is the interactive communication with an R console. You can start with a little bit of code which produces a basic outline of your figure and continue adding useful bits and pieces to it, modifying the outcome as you go.\n\n\n\n\nIt’s useful to keep in mind that you can always pipe a data frame into a ggplot() function using the %&gt;% operator. After all, %&gt;% always places the “data frame from the left” as the first argument in the ggplot() call, where the function expects it anyway. I.e., instead of writing ggplot(metadata, aes(...)), you can also do metadata %&gt;% ggplot(aes(...)). This allows you to transform or summarize data before plotting, which makes the combination of tidyverse and ggplot2 infinitely stronger.\nAs a refresher and for a bit more practice, filter() the metadata first for individuals who are 10000 years or older, discarding the rest. Then pipe this filter() result into your ggplot() code for the bar plot you have created previously, keeping the plotting part exactly the same (maybe skipping the log-scale).\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nmetadata %&gt;%\nfilter(age &gt; 10000) %&gt;%\nggplot(aes(x = age_bin)) +\n  geom_bar() +\n  labs(\n    y = \"Number of individuals\",\n    x = \"Time period [years before present]\",\n    title = \"Distribution of sample counts in each time period\",\n    subtitle = \"Counts for individuals in 1000 Genomes Project and MesoNeo data\",\n    caption = \"Here is an optional caption for the figure, similarly to what\n      you might do in a real scientific article as a more detailed description of\n      of what is going on in the plot.\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nBelieve it or not, with this basic structure of ggplot2 visualization and combining this with any other type of tidyverse manipulation, filtering, and (as we’ll later see), summarization, you’re ready to make any figure imaginable. Let’s take this one step further by taking a look at another type of visualization.\n\n\n\n\nExercise 3: Distribution of a numerical variable\nIn the previous exercise you visualized a distribution of a categorical variable, specifically the counts of samples in an age category. Now let’s look at continuous variables and this time consider not the metadata table, but our ibd_merged table of IBD segments between pairs of individuals. Here it is again, as a refresher:\n\nhead(ibd_merged)\n\n# A tibble: 6 × 20\n  sample1 sample2 country_pair   region_pair   time_pair chrom start   end rel  \n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt;         &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1 VK548   NA20502 Norway:Italy   Europe:Europe (0,10000…    21  58.3  62.1 none \n2 VK528   HG01527 Norway:Spain   Europe:Europe (0,10000…    21  24.8  27.9 none \n3 PL_N28  NEO36   Poland:Sweden  Europe:Europe (0,10000…    21  14.0  16.5 none \n4 NEO556  HG02230 Russia:Spain   Europe:Europe (0,10000…    21  13.1  14.0 none \n5 HG02399 HG04062 China:India    Asia:Asia     present-…    21  18.8  21.0 none \n6 EKA1    NEO220  Estonia:Sweden Europe:Europe (0,10000…    21  30.1  35.8 none \n# ℹ 11 more variables: length &lt;dbl&gt;, country1 &lt;chr&gt;, continent1 &lt;chr&gt;,\n#   age1 &lt;dbl&gt;, hgYMajor1 &lt;chr&gt;, age_bin1 &lt;fct&gt;, country2 &lt;chr&gt;,\n#   continent2 &lt;chr&gt;, age2 &lt;dbl&gt;, hgYMajor2 &lt;chr&gt;, age_bin2 &lt;fct&gt;\n\n\nThe most interesting quantity we might want to analyse in the IBD context is the length of IBD segments, which indicates the degree of genetic relatedness between individuals. So let’s take a look at the distribution of IBD lengths across our data set as an opportunity to introduce new functionality and useful tricks of ggplot2.\n\ngeom_histogram() and geom_density()\nYou can visualize a distribution of values using the functions geom_histogram() and geom_density(). Both are useful for similar things, each has its strengths and weaknesses from a visualization perspective.\nVisualize the distribution of length values across all individuals in your IBD data set, starting with these two general patterns, just like you did in the previous geom_bar() example:\n\nggplot() + geom_histogram(), and also\nggplot() + geom_density().\n\nOf course, you have to specify ibd_merged as the first parameter of ggplot(), and fill in the aes(x = length) accordingly. Try to use the knowledge you obtained in the previous exercise! As a reminder, the length is expressed in units of centimorgans, so immediately add an x-axis label clarifying these units to anyone looking at your figure, either using xlab() or labs(x = ...).\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nHere’s a histogram:\n\nibd_merged %&gt;%\n  ggplot(aes(x = length)) +\n  geom_histogram() +\n  labs(x = \"IBD length [centimorgans]\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nHere’s a density plot:\n\nibd_merged %&gt;%\n  ggplot(aes(x = length)) +\n  geom_density() +\n  labs(x = \"IBD length [centimorgans]\")\n\n\n\n\n\n\n\n\n\n\n\n\nTo make our data exploration a little easier at the beginning, let’s create a new variable ibd_eur from the total ibd_merged table as its subset, using the following tidyverse transformations (both of them):\n\nfilter() for region_pair == \"Europe:Europe\"\nfilter() for time_pair == \"present-day:present-day\"\n\nSave the result of both filtering steps as ibd_eur, and feel free to use the %&gt;% pipeline concept that you learned about previously.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nThis should be easy for you now as budding tidyverse data-munging experts!\n\nibd_eur &lt;-\n  ibd_merged %&gt;%\n  filter(\n    region_pair == \"Europe:Europe\" &\n    time_pair == \"present-day:present-day\"\n  )\n\nibd_eur\n\n# A tibble: 9,193 × 20\n   sample1 sample2 country_pair    region_pair time_pair chrom start   end rel  \n   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;           &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n 1 HG00274 NA20760 Finland:Italy   Europe:Eur… present-…    21 49.2  53.1  none \n 2 HG01620 HG01766 Spain:Spain     Europe:Eur… present-…    21 43.5  45.6  none \n 3 NA20516 NA20821 Italy:Italy     Europe:Eur… present-…    21 10.4  11.9  none \n 4 HG01632 NA20813 Spain:Italy     Europe:Eur… present-…    21 46.1  49.0  none \n 5 HG01685 NA20818 Spain:Italy     Europe:Eur… present-…    21 55.7  58.0  none \n 6 NA20502 NA20508 Italy:Italy     Europe:Eur… present-…    21  3.67  5.71 none \n 7 NA20531 NA20802 Italy:Italy     Europe:Eur… present-…    21 14.6  16.8  none \n 8 HG00119 HG00321 UK:Finland      Europe:Eur… present-…    21 39.4  41.7  none \n 9 HG00365 HG00376 Finland:Finland Europe:Eur… present-…    21 13.1  15.1  none \n10 HG00102 HG01631 UK:Spain        Europe:Eur… present-…    21 15.9  18.2  none \n# ℹ 9,183 more rows\n# ℹ 11 more variables: length &lt;dbl&gt;, country1 &lt;chr&gt;, continent1 &lt;chr&gt;,\n#   age1 &lt;dbl&gt;, hgYMajor1 &lt;chr&gt;, age_bin1 &lt;fct&gt;, country2 &lt;chr&gt;,\n#   continent2 &lt;chr&gt;, age2 &lt;dbl&gt;, hgYMajor2 &lt;chr&gt;, age_bin2 &lt;fct&gt;\n\n\nNote that I always like to indent individual steps of a data processing pipeline so that each component is on its separate line. It helps with readability a lot!\n\n\n\n\nNow visualize the distribution of lengths in the reduced, Europe-only IBD data set stored in the ibd_eur variable. Do this in two versions again, using geom_histogram() or geom_density(), exactly like you did above. Additionally, for each of the two functions specify either fill or color (or both of them—experiment!) based on the variable country_pair. Experiment with the looks of the result and find your favourite (prettiest, cleanest, easiest to interpret, etc.).\nNote: Think about how powerful the ggplot2 framework is. You only need to change one little thing (like swapping geom_histogram() for geom_density()) and keep everything else the same, and you get a completely new figure! This is amazing for brainstorming figures for papers, talks, etc.\nHow do you compare these different approaches based on the visuals of the graphics they produce (readability, aesthetics, scientific usefulness, etc.)? When does one seems more appropriate than other?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nHistogram version with fill set (I personally don’t find the color version of histograms useful at all so I’m not showing it here):\n\n\nibd_eur %&gt;%\n  ggplot(aes(x = length, fill = country_pair)) +\n  geom_histogram() +\n  labs(x = \"IBD segment length [centimorgans]\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nDensity versions:\n\n\nusing color only:\n\n\nibd_eur %&gt;%\n  ggplot(aes(x = length, color = country_pair)) +\n  geom_density() +\n  labs(x = \"IBD segment length [centimorgans]\")\n\n\n\n\n\n\n\n\n\n\n\n\nWhen you create a density plot again, setting the fill = country_pair aesthetic, you get overlapping densities which are hard to disentangle. Does changing geom_density() for geom_density(alpha = 0.25) help a bit? What does alpha do?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nalpha sets the transparency value to avoid geoms hiding behind each other, but it doesn’t help that much here. It’s a useful parameter to keep in mind though!\n\nibd_eur %&gt;%\n  ggplot(aes(x = length, fill = country_pair, color = country_pair)) +\n  geom_density(alpha = 0.5) +\n  labs(x = \"IBD segment length [centimorgans]\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4: Categories in facets (panels)\nLet’s talk a little bit more about multiple dimensions of data (i.e., the number of columns in our data frame) and how we can represent them. This topic boils down to the question of “How many dimensions of our data can we capture in a single figure and be it as comprehensible as possible?” which is more and more important the more complex (and interesting!) our questions become.\nLooking at the figures of IBD length distributions again (both histograms and densities), it’s clear there’s a lot of overlapping information that is a little hard to interpret. Just look at it again:\n\n\nCode\nibd_eur %&gt;%\n  ggplot(aes(x = length, fill = country_pair)) +\n  geom_histogram() +\n  labs(x = \"IBD segment length [centimorgans]\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nIn situations like this, facetting can be very helpful.\nFaceting is easier to demonstrate than explain, so just go ahead, take your IBD segment length histogram plot created by geom_histogram() above and add yet another layer with + facet_wrap(~ country_pair) at the end of the plotting command. How do you interpret the result now? Is it easier to read?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nibd_eur %&gt;%\n  ggplot(aes(x = length, fill = country_pair)) +\n  geom_histogram() +\n  labs(x = \"IBD segment length [centimorgans]\") +\n  facet_wrap(~ country_pair)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat happens when you add these variations of the facet_wrap() layer instead? When would you use one or the other?\n\n+ facet_wrap(~ country_pair)\n+ facet_wrap(~ country_pair, scales = \"free_x\")\n+ facet_wrap(~ country_pair, scales = \"free_y\")\n+ facet_wrap(~ country_pair, scales = \"free\")\n\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nI’ll use this opportunity to show you a useful trick of storing a ggplot2 result in a variable… because we can add to contents of variables using the + operator thanks to the magic provided by ggplot2!\nWe can create our “base histogram” like this, storing it in the variable p_histogram—try it yourself, please!\n\np_histogram &lt;- ggplot(ibd_eur, aes(length, fill = country_pair)) +\n  geom_histogram() +\n  labs(x = \"IBD segment length [centimorgans]\")\n\n\nThis is our base histogram (very hard to read)\nWe you type out just the variable, you get the figure!\n\np_histogram\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n1. option\n\np_histogram + facet_wrap(~ country_pair)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nAnd here is what you get with various facet_wrap() options:\n\n\n2. option – each facet has its own x-axis scale\n\np_histogram + facet_wrap(~ country_pair, scales = \"free_x\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n3. option – each facet has its own y-axis scale\n\np_histogram + facet_wrap(~ country_pair, scales = \"free_y\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n4. option – each facets has its own x- and y-axis scales\n\np_histogram + facet_wrap(~ country_pair, scales = \"free\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nAll approaches have their benefits. In general, personally, I think that figures which show the same statistic across different categories should generally use the same scales on both x- and y-axes, because this makes each facet comparable to each other.\nBut there are pros and cons to doing all of these in different situations!\n\n\n\n\n\n\n\nExercise 5: Limiting axis scales\nSometimes when visualizing distributions of numerical variables (histograms, densities, any other geom we’ll see later), the distribution is too widely spread out to be useful. We can mitigate this using these two additional new layers which restrict the range of the x-axis of our figures:\n\n+ xlim(1, 7)\n+ coord_cartesian(xlim = c(1, 7))\n\nTake your density distribution ggplot of lengths of IBD segments again (you can use my code right below) and add 1. or 2. option above. What’s the difference between both possibilities?\n\nibd_eur %&gt;%\n    ggplot(aes(x = length, color = country_pair)) +\n    geom_density() +\n    labs(x = \"IBD segment length [centimorgans]\")\n\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nHere are the two versions of restricting the x-axis scale. Note that they differ only by the command used at the 5th line!\n\ncoord_cartesian() performs so-called “soft clipping” – it restricts the “viewpoint” on the entire data set:\n\n\nibd_eur %&gt;%\n  ggplot(aes(x = length, color = country_pair)) +\n  geom_density() +\n  labs(x = \"IBD segment length [centimorgans]\") +\n  coord_cartesian(xlim = c(1, 7))\n\n\n\n\n\n\n\n\n\nxlim() does “hard clipping” – it effectively removes the points outside of the given range. This usually isn’t what we want, so I personally rarely if ever use this:\n\n\nibd_eur %&gt;%\n  ggplot(aes(x = length, color = country_pair)) +\n  geom_density() +\n  labs(x = \"IBD segment length [centimorgans]\") +\n  xlim(1, 7)\n\nWarning: Removed 653 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\nAdmittedly they both give the same result in this situation, but that’s not always the case, so be careful! I personally usually use the option 1., just to be safe.\n\n\n\n\nIs there evidence of some countries sharing more IBD sequence within themselves. If that’s the case, it could be a result of a stronger population bottleneck. The previous density plot is a little too busy to be able to see this, so filter your ibd_eur table for rows in which country1 is the same as country2, and then %&gt;% pipe it into the usual ggplot2 density command.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nibd_eur %&gt;%\n  filter(country1 == country2) %&gt;%\n  ggplot(aes(length, color = country_pair)) +\n  geom_density() +\n  labs(x = \"IBD segment length [centimorgans]\") +\n  xlim(0, 10)\n\nWarning: Removed 23 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\nIt looks like there are more IBD sequence shared within Finland, which is consistent with a previously established hypothesis that ancestors of present-day Finns experienced a strong popularion bottleneck.\n\n\n\n\n\ngeom_boxplot() and geom_violin()\nLet’s introduce another useful geom for visualization of this type of numerical data, the boxplot! You can visualize boxplots with the function geom_boxplot(), and calling (in our case right here) aes(x = country_pair, y = length, color = country) within the ggplot() function. Adjust your previous density example according to these instructions to show boxplots instead!\nWhen you have your boxplots, swap out geom_boxplot() for geom_violin().\nNote: Again, notice how super modular and customizable ggplot2 for making many different types of visualizations. We just had to swap one function for another.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nibd_eur %&gt;%\n  filter(country1 == country2) %&gt;%\n  ggplot(aes(x = country_pair, y = length, color = country_pair)) +\n  geom_boxplot() +\n  labs(y = \"IBD segment length [centimorgans]\") +\n  ylim(0, 10)\n\nWarning: Removed 23 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nibd_eur %&gt;%\n  filter(country1 == country2) %&gt;%\n  ggplot(aes(x = country_pair, y = length, color = country_pair)) +\n  geom_violin() +\n  labs(y = \"IBD segment length [centimorgans]\") +\n  ylim(0, 10)\n\nWarning: Removed 23 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\n\n\n\n\n\n\n\n\n\n\nI’m personally a much bigger fan of the violin plot for this kind of purpose, because I feel like boxplot hides too much about the distribution of the data.\nOn that note, another useful trick we haven’t mentioned yet is that we can plot multiple geoms from the same data! You’re already quite familiar with the ggplot2 concept of layering functions on top of each other using the + operator. What happens when you add + geom_jitter(size = 0.3, color = \"darkgray\") (again at the end of the code which produces violin)? How do you read the resulting figure?\nNote: We specified color = \"darkgray\", which effectively overrides the color = country_pair assigned “globally” in the ggplot() call (and which normally sets the color for every single geom we would add to our figure).\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\ngeom_jitter() is extremely useful to really clearly see the true distribution of our data points (which is often otherwise hidden by the “summarizing geoms”, such as boxplot above – one reason why I’m not a fan of boxplots, as I mentioned!):\n\nibd_eur %&gt;%\n  filter(country1 == country2) %&gt;%\n  ggplot(aes(x = country_pair, y = length, color = country_pair)) +\n  geom_violin() +\n  geom_jitter(size = 0.3, color = \"darkgray\") +\n  labs(y = \"IBD segment length [centimorgans]\") +\n  ylim(0, 10)\n\nWarning: Removed 23 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\nWarning: Removed 23 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do you think happens when you swap the order of geom_violin() and geom_jitter(...) in your previous solution? Why does the order matter?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nggplot2 arranges elements on a canvas literally like a painter would, if an element is added and another element is added later, the latter can (partially) overlap the former:\n\nibd_eur %&gt;%\n  filter(country1 == country2) %&gt;%\n  ggplot(aes(x = country_pair, y = length, color = country_pair)) +\n  geom_jitter(size = 0.3, color = \"darkgray\") + # we flipped the order\n  geom_violin() +                               # of these two geoms\n  labs(y = \"IBD segment length [centimorgans]\") +\n  ylim(0, 10)\n\nWarning: Removed 23 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\nWarning: Removed 23 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nOf course, this can also be intentional sometimes! It all depends on the aesthetic you’re going for as a visualization artist. :)\n\n\n\n\n\n\nIntermezzo: Saving ggplot2 figures to PDF\nLooking at figures in RStudio is useful for exploratory analysis, but when we want to use a figure in a paper or a presentation, we need to save it to disk. Doing this is very easy using the function ggsave().\nThere are generally three options:\n\nSimply running a ggplot2 command, and calling ggsave() right after to save a single figure. Something like this:\n\n\n# we first plot a figure by running this in the R console:\nggplot(ibd_eur, aes(length, fill = country_pair)) +\n  geom_histogram() +\n  labs(x = \"IBD segment length [centimorgans]\")\n\n# this then saves the last figure to a given file\nggsave(\"~/Desktop/ibd_length_histogram.pdf\")\n\n\nOr, sometimes in a larger script (like we’ll see later), it’s useful to actually save a ggplot to a variable (yes, you can do that!), and then give that variable to ggsave() as its second parameter. This is useful when you want to first create multiple plots, store them in their own variables, then save them to files at once. Something like this:\n\n\n# we first create a histogram plot\np_histogram &lt;- ggplot(ibd_eur, aes(length, fill = country_pair)) +\n  geom_histogram() +\n  labs(x = \"IBD segment length [centimorgans]\")\n\n# then we create a density plot\np_density &lt;- ggplot(ibd_eur, aes(length, fill = country_pair)) +\n  geom_density(alpha = 0.7) +\n  labs(x = \"IBD segment length [centimorgans]\")\n\n# note that if you type the variable into your R console (try this!),\n# you will get your figure visualized:\np_histogram\n\n# we can save figures stored in variables like this\nggsave(\"~/Desktop/ibd_length_histogram.pdf\", p_histogram)\nggsave(\"~/Desktop/ibd_length_density.pdf\", p_density)\n\n\nSave multiple figures into a single PDF. This requires a different approach, in which we first initialize a PDF file, then we have to print(!) individual variables with each ggplot2 figure, and then close that PDF file. Yes, this is confusing, but don’t worry about the technical details of it! Just take this as a recipe you can revisit later if you need to.\n\n\n# this creates an empty PDF\npdf(\"~/Desktop/multiple_plots.pdf\")\n\n# we have to print(!) individual variables with each plot\nprint(p_histogram)\nprint(p_density)\n\n# this closes the PDF\ndev.off()\n\nI don’t think it’s very interesting to create a specific exercise for it. Just try running these three examples on your own, and see what happens! We’ll be using these techniques a lot in the next session, so just keep them in mind for now.\nNote: Take a peek at ?ggsave and ?pdf help pages to see customization options, particularly on the sizes of the PDFs. Also, how would you create a PNG images with the figures? Use the help pages!\n\n\nExercise 6: Relationship between two variables\n\ngeom_point()\nWe’ll now look at creating another type of visualization which is extremely useful and very common: a scatter plot!\nBut first, let’s do some more data summarization so that we can demonstrate building of scatter plots using ggplot2. I actually created a new data set for you, using the original IBD segments table.\nIt contains, for a given pair of individuals in the data set, several summary statistics about their level of IBD sharing across the entire genome (not just chromosome 21). Specifically, you will now focus on three columns of the new table:\n\nn_ibd — the total number of IBD segments between those two individuals\ntotal_ibd — the total amount of genome in IBD between those two individuals\nrel — indicator of the relatedness between those two individuals\n\nRead this table using this command:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nibd_sum &lt;- read_tsv(\"http://tinyurl.com/simgen-ibd-sum\")\n\nRows: 15119 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (7): sample1, sample2, rel, country_pair, region_pair, time_pair, distance\ndbl (2): n_ibd, total_ibd\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nWith our summarized table ibd_sum ready, we’re ready to create our scatter plot. When we worked with geom_histogram(), geom_density(), and geom_bar(), we only needed to do something like this:\n\ndf %&gt;%\n  ggplot(aes(x = COLUMN_NAME)) +\n  geom_FUNCTION()\n\nBecause scatter plot is two-dimensional, we have to specify not only the x but also the y in the aes() aesthetic mapping… and with that bit of information, I think you already know how to do this based on your knowledge of basic ggplot2 techniques introduced above!\nUse the function geom_point() to visualize the scatter plot of x = total_ibd against y = n_ibd. We know that related individuals share a large amounts of IBD sequence between each other. As a reminder, the entire human genome spans about 3000 centimorgans. Can you guess from your figure which pairs of individuals (a point on your figure representing one such pair) could be potentially closely related?\nNote: Don’t worry about this too much, just take a guess. Below we’ll look at this question more closely.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nIt looks like some pairs of individuals share almost their entire genome in IBD, around the whole 3 gigabases of sequence!\nWe’ll work on clarifying that below.\n\nibd_sum %&gt;%\n  ggplot(aes(x = total_ibd, y = n_ibd)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the introduction of histograms and densities, you’re already familiar with modifying the aes() aesthetic of a geom layer using the color argument of aes(). Right now your dots are all black, which isn’t super informative. Luckily, your IBD table has a “mysterious” column called rel. What happens when you color points based on the values in this column inside the aes() function (i.e., set color = rel)? Similarly, what happens when you set shape = rel?\nLook at a figure from this huge study of IBD patterns and their relationship to the degree of relatedness between individuals? This is completely independent data set and independent analysis, but do you see similarities between this paper and your figure?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nibd_sum %&gt;%\n  ggplot(aes(x = total_ibd, y = n_ibd, color = rel)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nI have to say I kind of hate that “none” (basically a baseline background signal) is given a striking purple colour (and I love purple). I wouldn’t want that in my paper, and we will show a potential solution to this soon! For now, let’s keep going.\n\nTo get a bit closer to a presentation-ready figure, use the + labs() layer function to properly annotate your x and y axes, and give your figure a nice title too.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nibd_sum %&gt;%\n  ggplot(aes(x = total_ibd, y = n_ibd, color = rel)) +\n  geom_point() +\n  labs(x = \"total IBD sequence\", y = \"number of IBD segments\",\n       title = \"Total IBD sequence vs number of IBD segments\",\n       subtitle = \"Both quantities computed only on IBD segments 10 cM or longer\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6: Multiple data sets in one plot\nSo far we’ve always followed this pattern of ggplot2 usage, to borrow our very first geom_histogram() example just for the sake of explanation:\n\nggplot(ibd_segments, aes(length)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nHowever, you can also do this and it will give the same result (try it– it will come in handy many times in the future!):\n\nggplot() +\n  geom_histogram(data = ibd_segments, aes(length))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nIn other words, each geom_&lt;...&gt;() function accepts both data data frame and also its own aes() aesthetic and mapping function. You could think of this as each individual geom having a “self-contained” ggplot() function’s ability.\nThis can be very useful whenever you want to plot different geoms with their own aesthetic parameters (colors, shapes, sizes)! Let’s work through an exercise to explain this better, step by step, to make our IBD scatter plot figure much more nicer to look at.\n\nAbove I complained a little bit how the “none” related data points are given just as strikingly colorful points as data points of interest. What we want to establish is different visual treatments of different kinds of data. Here’s one solution to do this (which will also demonstrate a very very useful ggplot2 trick).\nFirst, take your ibd_sum table and create two versions of it using filter() like this:\n\nibd_unrel &lt;- filter(ibd_sum, rel == \"none\")\nibd_rel &lt;- filter(ibd_sum, rel != \"none\")\n\nThe first table contains only those pairs of individuals with missing information about relatedness, the second one contains only those rows with this information present.\n\nNow visualize the ibd_unrel (i.e., IBD summaries between unrelated individuals) table with the following code:\nggplot() +\n  geom_point(data = ibd_unrel, aes(x = n_ibd, y = total_ibd), color = \"lightgray\")\nNote: The ggplot() in this case has no arguments, just like we showed in the two versions of geom_histogram() above. Everything is in the geom_point() call!\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nggplot() +\n  geom_point(data = ibd_unrel, aes(x = n_ibd, y = total_ibd), color = \"lightgray\")\n\n\n\n\n\n\n\n\n\n\n\n\nNow visualize the scatter plot of pairs of individuals which are known to be related, ibd_rel:\nggplot() +\n  geom_point(data = ibd_rel, aes(x = n_ibd, y = total_ibd, color = rel))\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nggplot() +\n  geom_point(data = ibd_rel, aes(x = n_ibd, y = total_ibd, color = rel))\n\n\n\n\n\n\n\n\n\n\n\n\nCompare the two figures you just created, based on ibd_rel and ibd_unrel to the complete figure based on ibd_sum you created above. You can see that these to plots together are showing the same data.\n\nYou have now plotted the two scatter plots separately, but here comes a very cool aspect of ggplot2 I wanted to show you. You can combine the two figures, each plotting a different data frame, into one figure by writing ggplot2 code which has both geom_point() calls for ibd_unrel and ibd_rel in the same command!\nDo this now and to make the figure even nicer (and even publication ready!), add proper x and y labels, overall plot title as well as subtitle (all using the labs() layer functions) as well as adjust the legend title (just like you did with the guides() layer function above).\nIn the geom_point() command plotting unrelated individuals (ibd_unrel), in addition to setting color = \"lightgray\", set also size = 0.75.\nHint: If it still isn’t clear, don’t worry. Here’s a bit of help of what your plotting code should look like:\nggplot() +\n  geom_point(&lt;YOUR CODE PLOTTING UNRELATED IBD PAIRS&gt;)\n  geom_point(&lt;YOUR CODE PLOTTING RELATED IBD PAIRS&gt;) +\n  labs(&lt;x, y, title, and subtitle text&gt;) +\n  guides(color = guide_legend(\"relatedness\"))\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nHere’s the complete solution to this exercise:\n\nibd_unrel &lt;- filter(ibd_sum, rel == \"none\")\nibd_rel &lt;- filter(ibd_sum, rel != \"none\")\n\nggplot() +\n  geom_point(data = ibd_unrel, aes(x = total_ibd, y = n_ibd), color = \"lightgray\", size = 0.75) +\n  geom_point(data = ibd_rel, aes(x = total_ibd, y = n_ibd, color = rel)) +\n  labs(\n    x = \"total IBD sequence\",\n    y = \"number of IBD segments\",\n    title = \"Total IBD sequence vs number of IBD segments\",\n    subtitle = \"Both quantities computed only on IBD segments 10 cM or longer\"\n  ) +\n  guides(color = guide_legend(title = \"relatedness\"))\n\n\n\n\n\n\n\n\n\n\n\n\nAs a bonus, and to learn another parameter of aes() useful for scatter plots, in addition to setting color = rel (which sets a color of each point based on the relatedness value), add also shape = rel. Observe what happens when you do this!\nFinally, I’m not personally fan of the default grey-filled background of ggplots. I like to add + theme_minimal() at the end of my plotting code of many of my figures, particularly when I’m creating visualizations for my papers. Try it too!\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nHere’s the complete solution to this exercise:\n\nibd_unrel &lt;- filter(ibd_sum, rel == \"none\")\nibd_rel &lt;- filter(ibd_sum, rel != \"none\")\n\nggplot() +\n  geom_point(data = ibd_unrel, aes(x = total_ibd, y = n_ibd), color = \"lightgray\", size = 0.75) +\n  geom_point(data = ibd_rel, aes(x = total_ibd, y = n_ibd, color = rel, shape = rel)) +\n  labs(\n    x = \"total IBD sequence\",\n    y = \"number of IBD segments\",\n    title = \"Total IBD sequence vs number of IBD segments\",\n    subtitle = \"Both quantities computed only on IBD segments 10 cM or longer\"\n  ) +\n  guides(\n    color = guide_legend(title = \"relatedness\"),\n    shape = guide_legend(title = \"relatedness\")\n  ) +\n  theme_minimal()",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "tidy-viz.html#bonus-part-2-practicing-visualization-on-time-series-data",
    "href": "tidy-viz.html#bonus-part-2-practicing-visualization-on-time-series-data",
    "title": "Data visualization",
    "section": "Bonus Part 2: Practicing visualization on time-series data",
    "text": "Bonus Part 2: Practicing visualization on time-series data\n\nExercise 8: Getting fresh new new data\nWe’ve spent a lot of time looking at metadata and IBD data, demonstrating various useful bits of techniques from across tidyverse and ggplot2. There are many more useful things to learn, but let’s switch gears a little bit and look at a slightly different kind of data set, just to keep things interesting.\nFirst, create a new script and save it as f4ratio.R.\nThen add the following couple of lines of setup code, which again loads the necessary libraries and also reads a new example data set:\n\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\n\nf4ratio &lt;- read_tsv(\"https://tinyurl.com/simgen-f4ratio\", show_col_types = FALSE)\n\nThis data set contains results from a huge population genetics simulation study I recently did, and contains estimates of the proportion of Neanderthal ancestry in a set of European individuals spanning the past 50 thousand years.\nHere’s a brief description of the contents of the columns:\n1 individual, time – name and time (in years before present) of an individual 2. statistic – a variant of an estimator of Neanderthal ancestry 3. proportion – the proportion of Neanderthal ancestry estimated by statistic 4. rate_eur2afr – the amount of ancient migration (gene flow) from Europe to Africa in a given scenario 5. replicate – the replicate number of a simulation run (each scenario was run multiple times to capture the effect of stochasticity)\n\nhead(f4ratio)\n\n# A tibble: 6 × 6\n  individual  time statistic       proportion rate_eur2afr replicate\n  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 eur_1      40000 direct f4-ratio   0.00687             0         1\n2 eur_2      38000 direct f4-ratio   0.00807             0         1\n3 eur_3      36000 direct f4-ratio   0.00192             0         1\n4 eur_4      34000 direct f4-ratio   0.0245              0         1\n5 eur_5      32000 direct f4-ratio   0.000390            0         1\n6 eur_6      30000 direct f4-ratio  -0.00267             0         1\n\n\nIn other words, each row of this table contains the proportion of Neanderthal ancestry estimated for a given individual at a given time in some scenario specified by a given set of other parameters (rate_afr2afr, rate_eur2afr, replicate).\nIn this exercise, I will be asking you some questions, the answers to which you should be able to arrive to by the set of techniques from tidyverse you’ve already learned, and visualize them using the ggplot2 functions you already know. When a new function will be needed, I will be dropping hints for you along the way!\nThis exercise is primarily intended for you to practice more ggplot2 visualization, because I think this is the most crucial skill in doing any kind of data analysis. Why? The faster you can get from an idea to a visualization of that idea, the faster you’ll be able to do research.\nYes, we could do this on the MesoNeo metadata or the IBD data too, but I think it would be too boring to stay with the same set of tables for this long. :)\nLet’s go!\n\nUse geom_point() to plot the relationship between time (x axis) andpropotionof Neanderthal ancestry (y` axis). Don’t worry about any other variables just yet (colors, facet, etc.).\nNote: The f4ratio table is too huge to plot every single point. Before you %&gt;% pipe it into ggplot(), subsample the rows to just 10% of it subset using this trick:\nf4ratio %&gt;%\n  sample_frac(0.1) %&gt;% # subsample 10% fraction of the data\n  ggplot(aes(...)) +\n    geom_point()\nHint: Look at ?sample_frac but also ?sample_n and check out their parameters. They are incredible useful for any kind of data exploration!\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nf4ratio %&gt;%\nsample_frac(0.1) %&gt;%\nggplot(aes(time, proportion)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\nYour plot now mashes together proportion values from two different statistic estimators. Fix this by facetting the plot, which will visualize both statistics in two separate panels. For the time being, keep doing the 10% subsampling step.\nHint: You can do this by adding + facet_wrap(~ statistic) to your previous code.**\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nf4ratio %&gt;%\nsample_frac(0.1) %&gt;%\nggplot(aes(time, proportion)) +\n  geom_point() +\n  facet_wrap(~ statistic)\n\n\n\n\n\n\n\n\n\n\n\n\nTake a look at the values on the x-axis of your figures. Purely for aesthetics, it is customary to visualize time going from oldest (left) to present-day (right). But currently the time direction is reversed. Fix this by adding a new layer with + scale_x_reverse()!\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nf4ratio %&gt;%\nsample_frac(0.1) %&gt;%\nggplot(aes(time, proportion)) +\n  geom_point() +\n  facet_wrap(~ statistic) +\n  scale_x_reverse()\n\n\n\n\n\n\n\n\n\n\n\n\nWhenever we visualize some biological statistics, we often want to compare their values to some kind of baseline (or a null hypothesis) value. In our case we have two:\n\nbaseline of “no Neanderthal ancestry” at y-axis value 0,\nbaseline of “known proportion of Neanderthal ancestry” today, which is about 3% (so y-axis value of 0.03).\n\nAdd those two baselines to your figure using a new function geom_hline() (standing for “horizontal line”). Like this:\n&lt;YOUR TIDYVERSE AND GGPLOT2 CODE ABOVE&gt; +\n  geom_hline(yintercept = 0.03, color = \"red\", linetype = \"dashed\") +\n  geom_hline(yintercept = 0, color = \"black\", linetype = \"dotted\")\nNote: You can immediately see the possibilities for adjusting the visuals like colors or linetype. Feel free to experiment with this!\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nf4ratio %&gt;%\nsample_frac(0.1) %&gt;%\nggplot(aes(time, proportion)) +\n  geom_point() +\n  facet_wrap(~ statistic) +\n  scale_x_reverse() +\n  geom_hline(yintercept = 0.03, color = \"red\", linetype = \"dashed\") +\n  geom_hline(yintercept = 0, color = \"blue\", linetype = \"dotted\")\n\n\n\n\n\n\n\n\n\n\n\n\nThere’s clearly a huge amount of statistical noise, represented particularly in the “indirect f4-ratio” figure. And we’re not even plotting all points because there’s just too many of them!\nAdditionally, what I cared about in this particular case (during my PhD) were not values for each individual person, but about the statistical trend in my data. An extremely important function for this is geom_smoot(), which fits a line through the data, capturing this trend.\nIn this next part, remove the subsampling done by sample_frac(), and also replace your geom_point() call in your figure with this: + geom_line(stat = \"smooth\", se = FALSE). What happens when you do this? Read ?geom_line to figure out what se = FALSE means. Why do we want to do this?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nf4ratio %&gt;%\nggplot(aes(time, proportion)) +\n  geom_line(stat = \"smooth\", se = FALSE) +\n  facet_wrap(~ statistic) +\n  scale_x_reverse() +\n  geom_hline(yintercept = 0.03, color = \"red\", linetype = \"dashed\") +\n  geom_hline(yintercept = 0, color = \"blue\", linetype = \"dotted\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\n\n\n\n\nWell, we removed a lot of noise but we also removed most of the interesting signal! This is because we are now basically “smoothing” (averaging) over a very important variable: the amount of migration from Europe to Africa (which was the primary research interest for me at the time). Let’s take a look at our table again:\n\nhead(f4ratio)\n\n# A tibble: 6 × 6\n  individual  time statistic       proportion rate_eur2afr replicate\n  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 eur_1      40000 direct f4-ratio   0.00687             0         1\n2 eur_2      38000 direct f4-ratio   0.00807             0         1\n3 eur_3      36000 direct f4-ratio   0.00192             0         1\n4 eur_4      34000 direct f4-ratio   0.0245              0         1\n5 eur_5      32000 direct f4-ratio   0.000390            0         1\n6 eur_6      30000 direct f4-ratio  -0.00267             0         1\n\n\nWhat we want is to plot not just a single line across every single scenario, but we want to plot one line for each value of our variable of interest, rate_eur2afr.\nLet’s add the variable into play by adding color = rate_eur2afr into your aes() call in the ggplot() function (i.e., in addition to the x = time and y = proportion you already got there).\nNote: You’ll get a warning which we will fix in the next step, don’t worry!\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nf4ratio %&gt;%\nggplot(aes(time, proportion, color = rate_eur2afr)) +\n  geom_line(stat = \"smooth\", se = FALSE) +\n  facet_wrap(~ statistic) +\n  scale_x_reverse() +\n  geom_hline(yintercept = 0.03, color = \"red\", linetype = \"dashed\") +\n  geom_hline(yintercept = 0, color = \"blue\", linetype = \"dotted\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: The following aesthetics were dropped during statistical transformation:\ncolour.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\nThe following aesthetics were dropped during statistical transformation:\ncolour.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\n\nDon’t worry about the warning, we’ll fix this soon!\n\n\n\n\nThe reason we got a warning here is a slightly confusing behavior of the smoothing function geom_smooth(). Because rate_eur2afr is a numerical variable, the smoothing function cannot plot a single smoothed line for each of its values, because there could be infinite numnber of them.\nFix the problem by adding group = rate_eur2afr right after your color = rate_eur2afr you added in the previous exercise. This explicitly instructs geom_smooth() to treat individual values of rate_eur2afr as discrete categories.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nf4ratio %&gt;%\nggplot(aes(time, proportion, color = rate_eur2afr, group = rate_eur2afr)) +\n  geom_line(stat = \"smooth\", se = FALSE) +\n  facet_wrap(~ statistic) +\n  scale_x_reverse() +\n  geom_hline(yintercept = 0.03, color = \"red\", linetype = \"dashed\") +\n  geom_hline(yintercept = 0, color = \"blue\", linetype = \"dotted\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s finalize our figure for publication! Use labs() to add proper x, y, and title descriptions (something like “Neanderthal ancestry proportion”, “years before present”, and “Expected Neanderthal ancestry trajectories”).\nDon’t forget about the title of the legend too, which you can adjust by + guides(color = guide_legend(\"EUR -&gt; AFR\\nmigration\")).\nFinally, add my favourite + theme_minimal() at the end.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nf4ratio %&gt;%\nggplot(aes(time, proportion, color = rate_eur2afr, group = rate_eur2afr)) +\n  geom_line(stat = \"smooth\", se = FALSE) +\n  facet_wrap(~ statistic) +\n  scale_x_reverse() +\n  geom_hline(yintercept = 0.03, color = \"red\", linetype = \"dashed\") +\n  geom_hline(yintercept = 0, color = \"blue\", linetype = \"dotted\") +\n  labs(x = \"Neanderthal ancestry proportion\",\n       y = \"years before present\",\n       title = \"Expected Neanderthal ancestry trajectories\",\n       subtitle = \"Results from a set of population genetic simulations\") +\n  guides(color = guide_legend(\"EUR -&gt; AFR\\nmigration\")) +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\n\n\n\nI hope that you now appreciate how adding little bits and pieces, one step at a time (and each step being very tiny and easily understandable!), makes it possible to create beautiful informative visualizations of any kind of tabular data!\n\n\nExercise 9: Combining figures\nSo, here’s an extremely cool revelation. Do you remember how we went through various data types and object classes in the R bootcamp? Numeric, logical, character types, etc?\nggplot() functions actually do return a (hidden) value, which is a normal object like any other R value! The fact that we immediately see a plot visualized is “just” a side effect that R provides for convenience! After all, when we write plotting code, we usually want to see the plot, right?\nWhat does this mean? Well, first of all, consider this bit of code:\n\np1 &lt;- ggplot()\n\nWeird, huh? Why would you do that? When you type p1 in your R console you get the usual (empty) figure!\n\np1\n\n\n\n\n\n\n\n\nOf course, we can also create (and save in a variable) a full figure like this:\n\np1 &lt;- ggplot(ibd_sum, aes(total_ibd)) + geom_density() + ggtitle(\"Normal x-axis scale IBD total sum\")\np1\n\n\n\n\n\n\n\n\nWe could also create and save another figure, this time a density on a log scale, just by adding a new layer to the p1 variable plot:\n\np2 &lt;- p1 + scale_x_log10() + ggtitle(\"Logarithmic x-axis scale IBD total sum\")\np2\n\n\n\n\n\n\n\n\nThis is extremely useful for making composite figures built out of individual panels (like you often see in scientific figures as panels A, B, C, etc.). The way we can do this is using a helper R package cowplot (don’t ask me about its name, I have no idea! :)) and its function plot_grid(). Check this out:\n\nlibrary(cowplot)\n\nplot_grid(p1, p2, labels = c(\"A\", \"B\"))\n\n\n\n\n\n\n\n\nThis is an extremely useful pattern because it basically eliminates the need for manually pasting figures in a graphics editor (which defies the purpose of reproducibility!). The sky is actually a limit, because you can place other plot_grid() calls within another plot_grid(), composing figures of various levels of complexity. Check out a figure from one of my papers, each panel is an independently made ggplot2 figure, all pasted together with plot_grid().\nFor instance, if we make p3 like this:\n\n# create a silly scatter plot of geographical coordinates of our samples\n# (do you see an outline of Europe? :))\np3 &lt;-\n  metadata %&gt;%\n  filter(continent == \"Europe\") %&gt;%\n  ggplot(aes(longitude, latitude)) +\n    geom_point() +\n  ggtitle(\"A silly toy map of Europe drawn using locations of metadata points\")\np3\n\nWarning: Removed 405 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nYou could combine them like this (this is just an example, don’t read too much into the meaning of putting these particular plots together).\n\nplot_grid(\n  p3,\n  plot_grid(p1, p2, labels = c(\"B\", \"C\")),\n  nrow = 2,\n  labels = \"A\"\n)\n\nWarning: Removed 405 rows containing missing values or values outside the scale range\n(`geom_point()`).",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "tidy-viz.html#advanced-exercises",
    "href": "tidy-viz.html#advanced-exercises",
    "title": "Data visualization",
    "section": "Advanced exercises",
    "text": "Advanced exercises\nIf you’ve already been introduced to ggplot2 before and the exercises above were too easy for you, here are a couple of additional challenges:\n\nTake a look at how the ggridges package can potentially improve the readability of many density distributions when plotted for several factors ordered along the y-axis. For instance, how could you use this package to visualize the (admittedly practically useless, but still) distributions of longitudes (on the x-axis) for all samples in our metadata, ordered along the y-axis by the country? Note that you might also need to learn about ordering factors using the R package forcats.\n\nAs an extra challenge, try to make this figure as beautiful, colorful, and magazine publication-ready using ggplot2 theming\n\nStudy how you could turn our visualization of total IBD vs number of IBD segments as a function of the degree of relatedness into a proper interactive figure using ggplotly. For instance, make an interactive figure which will show the exact count and total IBD, as well as the names of individuals upon hovering over each data point with a mouse cursor.",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "tidy-viz.html#take-home-exercise",
    "href": "tidy-viz.html#take-home-exercise",
    "title": "Data visualization",
    "section": "“Take home exercise”",
    "text": "“Take home exercise”\nTake your own data and play around with it using the concepts you learned above. If the data isn’t in a form that’s readily readable as a table with something like read_tsv(), please talk to me! tidyverse is huge and there are packages for munging all kinds of data. I’m happy to help you out.\nDon’t worry about “getting somewhere”. Playing and experimenting (and doing silly things) is the best way to learn.\nFor more inspiration on other things you could do with your data, take a look at the dplyr cheatsheet and the ggplot2 cheatsheet. You can also get an inspiration in the huge gallery of ggplot2 figures online!",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "tidy-viz.html#additional-resources",
    "href": "tidy-viz.html#additional-resources",
    "title": "Data visualization",
    "section": "Additional resources",
    "text": "Additional resources\nAs with our introduction to tidyverse, there’s so much more to ggplot2 than we had a chance to go through. I hope we’ll get through some spatial visualization features of R (which are tightly linked with tidyverse and ggplot2 in the modern R data science landscape), but there’s much more I wish we had time to go through.\nHere are a couple of resources for you to study after the conclusion of the workshop, or even during the workshop itself if you find yourself with more time and more energy to study.\n\nFactors are both a curse and blessing. When dealing with visualization, often a curse, because our factors are rarely ordered in the way we want. If our factors are numerical but not straightforward numbers (like our age_bin column above), their order in ggplot2 figures is often wrong. forcats is an incredible package which helps with this. Read about it in this vignette and then experiment with using it for figures in this session which featured factors.\ncowplot is another R package that’s very useful for complex composite figures. We touched upon it in this session, but only extremely vaguely. Study this introduction to learn much more about its features! You will never need Inkscape again.\nggridges is a very cool package to visualize densities across multiple categories. It can often lead to much more informative figures compared to overlapping geom_density() plots we did in our session. Here’s a great official introduction to the package.",
    "crumbs": [
      "Welcome to _tidyverse_",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "repro-projects.html",
    "href": "repro-projects.html",
    "title": "Structuring (R) projects",
    "section": "",
    "text": "Introduction\nYou’ve done a huge amount of programming already. Processing data, filtering data, annotating data, summarizing data, but also plotting data using the most powerful tool for scientific visualization ggplot2! I said it before but it’s worth repeating again—you have now learned 70% of all the tools you might need to do data science on a day-to-day basis using R. This is not an exaggeration.\nSure, you might not remember everything, but remembering comes from repetition and practice. You’ve been exposed to the most important concepts, and you know where to come back for more information when needed later.\nIn this session on reproducibility, we will take things a bit easier than the earlier, programming-heavy sessions earlier.\nInstead of focusing on specific programming techniques and R functions, we’ll go through guidelines on how to organize (and run) computational projects on a practical basis—taking what you’ve learned, and learning how to do it in practice.\nIn this session your tasks will be to take bits of code from previous exercises on IBD and other data (currently scattered over multiple separate scripts and files, or maybe even nowhere!) and learn how to organize them in a structure suitable for maximizing reproducibility and effectiveness of doing computational research.",
    "crumbs": [
      "Reproducible computing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Structuring (R) projects</span>"
    ]
  },
  {
    "objectID": "repro-projects.html#exercise-1-creating-a-formal-r-project",
    "href": "repro-projects.html#exercise-1-creating-a-formal-r-project",
    "title": "Structuring (R) projects",
    "section": "Exercise 1: Creating a formal R project",
    "text": "Exercise 1: Creating a formal R project\nSo far, you’ve worked in an “anonymous” RStudio session, with scripts saved somewhere on your filesystem, disconnected from the data, or with ad hoc interactions with an R console (i.e., with commands not saved anywhere). This is not how reproducible research should be done to be sustainable over weeks or months of working on a project.\nTo move forward, set up a formal R project and proper project structure by following the following steps:\n\nClick File -&gt; New Project...\nIn the new window, click on New Directory -&gt; New Project\nUnder \"Directory name\" type in \"simgen-project\" (the name of our course, and the directory where all project files will be stored). Pick where you would like to save that directory (this doesn’t matter, just put the project somewhere you can later find it). Check the \"Open in new session\" box, leave the other options related to “git” and “renv” unchecked.\nThen click on the “Create Project” button.\n\nThis will create a new RStudio window. Your original RStudio window (where you worked so far) is still open. The task for the following exercises will be to convert the (probably disorganized) bits of code into a “proper reproducible R project”.\nNotice the new simgen-project.Rproj file that is currently the only file in your project directory! We will come back to it soon.",
    "crumbs": [
      "Reproducible computing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Structuring (R) projects</span>"
    ]
  },
  {
    "objectID": "repro-projects.html#exercise-2-seting-up-a-project-file-structure",
    "href": "repro-projects.html#exercise-2-seting-up-a-project-file-structure",
    "title": "Structuring (R) projects",
    "section": "Exercise 2: Seting up a project file structure",
    "text": "Exercise 2: Seting up a project file structure\nWhat makes for a good computational project structure? There are endless possibilities but a good guidelines are:\n\nBe consistent – put files that “belong together” in the same place (i.e., in the same directory). For example, an Excel table shouldn’t be saved in a directory with scripts, a PDF with a plot shouldn’t be in a directory with tables.\nBe predictable – even a person unfamiliar with your project should be able to guess where is what just by looking at your folder. Remember, when you publish your paper or your thesis, you will have to provide all your data and scripts as supplementary materials, so others should be able to understand all of them!\nDocument everything – write a lot of # comments in code, describing what different pieces of code do, and why you wrote things in a certain way. Your future self will thank you!\n\n\nNow let’s set up an example computational project structure for our IBD data our ancient DNA metadata (we’ll leave the \\(f_4\\)-ratio Neanderthal estimates for next chapter), just like you would do this for a real study.\nNote: This is all just an example! Again, as long as you follow the guidelines numbered above, everything works!\nIn the Files pane of RStudio, click on \"New Folder\" and create the following directories in the “root of your project directory simgen-project/”:\n\ncode/\ndata/, and within it create directories raw/ and processed/\nfigures/\nresults/\nreports/\n\nNote: You can do all this manually, or you can play around with doing it using code with the incredibly useful function ?dir.create! For instance, a single command to do all of the above could be the following. (If you’re curious about the recursive = TRUE, look at help of ?dir.create.)\n\ndir.create(\"code/\")\ndir.create(\"data/raw\", recursive = TRUE)\ndir.create(\"data/processed\", recursive = TRUE)\ndir.create(\"figures/\")\ndir.create(\"results/\")\ndir.create(\"reports/\")\n\n\nWhen you hit the \"Refresh file listing\" circular arrow button in the top-right of the \"Files\" pane, you should now see all the directories you just created. Make sure you see them, and not just the simgen-project.Rproj file like before!",
    "crumbs": [
      "Reproducible computing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Structuring (R) projects</span>"
    ]
  },
  {
    "objectID": "repro-projects.html#exercise-3-building-an-example-reproducible-pipeline",
    "href": "repro-projects.html#exercise-3-building-an-example-reproducible-pipeline",
    "title": "Structuring (R) projects",
    "section": "Exercise 3: Building an example reproducible pipeline",
    "text": "Exercise 3: Building an example reproducible pipeline\nLet’s get something out of the way first. Of course, if there are a million possible ways how to properly structure a computational project, there are infinite ways how a “reproducible research pipeline” should be actually organized.\nObviously, all research projects are different, they focus on different kinds of data (archaeological data, ancient DNA sequences, linguistic data, field observation data, etc.), so naturally they will require different code which will have to be structured in different ways.\nStill, there are some common workflows which practically every single computational scientific research study does:\n\nData gathering – in our IBD and metadata examples, this involved downloading data from the internet.\nData processing – in our case, this involved filtering the data, processing it to bin individuals based on their ages, joining IBD data with metadata, etc.\nData analysis – this involves computing summary statistics, creating figures, etc.\n\nThe dirty secret of many scientific research studies (especially in the “old days”) is that all of these steps are often clumped together in huge scripts, and its very difficult (even for the author) to sometimes tell where is what. This can be a big problem, especially if a bug needs to be fixed, a new step of a processing pipeline needs to be added, etc.\nIt gets even worse, when after a long time you come back to a project and need to remember some details about where in your code is that one line that does something that needs changing!\nLet’s demonstrate how you could organize your code in practice, and hopefully you’ll see how investing a bit more time and thinking into properly organizing code in your research project makes your life a lot easier in the long run (and much easier for everyone who might pick up your project later too).\n\n\n1. Data gathering\nCreate a standalone R script (File -&gt; New File -&gt; R Script). Paste the following code to that script, and then save it as 01_download_data.R in the root of your simgen-project project directory.\nDon’t copy it without skimming through it and understanding it! You should recognize these commands from our earlier exercises on tidyverse? Ideally, you’ve written those commands before yourself. That example code was a little messy and random, because it was structured as an ad hoc tutorial. What we’re doing here is showing how to organize computational research properly.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nYou can find the complete script here.\n\n\n\n\nWhen you have your script 01_download_data.R, run it by calling source(\"01_download_data.R\") in your R console and observe what happens. You can also press Cmd/Ctrl + Shift + S — a very useful shortcut!\nThen, click through your \"Files\" panel and look in data/raw — do you see files that your new script should create?\nNote: Notice the cat(\"some kind of message\\n\") command in the script. This is extremely useful for printing log information about a (potentially) log running process. If you’re confused about what it does, write this into your R console: cat(\"Hello friend!\\n\").\n\nCreating 01_download_data.R is a first step towards reproducibility. Downloading of all data set now happens in a dedicated script, which means:\n\nWe only have to run it once, and have all data available in data/raw for later use.\nIf we have to include a new data set to be included, we just edit that script 01_download_data.R and run it again!\nExcept for running the script top to bottom, we don’t do any “manual work”.\n\nThis doesn’t sound like much, but it’s absolutely crucial. Automation is the most important aspect of reproducible research. Unless something isn’t fully automated, it’s not reproducible.\nNow let’s move to the next step!\n\n\n\n2. Data processing\nCreate a standalone R script (File -&gt; New File -&gt; R Script). Add the following code to that script, and then save it as 02_process_data.R in the root of your simgen-project project directory.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nYou can find the complete script here.\n\n\n\nAgain, please don’t just blindly copy it! Look at the script and try to recognize the commands from our earlier exercises on tidyverse. You ran all of this yourself, manually, command-by-command, in the R console.\nWhen you have your script, close RStudio.\n…\nLet’s pretend that some time passed, you’re done for the day and went home.\n…\nFind the location of your simgen-project directory on your computer, go to that directory, and then double-click on the R project file simgen-project.Rproj. You’ll get your old session back, just like you left it (including history!\n\nNow press Cmd/Ctrl + Shift + S to run the processing script, and observe what happens! Then click through your \"Files\" panel and look in data/processed — do you see your files, process and tidy, ready for analysis?\n\n\n\n3. Data analysis\nHopefully you’re now starting to get an idea about what a well-organized, reproducible pipeline means. It’s about structuring the directory where your project lives, and about partitioning your code into scripts which represent logical components of your project — from downloading data (and saving it to a particular storage location), and processing it (and again saving it to a proper storage location), and, finally, to answering research questions based on this data.\nYour project structure should now look something like this. Notice that there are already data files present, not just directories:\n├── 01_download_data.R           # data fetching code\n├── 02_process_data.R            # data processing code\n├── code                         # directory for future custom functions\n├── data\n│   ├── processed                # processed / filter / cleaned data\n│   │   ├── ibd_segments.tsv\n│   │   ├── ibd_sum.tsv\n│   │   └── metadata.tsv\n│   └── raw                      # raw unprocessed data\n│       ├── ibd_segments.tsv\n│       └── metadata.tsv\n├── figures                      # location for PDF figures\n├── results                      # various other output data (tables, cache files, etc.)\n├── reports                      # location for slides and reports (next section!)\n└── simgen-project.Rproj\nYou have implemented the first two steps of a data processing pipeline. Now it’s your job to implement additional components of the project pipeline, again using a couple of examples of what we’ve already done before, but this time properly integrating it into a nice and tidy package.\n\n\nGenerating figures\nCreate a standalone R script called 03_plot_metadata.R, which will:\n\nRead the table in data/processed/metadata.tsv using read_tsv() function.\nCreate the following figures using ggplot2 package and the function ggsave() you’ve learned about earlier.\n\n\nCount of samples in each age_bin of metadata (use geom_bar()).\nA histogram of coverage across different age_bin categories, only for individuals with age &gt; 0 (i.e., ancient individuals), with one facet showing one histogram from a given age_bin category (use geom_histogram() and facet_wrap()).\n\n\nSave the figure(s) in the figures/ directory (whether you save individual PDF for each figure, or put multiple figures into one PDF is up to you).\n\nNote: Remember that you can use the function ggsave() to save ggplot2 figures stored in normal R variables, or the pdf() & dev.off() trick!\nHint: If you need help, we solved all of these in previous exercises.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nYou can find my solution here.\n\n\n\n\nCreate a standalone R script called 04_plot_ibd_sharing.R, which will:\n\nRead the table in data/processed/ibd_merged.tsv using read_tsv() function.\nFilter the table to only length &gt; 5 (segments longer than 5cM) and time_pair == \"present-day:present-day\" (only present-day humans).\nCreate the following figures using ggplot2 package and the function ggsave() you’ve learned about earlier, saving all of these figures into a single file figures/ibd_sharing.pdf:\n\nFor each pair of continents region_pair == \"Europe:Europe\", region_pair == \"America:America\", region_pair == \"Asia:Asia\", and region_pair == \"Africa:Africa\", plot a histogram of lengths of IBD longer than 5 cM.\nUse facet_wrap(~ country_pair) to visualize each histogram for each country pair in its own facets.\nTherefore, your PDF should have four plots (one plot per page), with each plot having a number of histograms, one histogram per facet.\n\n\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nYou can find my solution here.\n\n\n\n\nCreate a standalone R script called 05_plot_ibd_relatedness.R, which will:\n\nRead the table in data/processed/ibd_sum.tsv using read_tsv() function.\nRecreate that beautiful figure we made, showing a scatterplot of total_ibd against n_ibd, and highlighting which individuals are duplicated, who is a 1st degree relative, 2nd degree relative, etc.\nSave that figure to figures/ibd_relatedness.pdf.\n\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nYou can find my solution here.",
    "crumbs": [
      "Reproducible computing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Structuring (R) projects</span>"
    ]
  },
  {
    "objectID": "repro-projects.html#exercise-4-generating-other-results",
    "href": "repro-projects.html#exercise-4-generating-other-results",
    "title": "Structuring (R) projects",
    "section": "Exercise 4: Generating other results",
    "text": "Exercise 4: Generating other results\nOf course, the main output of our research is visualizations. But sometimes, a good old table is also very useful! Our papers often have both plots and tables.\nJust to pracice a little bit more, create a standalone R script called 06_sumarize_coverage.R, which will:\n\nRead the table in data/processed/metadata.tsv using read_tsv() function.\nfilter() the table to only age &gt; 0 (only rows for ancient individuals) and continent == \"Europe\" (i.e., only aDNA European samples).\nsummarize() the rows to compute across grouping based on country and age_bin (remember group_by()?):\n\navg_coverage as a mean() of the coverage column,\nn_samples as the n() number of rows.\n\nThe rows should be sorted in a descending order by the column n_samples.\nStore the result of the filter / group_by / summarize %&gt;% pipeline as df_summary, and save the table using write_tsv() in a file called results/coverage_summary.tsv.\n\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nYou can find my solution here.",
    "crumbs": [
      "Reproducible computing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Structuring (R) projects</span>"
    ]
  },
  {
    "objectID": "repro-projects.html#exercise-5-adding-a-readme-file",
    "href": "repro-projects.html#exercise-5-adding-a-readme-file",
    "title": "Structuring (R) projects",
    "section": "Exercise 5: Adding a README file",
    "text": "Exercise 5: Adding a README file\nIt’s a good idea for every project to have a README file at the root of the directory. Traditionally, this file is formatted in a Markdown format, which makes it easy to read in plain text, but is easily visualized on the web (many websites such as GitHub and other data repositories render Markdown text beautifully).\nLook up the basic syntax of Markdown formatting, then create File -&gt; New File -&gt; Markdown File (not R Markdown file!). Then write a short list (again, take a look at the Markdown syntax and read how to create a list), which will describe, in one sentence:\n\nWhich directories are in your project, and what they countain (briefly!)\nWhich scripts you created and what they do.\nWhich results are created (so far just figures), and which scripts created those results.\n\nHint: Try clicking the “Visual” button on top of your editing window. I personally don’t use it because I like to write Markdown in plain text, but it can be very helpful, because you can create lists and other formatting by clicking with the mouse, instead of having to type Markdown syntax manually.\n\nCreating a README is an important practice of documenting computational projects. If anyone comes back to your project later (even you!) they will have a summary of what they project contains, what code it contains, and what that code does.\nThis is infinitely easier than having to read code and decipher what it does!\nAlways have a README file in your project root directory, especially when you publish it in a thesis or in a paper.",
    "crumbs": [
      "Reproducible computing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Structuring (R) projects</span>"
    ]
  },
  {
    "objectID": "repro-projects.html#what-were-still-missing-theoratically",
    "href": "repro-projects.html#what-were-still-missing-theoratically",
    "title": "Structuring (R) projects",
    "section": "What we’re still missing (theoratically)",
    "text": "What we’re still missing (theoratically)\nHere are a couple of topics which can be extremely helpful for pushing reproducibility even further, but that we don’t have bandwidth to go through, unfortunately. I’m mentioning them in case you’d like to do more studying on your own, perhaps as you get further into your own research projects.\nI use all of these in my own work, but I consider them much more advanced topics. Our workshop focuses on the fundamentals of reproducible research. In this sense, you could do very much\n\nBackups. Unless you use git actively (which solves the backup problem as a side-effect—see below), you need to back up regularly. Even right-clicking your simgen-project directory, archiving it in a zip file named like \"yyyy-mm-dd-simgen-project.zip\" regularly will get you through any trouble.\ngit – an incredibly powerful version control system, which is also at the back end of the online code repository service GitHub. This is the big one and probably the most important tool I’ve ever learned as a programmer. It would require an entire workshop on its own, unfortunately.\nrenv – an R package which allows you to lock the exact versions of R packages used in your project (and have anyone else restore those exact versions of packages at a later point).\ntargets – an R package for building and orchestrating truly complex computational pipelines. For instance, it can track whether or not a particular function or piece of data has changed and, therefore, whether other components of a pipeline downstream from it need to be re-run (unlike our 01_download_data.R or 02_process_data.R scripts which need to be re-run in their entirety after each modification).",
    "crumbs": [
      "Reproducible computing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Structuring (R) projects</span>"
    ]
  },
  {
    "objectID": "repro-projects.html#closing-remarks",
    "href": "repro-projects.html#closing-remarks",
    "title": "Structuring (R) projects",
    "section": "Closing remarks",
    "text": "Closing remarks\nOur workshop focuses primarily on the R programming language and various R packages useful for data analysis and plotting. Of course, real-world computational projects often rely on other programs and scripts written in traditional shell (like BASH) or Python. However, the same ideas apply, regardless of the exact language. In fact, in the sessions of population genomics and inference, we will see how everything fits very naturally in the overall structure we’ve established here.",
    "crumbs": [
      "Reproducible computing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Structuring (R) projects</span>"
    ]
  },
  {
    "objectID": "repro-quarto.html",
    "href": "repro-quarto.html",
    "title": "Quarto reports and slides",
    "section": "",
    "text": "Introduction\nFrom the previous section, you have now set up a proper computational project structure. You’ve added your pipeline R scripts which download, manipulate, filter, and otherwise process “raw data” into “processed data”, the latter being the starting point of data analysis, visualization, and statistical inference. You’ve also learned to write standalone scripts to do all of that.\nHonestly, what you’ve learned in the previous chapter on general R project setup is everything you might need to get through a Master’s project or even a PhD. To reiterate:\nIn principle, you don’t need anything else beyond nicely organized scripts which produce results (tables, figures, etc.) in an equally organized way.\nIn this session, you will learn about another possibility of doing reproducible data science. Not a replacement for the previous script-based workflows, but a complementary approach to doing the same thing. I personally use both approaches.\nLet’s introduce the Quarto system for reproducible scientific research. First, please take a moment to watch this wonderful presentation (you can stop watching by about 15 minutes, when the discussion turns to building websites).",
    "crumbs": [
      "Reproducible computing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Quarto reports and slides</span>"
    ]
  },
  {
    "objectID": "repro-quarto.html#introduction",
    "href": "repro-quarto.html#introduction",
    "title": "Quarto reports and slides",
    "section": "",
    "text": "You’re data processing code is now fully automated and completely reproducible.\nYou can also generate results, figures, and summaries in an equally automated way.",
    "crumbs": [
      "Reproducible computing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Quarto reports and slides</span>"
    ]
  },
  {
    "objectID": "repro-quarto.html#exercise-1-creating-a-reproducible-report",
    "href": "repro-quarto.html#exercise-1-creating-a-reproducible-report",
    "title": "Quarto reports and slides",
    "section": "Exercise 1: Creating a reproducible report",
    "text": "Exercise 1: Creating a reproducible report\nThe previous exercises were focused on our metadata and IBD data, and turning our disorganized pile of code into proper project structure.\nIn this session, we’ll do a similar thing, but focus instead on the analysis of Neanderthal proportions in a time-series of aDNA individuals from Europe discussed here.\nFirst create a new blank Quarto document by doing the following:\n\nClick on File -&gt; New File -&gt; Quarto Document....\nClick on Create Empty Document.\nSave the file under notebooks/neand_ancestry.qmd.\n\n(I like to call what we’ll be creating as “a notebook”, because it’s very similar to a normal lab notebook).\n\nIn your new Quarto file, first make sure you have the “Source” view turned on (top left of your editor window). For now at least, you will want to switch to “Visual” when you start writing!\n\nPaste in the following template (just to save you a lot of annoying typing):\n\n\n\n\n\n\nClick to reveal code to be added to your Quarto document\n\n\n\n\n\n\n\n---\ntitle: \"Neanderthal ancestry throuhg time\"\nauthor: \"WRITE YOUR NAME HERE :)\"\ndate: now\nformat: html\nself-contained: true\n---\n\n# Basic processing\n\n## Load required packages\n\n```{r}\n# add required code here\n```\n\n## Read in data from the internet\n\n```{r}\n# add required code here\n```\n\n## Inspect the format of the table\n\nFirst couple of rows of the data:\n\n```{r}\n# add required code here\n```\n\nA `glimpse` at the column values:\n\n```{r}\n# add required code here\n```\n\n# Visualizing data\n\n## Visualize the 10% of the data points\n\n```{r}\n# add required code here (it used the `sample_frac()` function)\n```\n\n## Fit a smoothed out line through all parameter combinations\n\n```{r}\n# add required code here which plotted the beautiful finalized figure\n# of smoothed lines of Neanderthal ancestry trajectories\n```\n\n# Fitting a formal linear regression model\n\n## Extract data for \"direct\" and \"indirect\" $f_4$-ratio\n\nFilter data on the parameter setting of `rate_eur2afr == 0.2`, and\nextract statistics for \"direct\" and \"indirect\" $f_4$-ratio statistics,\nsaving them in data frames `df_direct` and `df_indirect`:\n\n```\ndf_direct &lt;- filter( ... add requried code here ...)\ndf_indirect &lt;- filter( ... add required code here ...)\n```\n\n## Run linear regression on \"direct\" vs \"indirect\" $f_4$-ratio\n\nRun a linear model using the `lm()` function (help can be found under\n`?lm`) to find a p-value of the statistical significance between trajectories\nof \"direct\" and \"indirect\" $f_4$-ratio estimates:\n\n```{r}\n# computing a linear regresion on df_direct and df_indirect data\nlm_direct &lt;- lm(proportion ~ time, data = df_direct)\nlm_indirect &lt;- lm(proportion ~ time, data = df_indirect)\n```\n\nRun `summary()` on both objects with result of `lm()` to get a p-value.\nWhich statistic gives a statistically significant decline of Neanderthal\nancestry over time?\n\n```{r}\n# run summary on the lm fit of the direct f4-ratio\n```\n\n```{r}\n# run summary on the lm fit of the indirect f4-ratio\n```\n\n# Conclusions\n\nHere is where you could conclude what you found out in the course of the analysis.\nTake home messages, pointers to follow up analyses in other later scripts and/or\nQuarto notebooks, etc.\n\n\n\n\n\n\nClick on the \"=&gt; Render\" button on top of your editor window and see the magic happen!\n\nYou can also check the \"Render on Save\" box on top of your editor window. See what happens when you save the document using CTRL / CMD + S. This can slow things down for long-running analyses, but is very convenient otherwise.\n\nIn a Quarto document (of any kind, reports, presentations, anything), this is a very document component. It’s called a “code block”:\n```{r}\n# here is your code\n```\nWhenever a Quarto document is rendered, R executes code in these code blocks! It then includes a figure, prints the result, etc., which then becomes the part of the resulting document. I hope you can now appreciate how useful this is for:\n\nMaking your reserach more reproducible – the code and the results are part of a single document, which is ran top to bottom, automatically!\nMaking your research easier to do – this is effectively a lab notebook of your research activity for that particular project. You can write notes, comments, reminders, conclusions, etc.\n\nThis allows you to avoid hunting down for “which bit of code and where created this particular figure?”, which is very very stressful at times, especially close to deadlines.",
    "crumbs": [
      "Reproducible computing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Quarto reports and slides</span>"
    ]
  },
  {
    "objectID": "repro-quarto.html#exercise-2-completing-your-report",
    "href": "repro-quarto.html#exercise-2-completing-your-report",
    "title": "Quarto reports and slides",
    "section": "Exercise 2: Completing your report",
    "text": "Exercise 2: Completing your report\nNow that you’ve rendered the document, you can see that I left you guidelines and blanks to fill in in the Quarto template. Using the set of exercises on the topic of Neanderthal proportion in a time-series of aDNA individuals from Europe discussed here, fill in the blanks accordingly.\nTry to get in the mindset of using this document as your lab notebook! If you didn’t manage to get through the exercises on analyzing and plotting Neanderthal ancestry proportions at the link above, use this opportunity to work on those exercises, this time using your Quarto document as a means to solve them.\nHint: Again, if you ever need help, here it is:\n\n\n\n\n\n\nClick to see the completed Quarto document\n\n\n\n\n\nYou can find the complete source document here.\nYou can download the rendered version here.",
    "crumbs": [
      "Reproducible computing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Quarto reports and slides</span>"
    ]
  },
  {
    "objectID": "repro-quarto.html#exercise-4-adjusting-code-chunk-options",
    "href": "repro-quarto.html#exercise-4-adjusting-code-chunk-options",
    "title": "Quarto reports and slides",
    "section": "Exercise 4: Adjusting code chunk options",
    "text": "Exercise 4: Adjusting code chunk options\nYou can see that the final report contains both the code and the results of this code. Sometimes you don’t want that, particularly when you want to create not a document, but presentation slides like you will do in the next exercise.\nMy favourites (and the only ones I personally remember) are these ones:\n\nShow code, but hide it first (reader has to click)! This is my favourite, because sometimes your supervisor doesn’t want to read code, they just want to see a figure. :)\n\n```{r}\n#| code-fold: true\n\n# here is your code which will not be shown in the report\n```\n\nDon’t show code, but show results:\n\n```{r}\n#| echo: false\n\n# here is your code which will not be shown in the report\n```\n\nShow code, but don’t evaluate it (it produces no results):\n\n```{r}\n#| eval: false\n\n# here is your code which will not be shown in the report\n```\nExperiment with the above mentioned options in your report (or slides in the next exercise). Here’s a very useful summary of many more options.",
    "crumbs": [
      "Reproducible computing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Quarto reports and slides</span>"
    ]
  },
  {
    "objectID": "repro-quarto.html#exercise-4-creating-slides",
    "href": "repro-quarto.html#exercise-4-creating-slides",
    "title": "Quarto reports and slides",
    "section": "Exercise 4: Creating slides",
    "text": "Exercise 4: Creating slides\nHere’s my favourite aspect of Quarto. You can not only create fully reproducible “lab notebooks”, but you can also create automatically generated slides. This is extremely useful as a means to have realiable means to have up-to-date presentations for group meetings, etc.\nCreate a new Quarto Document (File -&gt; New File -&gt; Quarto Presentation... -&gt; Create Empty Document.). Then copy the entire contents of yourreports/neand_ancestry.qmd into this new document, and save it as reports/neand_ancestry_slides.qmd.\nThen change this one single line in the header at the top of your file, changing format: html to format: revealjs.\nThen click the \"Render\" button again! Observe the magic happen!\n\nIt’s pretty obvious to you now that slides have different requirements than documents. For one, including lots of code (or maybe any code) isn’t that useful. Additionally, showing library(...) calls in a presentation doesn’t make any sense either plus, slightly different formatting might be needed.\nTake a look at this overview of the Quarto slides functionality. Then edit your slides (remove unnecessary headings/slide titles, etc.) to make them more suitable for presentation in a meeting.\nFor a more practical set of tips (how to include animated slides, how to do formatting, how to include images, etc.), you can take a look at the source .qmd file for the [introduction presentation for this workshop] (https://github.com/bodkan/simgen/blob/main/slides_whoami.qmd). You can click through them yourself interactively here.\nNote: Remove slides which are not useful, show only code which is important (like the lm model?), focus on figures and statistical summary() on the linear regression results.\n\n\n\n\n\n\nClick to see the see my attempt at cleaner slides\n\n\n\n\n\nYou can find the complete source document here.\nYou can download the slides here.",
    "crumbs": [
      "Reproducible computing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Quarto reports and slides</span>"
    ]
  },
  {
    "objectID": "repro-quarto.html#exercise-5-recording-r-session-information",
    "href": "repro-quarto.html#exercise-5-recording-r-session-information",
    "title": "Quarto reports and slides",
    "section": "Exercise 5: Recording R session information",
    "text": "Exercise 5: Recording R session information\nThe following command should be included at the end of your “Quarto reports”. When you run it, how would you read and interpret the information it provides? What do you think is the most important information which might be missing in case you need to pick up someone else’s project or script?\n\nsessionInfo()\n\nR version 4.5.0 (2025-04-11)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Tallinn\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.5.0    fastmap_1.2.0     cli_3.6.5        \n [5] tools_4.5.0       htmltools_0.5.8.1 rstudioapi_0.17.1 yaml_2.3.10      \n [9] rmarkdown_2.29    knitr_1.50        jsonlite_2.0.0    xfun_0.52        \n[13] digest_0.6.37     rlang_1.1.6       evaluate_1.0.4   \n\n\nCreate a new chunk at the end of your document and add this command to this chunk to be included every time it is rendered.",
    "crumbs": [
      "Reproducible computing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Quarto reports and slides</span>"
    ]
  },
  {
    "objectID": "repro-quarto.html#exercise-6-the-entire-point-of-doing-all-this-workshop",
    "href": "repro-quarto.html#exercise-6-the-entire-point-of-doing-all-this-workshop",
    "title": "Quarto reports and slides",
    "section": "Exercise 6: The entire point of doing all this workshop",
    "text": "Exercise 6: The entire point of doing all this workshop\nIn this final exercise, I would like you to take whatever data you have, and try to use some of what you’ve learned so far — about R programming, about tidyverse, about ggplot2 — and create a Quarto report in which you will put some of what you’ve learned into practice.\nAlternatively, if you have a messy set of scripts (we all have that, even my stuff is messy, don’t worry) ready and some results already generated, try to transform them into what would be a nice, automated, and reproducible Quarto report.\nYou could also do work on transforming code which you now realize could be organized in a more structured way, perhaps like we’ve learned in the previous session on building R pipelines, into a tidy step-by-step cascade of R scripts.\nThe sky is a limit!",
    "crumbs": [
      "Reproducible computing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Quarto reports and slides</span>"
    ]
  },
  {
    "objectID": "spatial-sf.html",
    "href": "spatial-sf.html",
    "title": "Working with tidy spatial data",
    "section": "",
    "text": "Setup and data processing\nThis chapter is still a work in progress. It’s a very advanced topic and it’s a little hard to guess how much use it is to the average intended participant of this workshop. If you do care about this (it’s my favourite topic, after all!), I will be adding materials and tutorials throughout the course which you will be able to play around with (and learn how to apply to your own data) towards the end of the course.\nWhat follows are just a few intermediate sketches of code ideas, using our metadata data (for point locations and region-based spatial summaries) and ibd_sum data (for links between spatial locations) as motivation examples.\nsf cheatsheet!\nRead spatial packages:\nlibrary(sf)\n\nLinking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE\n\nlibrary(rnaturalearth)\nRead other data analysis packages:\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(readr)\nlibrary(ggplot2)\nRead our pre-processed data:\nsource(\"scripts/ibd_utils.R\")\nRead and process our IBD and metadata using our utility functions:\n# download and process the metadata and IBD data set\nmetadata &lt;- process_metadata(bin_step = 5000)\n\nDownloading and processing metadata...\n\nibd_segments &lt;- process_ibd()\n\nDownloading and processing IBD data...\n\n# combine the IBD table with metadata information\nibd_merged &lt;- join_metadata(ibd_segments, metadata)\n\nJoining IBD data and metadata...\nFilter metadata to only individuals who have longitude and latitude values available (use filter(), !is.na() and the & operator):\nmetadata &lt;- filter(metadata, !is.na(longitude) & !is.na(latitude))\n\nhead(metadata)\n\n# A tibble: 6 × 9\n  sample country    continent   age coverage longitude latitude hgYMajor age_bin\n  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;fct&gt;  \n1 baa01  South Afr… Africa    1908.   13.5        31.2    -29.5 A1b      (0,500…\n2 bab01  South Afr… Africa    2040.    1.30       31.2    -29.5 A1b      (0,500…\n3 I9133  South Afr… Africa    1970     2.08       18.5    -32.0 A1b      (0,500…\n4 I9028  South Afr… Africa    2103     1.19       18.0    -32.8 A1b      (0,500…\n5 I9134  South Afr… Africa    1198.    0.699      18.0    -32.8 &lt;NA&gt;     (0,500…\n6 ela01  South Afr… Africa     493    13.5        29.1    -28.9 &lt;NA&gt;     (0,500…\nUse the function class() to check what kind of data type our metadata is (I know you know it’s a data frame, but let’s do this for practice):\nclass(metadata)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\nUse function st_as_sf() to convert the (non-spatially aware) data frame object into one that can be interpreted as spatial data. Save the result as a sf_metadata variable. What do you see when you type sf_metadata to your R console and when you run class() on that variable? Compare this to the output you got for the original metadata object. (You’ll probably get an error message, don’t worry and move along!)\nsf_metadata &lt;- st_as_sf(metadata, crs = \"EPSG:4326\")\n\nError in st_sf(x, ..., agr = agr, sf_column_name = sf_column_name): no simple features geometry column present\nThe above got you a rather cryptic error. Honestly, even I don’t understand it. But, here’s how you fix it: run st_as_sf(metadata, coords = c(\"longitude\", \"latitude\")) instead.\nWhy do you think this is needed? Think about a situation in which the spatial coordinate columns in your data frame were called something else than “longitude” and “latitude”?\nWhen you have created the sf_metadata successfully, go back to the previous exercise.\nsf_metadata &lt;- st_as_sf(metadata, crs = \"EPSG:4326\", coords = c(\"longitude\", \"latitude\"))\n\nhead(sf_metadata)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 17.95 ymin: -32.81 xmax: 31.22 ymax: -28.92\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 8\n  sample country      continent   age coverage hgYMajor age_bin \n  &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;fct&gt;   \n1 baa01  South Africa Africa    1908.   13.5   A1b      (0,5000]\n2 bab01  South Africa Africa    2040.    1.30  A1b      (0,5000]\n3 I9133  South Africa Africa    1970     2.08  A1b      (0,5000]\n4 I9028  South Africa Africa    2103     1.19  A1b      (0,5000]\n5 I9134  South Africa Africa    1198.    0.699 &lt;NA&gt;     (0,5000]\n6 ela01  South Africa Africa     493    13.5   &lt;NA&gt;     (0,5000]\n# ℹ 1 more variable: geometry &lt;POINT [°]&gt;\nThe result in sf_metadata looks almost like a data frame, right? What do you get when you run class(sf_metadata) again?\nclass(sf_metadata)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\nThe st_as_sf() function converts a plain data frame into a “spatially annotated data frame”. Note the additional information about “Dimension”, and “CRS” (Coordinate Reference System “WGS 84”). Take a look at the Wikipedia article about WGS 84 and decipher what this could mean and why could this be needed.\nUnlike tidyverse munging of data frames (where we introduced ggplot2 visualizations at the end), discussing spatial data is much easier to do the other way around, starting from a visualization.\nIt turns out that sf spatial data is basically a normal data frame with a little bit of extra annotation. This means two things:\nAs a reminder, the general pattern of plotting with ggplot2 was something like this:\nTry plotting your sf_metadata using the same pattern, but with a new geom_sf():\nsf_metadata %&gt;%\nggplot(aes(color = coverage)) +\n  geom_sf()\nsf_metadata %&gt;%\nfilter(continent == \"Europe\") %&gt;%\nggplot(aes(color = coverage)) +\n  geom_sf()\nsf_countries &lt;- ne_countries(continent = \"Europe\")\nsf_countries\n\nSimple feature collection with 39 features and 168 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: 2.053389 xmax: 180 ymax: 81.2504\nGeodetic CRS:  WGS 84\nFirst 10 features:\n         featurecla scalerank labelrank sovereignt sov_a3 adm0_dif level\n19  Admin-0 country         1         2     Russia    RUS        0     2\n22  Admin-0 country         1         3     Norway    NOR        0     2\n44  Admin-0 country         1         2     France    FR1        1     2\n111 Admin-0 country         1         3     Sweden    SWE        0     2\n112 Admin-0 country         1         4    Belarus    BLR        0     2\n113 Admin-0 country         1         3    Ukraine    UKR        0     2\n114 Admin-0 country         1         3     Poland    POL        0     2\n115 Admin-0 country         1         4    Austria    AUT        0     2\n116 Admin-0 country         1         5    Hungary    HUN        0     2\n117 Admin-0 country         1         6    Moldova    MDA        0     2\n                 type  tlc   admin adm0_a3 geou_dif geounit gu_a3 su_dif\n19  Sovereign country    1  Russia     RUS        0  Russia   RUS      0\n22  Sovereign country &lt;NA&gt;  Norway     NOR        0  Norway   NOR      0\n44            Country    1  France     FRA        0  France   FRA      0\n111 Sovereign country    1  Sweden     SWE        0  Sweden   SWE      0\n112 Sovereign country    1 Belarus     BLR        0 Belarus   BLR      0\n113 Sovereign country    1 Ukraine     UKR        0 Ukraine   UKR      0\n114 Sovereign country    1  Poland     POL        0  Poland   POL      0\n115 Sovereign country    1 Austria     AUT        0 Austria   AUT      0\n116 Sovereign country    1 Hungary     HUN        0 Hungary   HUN      0\n117 Sovereign country    1 Moldova     MDA        0 Moldova   MDA      0\n    subunit su_a3 brk_diff    name          name_long brk_a3 brk_name brk_group\n19   Russia   RUS        0  Russia Russian Federation    RUS   Russia      &lt;NA&gt;\n22   Norway   NOR        0  Norway             Norway    NOR   Norway      &lt;NA&gt;\n44   France   FRA        0  France             France    FRA   France      &lt;NA&gt;\n111  Sweden   SWE        0  Sweden             Sweden    SWE   Sweden      &lt;NA&gt;\n112 Belarus   BLR        0 Belarus            Belarus    BLR  Belarus      &lt;NA&gt;\n113 Ukraine   UKR        0 Ukraine            Ukraine    UKR  Ukraine      &lt;NA&gt;\n114  Poland   POL        0  Poland             Poland    POL   Poland      &lt;NA&gt;\n115 Austria   AUT        0 Austria            Austria    AUT  Austria      &lt;NA&gt;\n116 Hungary   HUN        0 Hungary            Hungary    HUN  Hungary      &lt;NA&gt;\n117 Moldova   MDA        0 Moldova            Moldova    MDA  Moldova      &lt;NA&gt;\n    abbrev postal           formal_en formal_fr name_ciawf note_adm0 note_brk\n19    Rus.    RUS  Russian Federation      &lt;NA&gt;     Russia      &lt;NA&gt;     &lt;NA&gt;\n22    Nor.      N   Kingdom of Norway      &lt;NA&gt;     Norway      &lt;NA&gt;     &lt;NA&gt;\n44     Fr.      F     French Republic      &lt;NA&gt;     France      &lt;NA&gt;     &lt;NA&gt;\n111   Swe.      S   Kingdom of Sweden      &lt;NA&gt;     Sweden      &lt;NA&gt;     &lt;NA&gt;\n112  Bela.     BY Republic of Belarus      &lt;NA&gt;    Belarus      &lt;NA&gt;     &lt;NA&gt;\n113   Ukr.     UA             Ukraine      &lt;NA&gt;    Ukraine      &lt;NA&gt;     &lt;NA&gt;\n114   Pol.     PL  Republic of Poland      &lt;NA&gt;     Poland      &lt;NA&gt;     &lt;NA&gt;\n115  Aust.      A Republic of Austria      &lt;NA&gt;    Austria      &lt;NA&gt;     &lt;NA&gt;\n116   Hun.     HU Republic of Hungary      &lt;NA&gt;    Hungary      &lt;NA&gt;     &lt;NA&gt;\n117   Mda.     MD Republic of Moldova      &lt;NA&gt;    Moldova      &lt;NA&gt;     &lt;NA&gt;\n             name_sort name_alt mapcolor7 mapcolor8 mapcolor9 mapcolor13\n19  Russian Federation     &lt;NA&gt;         2         5         7          7\n22              Norway     &lt;NA&gt;         5         3         8         12\n44              France     &lt;NA&gt;         7         5         9         11\n111             Sweden     &lt;NA&gt;         1         4         2          4\n112            Belarus     &lt;NA&gt;         1         1         5         11\n113            Ukraine     &lt;NA&gt;         5         1         6          3\n114             Poland     &lt;NA&gt;         3         7         1          2\n115            Austria     &lt;NA&gt;         3         1         3          4\n116            Hungary     &lt;NA&gt;         4         6         1          5\n117            Moldova     &lt;NA&gt;         3         5         4         12\n      pop_est pop_rank pop_year  gdp_md gdp_year                    economy\n19  144373535       17     2019 1699876     2019   3. Emerging region: BRIC\n22    5347896       13     2019  403336     2019 2. Developed region: nonG7\n44   67059887       16     2019 2715518     2019    1. Developed region: G7\n111  10285453       14     2019  530883     2019 2. Developed region: nonG7\n112   9466856       13     2019   63080     2019       6. Developing region\n113  44385155       15     2019  153781     2019       6. Developing region\n114  37970874       15     2019  595858     2019 2. Developed region: nonG7\n115   8877067       13     2019  445075     2019 2. Developed region: nonG7\n116   9769949       13     2019  163469     2019 2. Developed region: nonG7\n117   2657637       12     2019   11968     2019       6. Developing region\n                income_grp fips_10 iso_a2 iso_a2_eh iso_a3 iso_a3_eh iso_n3\n19  3. Upper middle income      RS     RU        RU    RUS       RUS    643\n22    1. High income: OECD     -99    -99        NO    -99       NOR    -99\n44    1. High income: OECD      FR    -99        FR    -99       FRA    -99\n111   1. High income: OECD      SW     SE        SE    SWE       SWE    752\n112 3. Upper middle income      BO     BY        BY    BLR       BLR    112\n113 4. Lower middle income      UP     UA        UA    UKR       UKR    804\n114   1. High income: OECD      PL     PL        PL    POL       POL    616\n115   1. High income: OECD      AU     AT        AT    AUT       AUT    040\n116   1. High income: OECD      HU     HU        HU    HUN       HUN    348\n117 4. Lower middle income      MD     MD        MD    MDA       MDA    498\n    iso_n3_eh un_a3 wb_a2 wb_a3   woe_id woe_id_eh\n19        643   643    RU   RUS 23424936  23424936\n22        578   -99   -99   -99      -90  23424910\n44        250   250    FR   FRA      -90  23424819\n111       752   752    SE   SWE 23424954  23424954\n112       112   112    BY   BLR 23424765  23424765\n113       804   804    UA   UKR 23424976  23424976\n114       616   616    PL   POL 23424923  23424923\n115       040   040    AT   AUT 23424750  23424750\n116       348   348    HU   HUN 23424844  23424844\n117       498   498    MD   MDA 23424885  23424885\n                                                               woe_note\n19                                           Exact WOE match as country\n22  Does not include Svalbard, Jan Mayen, or Bouvet Islands (28289410).\n44                Includes only Metropolitan France (including Corsica)\n111                                          Exact WOE match as country\n112                                          Exact WOE match as country\n113                                          Exact WOE match as country\n114                                          Exact WOE match as country\n115                                          Exact WOE match as country\n116                                          Exact WOE match as country\n117                                          Exact WOE match as country\n    adm0_iso adm0_diff adm0_tlc adm0_a3_us adm0_a3_fr adm0_a3_ru adm0_a3_es\n19       RUS      &lt;NA&gt;      RUS        RUS        RUS        RUS        RUS\n22       NOR      &lt;NA&gt;      NOR        NOR        NOR        NOR        NOR\n44       FRA      &lt;NA&gt;      FRA        FRA        FRA        FRA        FRA\n111      SWE      &lt;NA&gt;      SWE        SWE        SWE        SWE        SWE\n112      BLR      &lt;NA&gt;      BLR        BLR        BLR        BLR        BLR\n113      UKR      &lt;NA&gt;      UKR        UKR        UKR        UKR        UKR\n114      POL      &lt;NA&gt;      POL        POL        POL        POL        POL\n115      AUT      &lt;NA&gt;      AUT        AUT        AUT        AUT        AUT\n116      HUN      &lt;NA&gt;      HUN        HUN        HUN        HUN        HUN\n117      MDA      &lt;NA&gt;      MDA        MDA        MDA        MDA        MDA\n    adm0_a3_cn adm0_a3_tw adm0_a3_in adm0_a3_np adm0_a3_pk adm0_a3_de\n19         RUS        RUS        RUS        RUS        RUS        RUS\n22         NOR        NOR        NOR        NOR        NOR        NOR\n44         FRA        FRA        FRA        FRA        FRA        FRA\n111        SWE        SWE        SWE        SWE        SWE        SWE\n112        BLR        BLR        BLR        BLR        BLR        BLR\n113        UKR        UKR        UKR        UKR        UKR        UKR\n114        POL        POL        POL        POL        POL        POL\n115        AUT        AUT        AUT        AUT        AUT        AUT\n116        HUN        HUN        HUN        HUN        HUN        HUN\n117        MDA        MDA        MDA        MDA        MDA        MDA\n    adm0_a3_gb adm0_a3_br adm0_a3_il adm0_a3_ps adm0_a3_sa adm0_a3_eg\n19         RUS        RUS        RUS        RUS        RUS        RUS\n22         NOR        NOR        NOR        NOR        NOR        NOR\n44         FRA        FRA        FRA        FRA        FRA        FRA\n111        SWE        SWE        SWE        SWE        SWE        SWE\n112        BLR        BLR        BLR        BLR        BLR        BLR\n113        UKR        UKR        UKR        UKR        UKR        UKR\n114        POL        POL        POL        POL        POL        POL\n115        AUT        AUT        AUT        AUT        AUT        AUT\n116        HUN        HUN        HUN        HUN        HUN        HUN\n117        MDA        MDA        MDA        MDA        MDA        MDA\n    adm0_a3_ma adm0_a3_pt adm0_a3_ar adm0_a3_jp adm0_a3_ko adm0_a3_vn\n19         RUS        RUS        RUS        RUS        RUS        RUS\n22         NOR        NOR        NOR        NOR        NOR        NOR\n44         FRA        FRA        FRA        FRA        FRA        FRA\n111        SWE        SWE        SWE        SWE        SWE        SWE\n112        BLR        BLR        BLR        BLR        BLR        BLR\n113        UKR        UKR        UKR        UKR        UKR        UKR\n114        POL        POL        POL        POL        POL        POL\n115        AUT        AUT        AUT        AUT        AUT        AUT\n116        HUN        HUN        HUN        HUN        HUN        HUN\n117        MDA        MDA        MDA        MDA        MDA        MDA\n    adm0_a3_tr adm0_a3_id adm0_a3_pl adm0_a3_gr adm0_a3_it adm0_a3_nl\n19         RUS        RUS        RUS        RUS        RUS        RUS\n22         NOR        NOR        NOR        NOR        NOR        NOR\n44         FRA        FRA        FRA        FRA        FRA        FRA\n111        SWE        SWE        SWE        SWE        SWE        SWE\n112        BLR        BLR        BLR        BLR        BLR        BLR\n113        UKR        UKR        UKR        UKR        UKR        UKR\n114        POL        POL        POL        POL        POL        POL\n115        AUT        AUT        AUT        AUT        AUT        AUT\n116        HUN        HUN        HUN        HUN        HUN        HUN\n117        MDA        MDA        MDA        MDA        MDA        MDA\n    adm0_a3_se adm0_a3_bd adm0_a3_ua adm0_a3_un adm0_a3_wb continent region_un\n19         RUS        RUS        RUS        -99        -99    Europe    Europe\n22         NOR        NOR        NOR        -99        -99    Europe    Europe\n44         FRA        FRA        FRA        -99        -99    Europe    Europe\n111        SWE        SWE        SWE        -99        -99    Europe    Europe\n112        BLR        BLR        BLR        -99        -99    Europe    Europe\n113        UKR        UKR        UKR        -99        -99    Europe    Europe\n114        POL        POL        POL        -99        -99    Europe    Europe\n115        AUT        AUT        AUT        -99        -99    Europe    Europe\n116        HUN        HUN        HUN        -99        -99    Europe    Europe\n117        MDA        MDA        MDA        -99        -99    Europe    Europe\n          subregion             region_wb name_len long_len abbrev_len tiny\n19   Eastern Europe Europe & Central Asia        6       18          4  -99\n22  Northern Europe Europe & Central Asia        6        6          4  -99\n44   Western Europe Europe & Central Asia        6        6          3  -99\n111 Northern Europe Europe & Central Asia        6        6          4  -99\n112  Eastern Europe Europe & Central Asia        7        7          5  -99\n113  Eastern Europe Europe & Central Asia        7        7          4  -99\n114  Eastern Europe Europe & Central Asia        6        6          4  -99\n115  Western Europe Europe & Central Asia        7        7          5  -99\n116  Eastern Europe Europe & Central Asia        7        7          4  -99\n117  Eastern Europe Europe & Central Asia        7        7          4  -99\n    homepart min_zoom min_label max_label   label_x  label_y      ne_id\n19         1        0       1.7       5.2 44.686469 58.24936 1159321201\n22         1        0       3.0       7.0  9.679975 61.35709 1159321109\n44         1        0       1.7       6.7  2.552275 46.69611 1159320637\n111        1        0       2.0       7.0 19.017050 65.85918 1159321287\n112        1        0       3.0       8.0 28.417701 53.82189 1159320427\n113        1        0       2.7       7.0 32.140865 49.72474 1159321345\n114        1        0       2.5       7.0 19.490468 51.99032 1159321179\n115        1        0       3.0       8.0 14.130515 47.51886 1159320379\n116        1        0       4.0       9.0 19.447867 47.08684 1159320841\n117        1        0       5.0      10.0 28.487904 47.43500 1159321045\n    wikidataid   name_ar name_bn         name_de name_en     name_es  name_fa\n19        Q159     روسيا  রাশিয়া        Russland  Russia       Rusia    روسیه\n22         Q20   النرويج   নরওয়ে        Norwegen  Norway     Noruega     نروژ\n44        Q142     فرنسا   ফ্রান্স      Frankreich  France     Francia   فرانسه\n111        Q34    السويد   সুইডেন        Schweden  Sweden      Suecia     سوئد\n112       Q184 بيلاروسيا  বেলারুশ         Belarus Belarus Bielorrusia   بلاروس\n113       Q212  أوكرانيا  ইউক্রেন         Ukraine Ukraine     Ucrania  اوکراین\n114        Q36    بولندا পোল্যান্ড           Polen  Poland     Polonia   لهستان\n115        Q40    النمسا অস্ট্রিয়া      Österreich Austria     Austria    اتریش\n116        Q28     المجر হাঙ্গেরি          Ungarn Hungary     Hungría مجارستان\n117       Q217   مولدوفا  মলদোভা Republik Moldau Moldova    Moldavia  مولداوی\n        name_fr    name_el  name_he name_hi          name_hu  name_id\n19       Russie      Ρωσία    רוסיה      रूस      Oroszország    Rusia\n22      Norvège   Νορβηγία נורווגיה    नॉर्वे         Norvégia Norwegia\n44       France     Γαλλία     צרפת   फ़्रान्स    Franciaország  Prancis\n111       Suède    Σουηδία   שוודיה   स्वीडन       Svédország   Swedia\n112 Biélorussie Λευκορωσία   בלארוס   बेलारूस Fehéroroszország  Belarus\n113     Ukraine   Ουκρανία אוקראינה    युक्रेन          Ukrajna  Ukraina\n114     Pologne    Πολωνία    פולין    पोलैंड    Lengyelország Polandia\n115    Autriche    Αυστρία  אוסטריה ऑस्ट्रिया         Ausztria  Austria\n116     Hongrie   Ουγγαρία  הונגריה    हंगरी     Magyarország Hongaria\n117    Moldavie   Μολδαβία  מולדובה मॉल्डोवा          Moldova  Moldova\n        name_it      name_ja    name_ko     name_nl  name_pl      name_pt\n19       Russia       ロシア     러시아     Rusland    Rosja       Rússia\n22     Norvegia   ノルウェー   노르웨이   Noorwegen Norwegia      Noruega\n44      Francia     フランス     프랑스   Frankrijk  Francja       França\n111      Svezia スウェーデン     스웨덴      Zweden  Szwecja       Suécia\n112 Bielorussia   ベラルーシ   벨라루스 Wit-Rusland Białoruś Bielorrússia\n113     Ucraina   ウクライナ 우크라이나    Oekraïne  Ukraina      Ucrânia\n114     Polonia   ポーランド     폴란드       Polen   Polska      Polónia\n115     Austria オーストリア 오스트리아  Oostenrijk  Austria      Áustria\n116    Ungheria   ハンガリー     헝가리   Hongarije    Węgry      Hungria\n117    Moldavia     モルドバ     몰도바    Moldavië Mołdawia     Moldávia\n       name_ru   name_sv     name_tr  name_uk name_ur   name_vi  name_zh\n19      Россия  Ryssland       Rusya    Росія     روس       Nga   俄罗斯\n22    Норвегия     Norge      Norveç Норвегія   ناروے     Na Uy     挪威\n44     Франция Frankrike      Fransa  Франція   فرانس      Pháp     法国\n111     Швеция   Sverige       İsveç   Швеція   سویڈن Thụy Điển     瑞典\n112 Белоруссия   Belarus Beyaz Rusya Білорусь بیلاروس   Belarus 白俄罗斯\n113    Украина   Ukraina     Ukrayna  Україна  یوکرین   Ukraina   乌克兰\n114     Польша     Polen     Polonya   Польща  پولینڈ    Ba Lan     波兰\n115    Австрия Österrike   Avusturya  Австрія  آسٹریا        Áo   奥地利\n116    Венгрия    Ungern  Macaristan Угорщина   ہنگری   Hungary   匈牙利\n117   Молдавия Moldavien     Moldova  Молдова مالدووا   Moldova 摩尔多瓦\n    name_zht      fclass_iso tlc_diff      fclass_tlc fclass_us fclass_fr\n19    俄羅斯 Admin-0 country     &lt;NA&gt; Admin-0 country      &lt;NA&gt;      &lt;NA&gt;\n22      挪威    Unrecognized     &lt;NA&gt;    Unrecognized      &lt;NA&gt;      &lt;NA&gt;\n44      法國 Admin-0 country     &lt;NA&gt; Admin-0 country      &lt;NA&gt;      &lt;NA&gt;\n111     瑞典 Admin-0 country     &lt;NA&gt; Admin-0 country      &lt;NA&gt;      &lt;NA&gt;\n112 白俄羅斯 Admin-0 country     &lt;NA&gt; Admin-0 country      &lt;NA&gt;      &lt;NA&gt;\n113   烏克蘭 Admin-0 country     &lt;NA&gt; Admin-0 country      &lt;NA&gt;      &lt;NA&gt;\n114     波蘭 Admin-0 country     &lt;NA&gt; Admin-0 country      &lt;NA&gt;      &lt;NA&gt;\n115   奧地利 Admin-0 country     &lt;NA&gt; Admin-0 country      &lt;NA&gt;      &lt;NA&gt;\n116   匈牙利 Admin-0 country     &lt;NA&gt; Admin-0 country      &lt;NA&gt;      &lt;NA&gt;\n117 摩爾多瓦 Admin-0 country     &lt;NA&gt; Admin-0 country      &lt;NA&gt;      &lt;NA&gt;\n    fclass_ru fclass_es fclass_cn fclass_tw fclass_in fclass_np fclass_pk\n19       &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n22       &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n44       &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n111      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n112      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n113      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n114      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n115      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n116      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n117      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n    fclass_de fclass_gb fclass_br fclass_il fclass_ps fclass_sa fclass_eg\n19       &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n22       &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n44       &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n111      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n112      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n113      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n114      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n115      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n116      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n117      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n    fclass_ma fclass_pt fclass_ar fclass_jp fclass_ko fclass_vn fclass_tr\n19       &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n22       &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n44       &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n111      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n112      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n113      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n114      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n115      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n116      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n117      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n    fclass_id fclass_pl fclass_gr fclass_it fclass_nl fclass_se fclass_bd\n19       &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n22       &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n44       &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n111      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n112      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n113      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n114      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n115      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n116      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n117      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n    fclass_ua                       geometry\n19       &lt;NA&gt; MULTIPOLYGON (((178.7253 71...\n22       &lt;NA&gt; MULTIPOLYGON (((15.14282 79...\n44       &lt;NA&gt; MULTIPOLYGON (((-51.6578 4....\n111      &lt;NA&gt; MULTIPOLYGON (((11.02737 58...\n112      &lt;NA&gt; MULTIPOLYGON (((28.17671 56...\n113      &lt;NA&gt; MULTIPOLYGON (((31.78599 52...\n114      &lt;NA&gt; MULTIPOLYGON (((23.48413 53...\n115      &lt;NA&gt; MULTIPOLYGON (((16.97967 48...\n116      &lt;NA&gt; MULTIPOLYGON (((22.08561 48...\n117      &lt;NA&gt; MULTIPOLYGON (((26.61934 48...\nggplot() +\n  geom_sf(data = sf_countries) +\n  coord_sf(xlim = c(-10, 70), ylim = c(30, 80))\nggplot() +\n  geom_sf(data = sf_countries) +\n  geom_sf(data = sf_metadata) +\n  coord_sf(xlim = c(-10, 70), ylim = c(30, 80))\nggplot() +\n  geom_sf(data = sf_countries) +\n  geom_sf(data = sf_metadata) +\n  coord_sf(xlim = c(-10, 70), ylim = c(30, 80))\nggplot() +\n  geom_sf(data = sf_countries) +\n  geom_sf(data = sf_metadata, aes(color = country)) +\n  coord_sf(xlim = c(-10, 70), ylim = c(30, 80))\nggplot() +\n  geom_sf(data = sf_countries) +\n  geom_sf(data = filter(sf_metadata, continent == \"Europe\"), aes(color = country)) +\n  coord_sf(xlim = c(-10, 70), ylim = c(30, 80))\nggplot() +\n  geom_sf(data = sf_countries) +\n  geom_sf(data = filter(sf_metadata, continent == \"Europe\"), aes(color = coverage)) +\n  coord_sf(xlim = c(-10, 70), ylim = c(30, 80))\n# ggplot() +\n#   geom_sf(data = sf_countries) +\n#   geom_sf(data = filter(sf_metadata, continent == \"Europe\"), aes(color = haplo_y)) +\n#   coord_sf(xlim = c(-10, 70), ylim = c(30, 80)) +\n#   facet_wrap(~ age_bin)",
    "crumbs": [
      "Spatial data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Working with _tidy_ spatial data</span>"
    ]
  },
  {
    "objectID": "spatial-sf.html#setup-and-data-processing",
    "href": "spatial-sf.html#setup-and-data-processing",
    "title": "Working with tidy spatial data",
    "section": "",
    "text": "All your tidyverse knowledge applies to spatial data just like it applied for “normal data frames”! We’ll practice a little bit about tidyverse in the context of spatial data later.\nNot only that, but ggplot2 automatically supports plotting of spatial data points using a dedicated “geom” function geom_sf()!\n\n\n\nggplot(DATA_FRAME, aes(MAPPING AESTHETICS, LIKE COLOR ETC.)) +\n  geom_...()",
    "crumbs": [
      "Spatial data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Working with _tidy_ spatial data</span>"
    ]
  },
  {
    "objectID": "popgen-models.html",
    "href": "popgen-models.html",
    "title": "Demographic models",
    "section": "",
    "text": "Part 1: Building a demographic model in slendr\nIn this exercise, you will practice building demographic models from scratch using the programmable interface provided by the slendr R package. In this context, you can understand “demographic model” as “a tree-like topological structure encoding the relationships between populations and gene flows between them”. For the time being, these models will be always neutral and will conform to Wright-Fisher assumptions.\nUse functions such as population(), gene_flow(), and compile_model(), which we discussed in “Introduction to slendr”, to program the following toy model of human demographic history in slendr. (Apologies for my bad handwriting and the lack of any artistic skill.)\nHint: Create a new script models.R in your RStudio session using the following “template”. Then add a sequence of appropriate population() calls using the syntax from the introductory slides (using the parent = &lt;pop&gt; argument for programming splits of daughter populations – which will be all except the CHIMP lineage in our example), etc.\nlibrary(slendr)\ninit_env()\n\n# &lt;replace this with `population()` definitions like in the slides&gt;\n# &lt;replace this with your gene-flow definition in variable `gf`&gt;\n\nmodel &lt;- compile_model(\n  populations = list(...), # &lt;put your list of populations here&gt;\n  gene_flow = gf,\n  generation_time = 30\n)\nHint: Remember that slendr is designed with interactivity in mind! When you write a chunk of code (such as a command to create a population through a population split, or model compilation to create a model object), execute that bit of code in the R console and inspect the summary information printed by evaluating the respective R object you just created. You can either copy-pasted stuff from your script to the R console, or use a convenient RStudio shortcut like Ctrl+Enter (Linux and Windows), or Cmd+Enter (Mac).",
    "crumbs": [
      "Modeling fundamentals",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Demographic models</span>"
    ]
  },
  {
    "objectID": "popgen-models.html#part-1-building-a-demographic-model-in-slendr",
    "href": "popgen-models.html#part-1-building-a-demographic-model-in-slendr",
    "title": "Demographic models",
    "section": "",
    "text": "Note: You could easily program the model so that different ancestral populations are represented by separate population() commands (i.e., your model would start with a population called “human_chimp_ancestor” from which a “CHIMP” and “hominin_ancestor” populations would split at 6 Mya, etc.) but generally this is too annoying to do and requires too much code.\nFeel free to write the model so that “CHIMP” is the first population, then “AFR” population splits from it at 6 Mya, etc… Although it probably isn’t the most accurate way to describe the real evolutionary history, it simplifies the coding significantly.\n [Mya = million years ago; kya = thousand years ago]\n\n\n\n\nNote: With slendr you can specify time in whatever format is more convenient or readable for your model. For instance here, because we’re dealing with historical events which are commonly expressed in times given as”years ago”, we can write them in a decreasing order – i.e. 7Mya → 6Mya → …, as shown above – or, in terms of R code, 7e6 (or 7000000), 6e6 (6000000), etc.\nIn a later example, you will see that you can also encode the events in the time direction going “forward” (i.e., the first event starting in generation 1, a following event in generation 42, and so on).\n\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nlibrary(slendr)\ninit_env()\n## The interface to all required Python modules has been activated.\n\ndir.create(\"files/popgen/introgression\", recursive = TRUE, showWarnings = FALSE)\n\n# Chimpanzee outgroup\nchimp &lt;- population(\"CHIMP\", time = 7e6, N = 5000)\n\n# Two populations of anatomically modern humans: Africans and Europeans\nafr &lt;- population(\"AFR\", parent = chimp, time = 6e6, N = 15000)\neur &lt;- population(\"EUR\", parent = afr, time = 60e3, N = 3000)\n\n# Neanderthal population splitting at 600 ky ago from modern humans\n# (becomes extinct by 40 ky ago)\nnea &lt;- population(\"NEA\", parent = afr, time = 600e3, N = 1000, remove = 40e3)\n\n# Neanderthal introgression event (3% admixture between 55-50 kya)\ngf &lt;- gene_flow(from = nea, to = eur, rate = 0.03, start = 55000, end = 50000)\n\n# Compile the entire model into a single slendr R object\nmodel &lt;- compile_model(\n  populations = list(chimp, nea, afr, eur),\n  gene_flow = gf,\n  generation_time = 30,\n  path = \"files/popgen/introgression\",     # &lt;--- don't worry about these two\n  overwrite = TRUE, force = TRUE   # &lt;--- lines of code (ask me if interested)\n)",
    "crumbs": [
      "Modeling fundamentals",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Demographic models</span>"
    ]
  },
  {
    "objectID": "popgen-models.html#part-2-inspecting-the-model-visually",
    "href": "popgen-models.html#part-2-inspecting-the-model-visually",
    "title": "Demographic models",
    "section": "Part 2: Inspecting the model visually",
    "text": "Part 2: Inspecting the model visually\nTo visualize a slendr model, you use the function plot_model(). Plot your compiled model to make sure you programmed it correctly! Your figure should roughly correspond to my doodle above.\n\n\nNote: Plotting of models in slendr can be sometimes a little wonky, especially if many things are happening at once. When plotting your model, experiment with arguments log = TRUE, proportions = TRUE, gene_flow = TRUE. Check ?plot_model for more information on these.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nplot_model(model)\n\n\n\n\n\n\n\nplot_model(model, sizes = FALSE)\n\n\n\n\n\n\n\nplot_model(model, sizes = FALSE, log = TRUE)\n\n\n\n\n\n\n\nplot_model(model, log = TRUE, proportions = TRUE)",
    "crumbs": [
      "Modeling fundamentals",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Demographic models</span>"
    ]
  },
  {
    "objectID": "popgen-models.html#part-3-simulating-genomic-data",
    "href": "popgen-models.html#part-3-simulating-genomic-data",
    "title": "Demographic models",
    "section": "Part 3: Simulating genomic data",
    "text": "Part 3: Simulating genomic data\nOnce you have a compiled slendr model stored in an R variable (from now on, model will always mean a variable containing a compiled slendr model object relevant for the given exercise, for simplicity), we can simulate data from it. By default, slendr models always produce a tree sequence.\n\n\nNote: Tree sequence provides an extremely efficient means to store and work with genomic data at a massive scale. However, you can always get simulated data even in traditional file formats, such as VCF, EIGENSTRAT, or a plain old table of ancestral/derived genotypes.\nIn this activity we will be only working with tree sequences, because it’s much easier and faster to get interesting statistics from it directly in R.\nThere are two simulation engines built into slendr implemented by functions msprime() and slim(). For traditional, non-spatial, neutral demographic models, the engine provided by the msprime() function is much more efficient, so we’ll be using that for the time being. However, from a popgen theoretical perspective, both simulation functions will give you the same results for any given compiled slendr model (up to some level of stochastic noise, of course).\n\n\nNote: Yes, this means you don’t have to write any msprime (or SLiM) code to simulate data from a slendr model!\nHere’s how you can use the function to simulate a tree sequence from the model you’ve just created using compile_model() in your script:\n\nts &lt;- msprime(\n  model,\n  sequence_length = &lt;length of sequence to simulate [as bp]&gt;,\n  recombination_rate = &lt;uniform recombination rate [per bp per generation]&gt;\n)\n\nYou will be seeing this kind of pattern over and over again in this exercise, so it’s a good idea to keep it in mind.\nHint: The msprime() function has also arguments debug and run which can be extremely useful for debugging.\nSimulate a tree sequence from your compiled model using the msprime() engine, storing it to a variable ts as shown right above. Use sequence_length = 1e6 (so 1 Mb of sequence) and recombination_rate = 1e-8 (crossover events per base pair per generation). Then experiment with setting debug = TRUE (this prints out msprime’s own debugging summary which you might already be familiar with from your previous activity?) and then run = FALSE (this prints out a raw command-line which can run a slendr simulation in the shell).\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\n# This simulates a tskit tree sequence from a slendr model. Note that you didn't have\n# to write any msprime or tskit Python code!\nts &lt;- msprime(model, sequence_length = 1e6, recombination_rate = 1e-8)\n\n# Setting `debug = TRUE` instructs slendr's built-in msprime script to print\n# out msprime's own debugger information. This can be very useful for debugging,\n# in addition to the visualization of the model as shown above.\nts &lt;- msprime(model, sequence_length = 1e6, recombination_rate = 1e-8, debug = TRUE)\n## DemographyDebugger\n## ╠═════════════════════════════════════╗\n## ║ Epoch[0]: [0, 1.67e+03) generations ║\n## ╠═════════════════════════════════════╝\n## ╟    Populations (total=4 active=4)\n## ║    ┌─────────────────────────────────────────────────────────────────────┐\n## ║    │       │     start│       end│growth_rate  │ CHIMP │ AFR │ NEA │ EUR │\n## ║    ├─────────────────────────────────────────────────────────────────────┤\n## ║    │  CHIMP│    5000.0│    5000.0│ 0           │   0   │  0  │  0  │  0  │\n## ║    │    AFR│   15000.0│   15000.0│ 0           │   0   │  0  │  0  │  0  │\n## ║    │    NEA│    1000.0│    1000.0│ 0           │   0   │  0  │  0  │  0  │\n## ║    │    EUR│    3000.0│    3000.0│ 0           │   0   │  0  │  0  │  0  │\n## ║    └─────────────────────────────────────────────────────────────────────┘\n## ╟    Events @ generation 1.67e+03\n## ║    ┌─────────────────────────────────────────────────────────────────────────────────────────┐\n## ║    │  time│type            │parameters              │effect                                  │\n## ║    ├─────────────────────────────────────────────────────────────────────────────────────────┤\n## ║    │  1666│Migration rate  │source=EUR, dest=NEA,   │Backwards-time migration rate from EUR  │\n## ║    │      │change          │rate=0.000179640718562  │to NEA → 0.00017964071856287425         │\n## ║    │      │                │87425                   │                                        │\n## ║    └─────────────────────────────────────────────────────────────────────────────────────────┘\n## ╠════════════════════════════════════════════╗\n## ║ Epoch[1]: [1.67e+03, 1.83e+03) generations ║\n## ╠════════════════════════════════════════════╝\n## ╟    Populations (total=4 active=4)\n## ║    ┌───────────────────────────────────────────────────────────────────────────┐\n## ║    │       │     start│       end│growth_rate  │ CHIMP │ AFR │    NEA    │ EUR │\n## ║    ├───────────────────────────────────────────────────────────────────────────┤\n## ║    │  CHIMP│    5000.0│    5000.0│ 0           │   0   │  0  │     0     │  0  │\n## ║    │    AFR│   15000.0│   15000.0│ 0           │   0   │  0  │     0     │  0  │\n## ║    │    NEA│    1000.0│    1000.0│ 0           │   0   │  0  │     0     │  0  │\n## ║    │    EUR│    3000.0│    3000.0│ 0           │   0   │  0  │ 0.0001796 │  0  │\n## ║    └───────────────────────────────────────────────────────────────────────────┘\n## ╟    Events @ generation 1.83e+03\n## ║    ┌────────────────────────────────────────────────────────────────────────────────────────┐\n## ║    │  time│type            │parameters             │effect                                  │\n## ║    ├────────────────────────────────────────────────────────────────────────────────────────┤\n## ║    │  1833│Migration rate  │source=EUR, dest=NEA,  │Backwards-time migration rate from EUR  │\n## ║    │      │change          │rate=0                 │to NEA → 0                              │\n## ║    │┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈│\n## ║    │  1833│Census          │                       │Insert census nodes to record the       │\n## ║    │      │                │                       │location of all lineages                │\n## ║    └────────────────────────────────────────────────────────────────────────────────────────┘\n## ╠═════════════════════════════════════════╗\n## ║ Epoch[2]: [1.83e+03, 2e+03) generations ║\n## ╠═════════════════════════════════════════╝\n## ╟    Populations (total=4 active=4)\n## ║    ┌─────────────────────────────────────────────────────────────────────┐\n## ║    │       │     start│       end│growth_rate  │ CHIMP │ AFR │ NEA │ EUR │\n## ║    ├─────────────────────────────────────────────────────────────────────┤\n## ║    │  CHIMP│    5000.0│    5000.0│ 0           │   0   │  0  │  0  │  0  │\n## ║    │    AFR│   15000.0│   15000.0│ 0           │   0   │  0  │  0  │  0  │\n## ║    │    NEA│    1000.0│    1000.0│ 0           │   0   │  0  │  0  │  0  │\n## ║    │    EUR│    3000.0│    3000.0│ 0           │   0   │  0  │  0  │  0  │\n## ║    └─────────────────────────────────────────────────────────────────────┘\n## ╟    Events @ generation 2e+03\n## ║    ┌───────────────────────────────────────────────────────────────────────────┐\n## ║    │  time│type        │parameters      │effect                                │\n## ║    ├───────────────────────────────────────────────────────────────────────────┤\n## ║    │  2000│Population  │derived=[EUR],  │Moves all lineages from the 'EUR'     │\n## ║    │      │Split       │ancestral=AFR   │derived population to the ancestral   │\n## ║    │      │            │                │'AFR' population. Also set 'EUR' to   │\n## ║    │      │            │                │inactive, and all migration rates to  │\n## ║    │      │            │                │and from the derived population to    │\n## ║    │      │            │                │zero.                                 │\n## ║    └───────────────────────────────────────────────────────────────────────────┘\n## ╠══════════════════════════════════════╗\n## ║ Epoch[3]: [2e+03, 2e+04) generations ║\n## ╠══════════════════════════════════════╝\n## ╟    Populations (total=4 active=3)\n## ║    ┌───────────────────────────────────────────────────────────────┐\n## ║    │       │     start│       end│growth_rate  │ CHIMP │ AFR │ NEA │\n## ║    ├───────────────────────────────────────────────────────────────┤\n## ║    │  CHIMP│    5000.0│    5000.0│ 0           │   0   │  0  │  0  │\n## ║    │    AFR│   15000.0│   15000.0│ 0           │   0   │  0  │  0  │\n## ║    │    NEA│    1000.0│    1000.0│ 0           │   0   │  0  │  0  │\n## ║    └───────────────────────────────────────────────────────────────┘\n## ╟    Events @ generation 2e+04\n## ║    ┌────────────────────────────────────────────────────────────────────────────┐\n## ║    │   time│type        │parameters      │effect                                │\n## ║    ├────────────────────────────────────────────────────────────────────────────┤\n## ║    │  2e+04│Population  │derived=[NEA],  │Moves all lineages from the 'NEA'     │\n## ║    │       │Split       │ancestral=AFR   │derived population to the ancestral   │\n## ║    │       │            │                │'AFR' population. Also set 'NEA' to   │\n## ║    │       │            │                │inactive, and all migration rates to  │\n## ║    │       │            │                │and from the derived population to    │\n## ║    │       │            │                │zero.                                 │\n## ║    └────────────────────────────────────────────────────────────────────────────┘\n## ╠══════════════════════════════════════╗\n## ║ Epoch[4]: [2e+04, 2e+05) generations ║\n## ╠══════════════════════════════════════╝\n## ╟    Populations (total=4 active=2)\n## ║    ┌─────────────────────────────────────────────────────────┐\n## ║    │       │     start│       end│growth_rate  │ CHIMP │ AFR │\n## ║    ├─────────────────────────────────────────────────────────┤\n## ║    │  CHIMP│    5000.0│    5000.0│ 0           │   0   │  0  │\n## ║    │    AFR│   15000.0│   15000.0│ 0           │   0   │  0  │\n## ║    └─────────────────────────────────────────────────────────┘\n## ╟    Events @ generation 2e+05\n## ║    ┌──────────────────────────────────────────────────────────────────────────────┐\n## ║    │   time│type        │parameters       │effect                                 │\n## ║    ├──────────────────────────────────────────────────────────────────────────────┤\n## ║    │  2e+05│Population  │derived=[AFR],   │Moves all lineages from the 'AFR'      │\n## ║    │       │Split       │ancestral=CHIMP  │derived population to the ancestral    │\n## ║    │       │            │                 │'CHIMP' population. Also set 'AFR' to  │\n## ║    │       │            │                 │inactive, and all migration rates to   │\n## ║    │       │            │                 │and from the derived population to     │\n## ║    │       │            │                 │zero.                                  │\n## ║    └──────────────────────────────────────────────────────────────────────────────┘\n## ╠════════════════════════════════════╗\n## ║ Epoch[5]: [2e+05, inf) generations ║\n## ╠════════════════════════════════════╝\n## ╟    Populations (total=4 active=1)\n## ║    ┌─────────────────────────────────────────┐\n## ║    │       │    start│      end│growth_rate  │\n## ║    ├─────────────────────────────────────────┤\n## ║    │  CHIMP│   5000.0│   5000.0│ 0           │\n## ║    └─────────────────────────────────────────┘\n\n# For debugging of technical issues (with msprime, with slendr, or both), it is\n# very useful to have the `msprime` function dump the \"raw\" command-line to\n# run the simulation on the terminal using plain Python interpreter\nmsprime(model, sequence_length = 1e6, recombination_rate = 1e-8, run = FALSE)\n## /Users/mp/Library/r-miniconda-arm64/envs/Python-3.12_msprime-1.3.4_tskit-0.6.4_pyslim-1.0.4_tspop-0.0.2/bin/python /Users/mp/Code/simgen/files/popgen/introgression/script.py --seed 1930481052 --model /Users/mp/Code/simgen/files/popgen/introgression --sequence-length 1000000 --recombination-rate 1e-08    --coalescent_only --path path_to_a_trees_file.trees",
    "crumbs": [
      "Modeling fundamentals",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Demographic models</span>"
    ]
  },
  {
    "objectID": "popgen-models.html#part-4-inspecting-the-tree-sequence-object",
    "href": "popgen-models.html#part-4-inspecting-the-tree-sequence-object",
    "title": "Demographic models",
    "section": "Part 4: Inspecting the tree-sequence object",
    "text": "Part 4: Inspecting the tree-sequence object\nAs we will see later, slendr provides an R-friendly interface to accessing a subset of tskit’s functionality for working with tree sequences and for computing various popgen statistics.\nFor now, type out the ts object in the terminal – what do you see? You should get a summary of a tree-sequence object that you’re familiar with from your msprime and tskit activity earlier in the day.\n\n\nNote: This is a very important feature of slendr – when a simulation is concluded (doesn’t matter if it was a slim() or msprime() simulation), you will get a normal tskit object. In fact, the fact that slendr supports (so far, and likely always) only a “subset” of all of tskit’s functionality isn’t stopping you to write custom Python/tskit processing code of a tree sequence generated from a slendr model. Under the hood, a slendr simulation really is just an msprime (or SLiM) simulation! It’s just executed through a simplified interface.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\n# Typing out the object with the result shows that it's a good old tskit\n# tree-sequence object\nts\n\n╔═══════════════════════════╗\n║TreeSequence               ║\n╠═══════════════╤═══════════╣\n║Trees          │      9,710║\n╟───────────────┼───────────╢\n║Sequence Length│  1,000,000║\n╟───────────────┼───────────╢\n║Time Units     │generations║\n╟───────────────┼───────────╢\n║Sample Nodes   │     48,000║\n╟───────────────┼───────────╢\n║Total Size     │    8.8 MiB║\n╚═══════════════╧═══════════╝\n╔═══════════╤═══════╤═════════╤════════════╗\n║Table      │Rows   │Size     │Has Metadata║\n╠═══════════╪═══════╪═════════╪════════════╣\n║Edges      │134,646│  4.1 MiB│          No║\n╟───────────┼───────┼─────────┼────────────╢\n║Individuals│ 24,000│656.3 KiB│          No║\n╟───────────┼───────┼─────────┼────────────╢\n║Migrations │      0│  8 Bytes│          No║\n╟───────────┼───────┼─────────┼────────────╢\n║Mutations  │      0│ 16 Bytes│          No║\n╟───────────┼───────┼─────────┼────────────╢\n║Nodes      │104,956│  2.8 MiB│          No║\n╟───────────┼───────┼─────────┼────────────╢\n║Populations│      4│341 Bytes│         Yes║\n╟───────────┼───────┼─────────┼────────────╢\n║Provenances│      1│  2.7 KiB│          No║\n╟───────────┼───────┼─────────┼────────────╢\n║Sites      │      0│ 16 Bytes│          No║\n╚═══════════╧═══════╧═════════╧════════════╝\n\n\n\n\n\nThe brilliance of the tree-sequence data structure rests on its elegant table-based implementation (much more information on that is here). slendr isn’t really designed to run very complex low-level manipulations of tree-sequence data (its strength lies in the convenient interface to popgen statistical functions implemented by tskit), but it does contain a couple of functions which can be useful for inspecting the lower-level nature of a tree sequence. Let’s look at a couple of them now.\nUse the ts_table function to inspect the low-level table-based representation of a tree sequence. For instance, you can get the table of nodes with ts_table(ts, \"nodes\"), edges with ts_table(ts, \"edges\"), and do the same thing for “individuals”, “mutations”, and “sites”. Does your tree sequence contain any mutations? If not, why, and how can we even do any popgen with data without any mutations? As you’re doing this, take a look at at the following figure (this was made from a different tree sequence than you have, but that’s OK) to help you relate the information in the tables to a tree sequence which those tables (particularly tables of nodes and edges) implicitly encode.\nThis should convince you that the final product of a slendr simulation really is the same kind of tree-sequence object that you learned about in the previous activities today. You don’t have to study these tables in detail!\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\n# slendr provides a helper function which allows access to all the low-level\n# components of every tree-sequence object\nts_table(ts, \"nodes\")\n\n# A tibble: 104,956 × 5\n   node_id    ind_id pop_id  time time_tskit\n     &lt;int&gt; &lt;int[1d]&gt;  &lt;int&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1       0         0      0     0          0\n 2       1         0      0     0          0\n 3       2         1      0     0          0\n 4       3         1      0     0          0\n 5       4         2      0     0          0\n 6       5         2      0     0          0\n 7       6         3      0     0          0\n 8       7         3      0     0          0\n 9       8         4      0     0          0\n10       9         4      0     0          0\n# ℹ 104,946 more rows\n\nts_table(ts, \"edges\")\n\n# A tibble: 134,646 × 5\n      id child parent  left   right\n   &lt;dbl&gt; &lt;int&gt;  &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1     0 11551  48000     0 1000000\n 2     1 33164  48000     0 1000000\n 3     2 43484  48001     0 1000000\n 4     3 46964  48001     0 1000000\n 5     4  5209  48002     0 1000000\n 6     5  6022  48002     0 1000000\n 7     6 46240  48003     0 1000000\n 8     7 47616  48003     0 1000000\n 9     8 19966  48004     0 1000000\n10     9 23801  48004     0 1000000\n# ℹ 134,636 more rows\n\nts_table(ts, \"individuals\")\n\n# A tibble: 24,000 × 5\n   ind_id  time    pop_id sampled   time_tskit\n    &lt;dbl&gt; &lt;dbl&gt; &lt;int[1d]&gt; &lt;lgl[1d]&gt;  &lt;dbl[1d]&gt;\n 1      0     0         0 TRUE               0\n 2      1     0         0 TRUE               0\n 3      2     0         0 TRUE               0\n 4      3     0         0 TRUE               0\n 5      4     0         0 TRUE               0\n 6      5     0         0 TRUE               0\n 7      6     0         0 TRUE               0\n 8      7     0         0 TRUE               0\n 9      8     0         0 TRUE               0\n10      9     0         0 TRUE               0\n# ℹ 23,990 more rows\n\n# We didn't simulate any mutations, so we only have genealogies for now.\nts_table(ts, \"mutations\")\n\n# A tibble: 0 × 5\n# ℹ 5 variables: id &lt;dbl&gt;, site &lt;int&gt;, node &lt;int&gt;, time &lt;dbl&gt;, time_tskit &lt;dbl&gt;\n\nts_table(ts, \"sites\")\n\n# A tibble: 0 × 2\n# ℹ 2 variables: id &lt;dbl&gt;, position &lt;dbl&gt;\n\n\n\n\n\nThere are also two slendr-specific functions called ts_samples() (which retrieves the “symbolic names” and dates of all recorded individuals at the end of a simulation) and ts_nodes(). You can run them simply as ts_samples(ts) and ts_nodes(ts). How many individuals (samples) are in your tree sequence as you simulated it? How is the result of ts_nodes() different from ts_samples()?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\n# slendr provides a convenient function `ts_samples()` which allows us to\n# inspect the contents of a simulated tree sequence in a more human-readable,\n# simplified way. We can see that our tree sequence contains a massive number\n# of individuals. Too many, in fact -- we recorded every single individual alive\n# at the end of our simulation, which is something we're unlikely to be ever lucky\n# enough to have, regardless of which species we study.\nts_samples(ts)\n\n# A tibble: 24,000 × 3\n   name      time pop  \n   &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;\n 1 CHIMP_1      0 AFR  \n 2 CHIMP_2      0 AFR  \n 3 CHIMP_3      0 AFR  \n 4 CHIMP_4      0 AFR  \n 5 CHIMP_5      0 AFR  \n 6 CHIMP_6      0 AFR  \n 7 CHIMP_7      0 AFR  \n 8 CHIMP_8      0 AFR  \n 9 CHIMP_9      0 AFR  \n10 CHIMP_10     0 AFR  \n# ℹ 23,990 more rows\n\nts_samples(ts) %&gt;% nrow()\n\n[1] 24000\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nts_samples(ts) %&gt;% group_by(pop) %&gt;% tally\n\n# A tibble: 4 × 2\n  pop       n\n  &lt;chr&gt; &lt;int&gt;\n1 AFR   15000\n2 CHIMP  5000\n3 EUR    3000\n4 NEA    1000\n\n# This function returns a table similar to the one produced by `ts_table(ts, \"nodes\")`\n# above, except that it contains additional slendr metadata (names of individuals\n# belonging to each node, spatial coordinates of nodes for spatial models, etc.).\n# It's a bit more useful for analyzing tree-sequence data than the \"low-level\" functions.\nts_nodes(ts) %&gt;% head(5)\n\n# A tibble: 5 × 8\n  name    pop   ind_id node_id  time time_tskit sampled pop_id\n  &lt;chr&gt;   &lt;fct&gt;  &lt;dbl&gt;   &lt;int&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;lgl&gt;    &lt;int&gt;\n1 CHIMP_1 AFR     5000   10000     0          0 TRUE         1\n2 CHIMP_1 AFR     5000   10001     0          0 TRUE         1\n3 CHIMP_2 AFR     5001   10002     0          0 TRUE         1\n4 CHIMP_2 AFR     5001   10003     0          0 TRUE         1\n5 CHIMP_3 AFR     5002   10004     0          0 TRUE         1\n\nts_nodes(ts) %&gt;% tail(5)\n\n# A tibble: 5 × 8\n  name  pop   ind_id node_id     time time_tskit sampled pop_id\n  &lt;chr&gt; &lt;fct&gt;  &lt;dbl&gt;   &lt;int&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;lgl&gt;    &lt;int&gt;\n1 &lt;NA&gt;  CHIMP     NA  104951 7338342.    244611. FALSE        0\n2 &lt;NA&gt;  CHIMP     NA  104952 7347263.    244909. FALSE        0\n3 &lt;NA&gt;  CHIMP     NA  104953 7673910.    255797. FALSE        0\n4 &lt;NA&gt;  CHIMP     NA  104954 8056706.    268557. FALSE        0\n5 &lt;NA&gt;  CHIMP     NA  104955 8087510.    269584. FALSE        0",
    "crumbs": [
      "Modeling fundamentals",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Demographic models</span>"
    ]
  },
  {
    "objectID": "popgen-models.html#part-6-more-complex-sampling-events",
    "href": "popgen-models.html#part-6-more-complex-sampling-events",
    "title": "Demographic models",
    "section": "Part 6: More complex sampling events",
    "text": "Part 6: More complex sampling events\nIn the table produced by the ts_samples() function you saw that the tree sequence we simulated recorded everyone. It’s very unlikely, unless we’re extremely lucky, that we’ll ever have a sequence of every single individual in a population that we study. To get a little closer to the scale of the genomic data that we usually work with on a day-to-day basis, we can restrict our simulation to only record a subset of individuals.\nWe can precisely define which individuals (from which populations, and at which times) should be recorded in a tree sequence using the slendr function schedule_sampling(). For instance, if we have a model with some slendr populations in variables eur and afr, we can schedule the recording of 5 individuals from each at times 10000 (years ago) and 0 (present-day) (using the “years before present” direction of time in our current model of Neanderthal introgression) with the following code:\n\npop_schedule &lt;- schedule_sampling(model, times = c(10000, 0), list(eur, 5), list(afr, 5))\n\nThis function simply returns a data frame. As such, we can create multiple of such schedules (of arbitrary complexity and granularity), and then bind them together into a single sampling schedule with a single line of code, like this:\n\n# Note that the `times =` argument of the `schedule_sampling()` function can be\n# a vector of times like here...\nancient_times &lt;- c(40000, 30000, 20000, 10000)\neur_samples &lt;- schedule_sampling(model, times = ancient_times, list(eur, 1))\n\n# ... but also just a single number like here\nafr_samples &lt;- schedule_sampling(model, times = 0, list(afr, 1))\nnea_samples &lt;- schedule_sampling(model, time = 60000, list(nea, 1))\n\n# But whatever the means you create the individual sampling schedules with,\n# you can always bind them all to a single table with the `rbind()` function\nschedule &lt;- rbind(eur_samples, afr_samples, nea_samples)\nschedule\n\nUsing the function schedule_sampling (and with the help of rbind as shown in the previous code chunk), program the sampling of the following sample sets at given times, saving it to variable called schedule:\n\n\n\ntime\npopulation\n# individuals\n\n\n\n\n70000\nnea\n1\n\n\n40000\nnea\n1\n\n\n0\nchimp\n1\n\n\n0\nafr\n5\n\n\n0\neur\n10\n\n\n\nAdditionally, schedule the sampling of a single eur individual at the following times:\n\nt &lt;- seq(40000, 2000, by = -2000)\n\n\n\nNote: You can provide a vector variable (such as t in this example) as the times = argument of schedule_sampling().\nIn total, you should schedule the recording of 38 individuals.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\n# Here we scheduled the sampling of two Neanderthals at 70kya and 40kya\nnea_samples &lt;- schedule_sampling(model, times = c(70000, 40000), list(nea, 1))\nnea_samples # (this function produces a plain old data frame!)\n\n# A tibble: 2 × 8\n   time pop       n name  y_orig x_orig y     x    \n  &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;lgl&gt; &lt;lgl&gt;  &lt;lgl&gt;  &lt;lgl&gt; &lt;lgl&gt;\n1 40000 NEA       1 NA    NA     NA     NA    NA   \n2 70000 NEA       1 NA    NA     NA     NA    NA   \n\n# Here we schedule one Chimpanzee sample, 5 African samples, and 10 European samples\npresent_samples &lt;- schedule_sampling(model, times = 0, list(chimp, 1), list(afr, 5), list(eur, 10))\n\n# We also schedule the recording of one European sample between 50kya and 2kya,\n# every 2000 years\ntimes &lt;- seq(40000, 2000, by = -2000)\nemh_samples &lt;- schedule_sampling(model, times, list(eur, 1))\n\n# Because those functions produce nothing but a data frame, we can bind\n# individual sampling schedules together\nschedule &lt;- rbind(nea_samples, present_samples, emh_samples)\nschedule\n\n# A tibble: 25 × 8\n    time pop       n name  y_orig x_orig y     x    \n   &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;lgl&gt; &lt;lgl&gt;  &lt;lgl&gt;  &lt;lgl&gt; &lt;lgl&gt;\n 1 40000 NEA       1 NA    NA     NA     NA    NA   \n 2 70000 NEA       1 NA    NA     NA     NA    NA   \n 3     0 CHIMP     1 NA    NA     NA     NA    NA   \n 4     0 AFR       5 NA    NA     NA     NA    NA   \n 5     0 EUR      10 NA    NA     NA     NA    NA   \n 6  2000 EUR       1 NA    NA     NA     NA    NA   \n 7  4000 EUR       1 NA    NA     NA     NA    NA   \n 8  6000 EUR       1 NA    NA     NA     NA    NA   \n 9  8000 EUR       1 NA    NA     NA     NA    NA   \n10 10000 EUR       1 NA    NA     NA     NA    NA   \n# ℹ 15 more rows\n\n\n\n\n\nThen, verify the correctness of your overall sampling schedule by visualizing it together with your model like this:\n\n\nNote: As you’ve seen above, the visualization is often a bit wonky and convoluted with overlapping elements and it can be even worse with samples added, but try to experiment with arguments to plot_model described above to make the plot a bit more helpful for sanity checking.\n\nplot_model(model, samples = schedule)\n\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nplot_model(model, sizes = FALSE, samples = schedule)\n\n\n\n\n\n\n\n\n\nplot_model(model, sizes = FALSE, log = TRUE, samples = schedule)",
    "crumbs": [
      "Modeling fundamentals",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Demographic models</span>"
    ]
  },
  {
    "objectID": "popgen-models.html#part-7-simulating-a-defined-set-of-individuals",
    "href": "popgen-models.html#part-7-simulating-a-defined-set-of-individuals",
    "title": "Demographic models",
    "section": "Part 7: Simulating a defined set of individuals",
    "text": "Part 7: Simulating a defined set of individuals\nYou have now both a compiled slendr model and a well-defined sampling schedule.\nUse your combined sampling schedule stored in the schedule variable to run a new tree-sequence simulation from your model (again using the msprime() function), this time restricted to just those individuals scheduled for recording. You can do this by providing the combined sampling schedule as the samples = schedule argument of the function msprime you used above. Just replace the line(s) with your first msprime() from the previous part of this exercise with the new one, which uses the schedule for customized sampling.\nAlso, while you’re doing this, use the ts_mutate() function to overlay neutral mutations on the simulated tree sequence right after the msprime() call. (Take a look at the handounts for a reminder of the %&gt;% pipe patterns I showed you.)\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nThe command below will likely take a few minutes to run, so feel free to go down from 100 Mb sequence_length to even 10Mb (it doesn’t matter much). (The random_seed = argument is there for reproducibility purposes.)\n\nts &lt;-\n  msprime(model, sequence_length = 100e6, recombination_rate = 1e-8, samples = schedule, random_seed = 1269258439) %&gt;%\n  ts_mutate(mutation_rate = 1e-8, random_seed = 1269258439)\n# Time difference of 2.141642 mins\n\nIf you’re bothered by how long the simulation takes, feel free to call these two lines to 100% reproduce my results without any expensive computation:\n\nurl_path &lt;- \"https://raw.githubusercontent.com/bodkan/simgen/refs/heads/main/files/popgen/introgression.trees\"\nts_path &lt;- tempfile()\ndownload.file(url_path, destfile = ts_path, mode = \"wb\")\nts &lt;- ts_read(file = ts_path, model = model)\n\n\n\n\nInspect the tree-sequence object saved in the ts variable by typing it into the R console again (this interactivity really helps with catching nasty bugs early during the programming of your script). You can also do a similar thing via the table produced by the ts_samples() function. You should see a much smaller number of individuals being recorded, indicating that the simulation was much more efficient and produced genomic data for only the individuals of interest.\n\n\nNote: When you think about it, it’s actually quite astonishing how fast msprime and tskit are when dealing with such a huge amount of sequence data from tens of thousands of individuals on a simple laptop!\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\n# Inspect the (tskit/Python-based) summary of the new tree sequence\n# (note the much smaller number of \"sample nodes\"!)\nts\n\n╔═══════════════════════════╗\n║TreeSequence               ║\n╠═══════════════╤═══════════╣\n║Trees          │      9,710║\n╟───────────────┼───────────╢\n║Sequence Length│  1,000,000║\n╟───────────────┼───────────╢\n║Time Units     │generations║\n╟───────────────┼───────────╢\n║Sample Nodes   │     48,000║\n╟───────────────┼───────────╢\n║Total Size     │    8.8 MiB║\n╚═══════════════╧═══════════╝\n╔═══════════╤═══════╤═════════╤════════════╗\n║Table      │Rows   │Size     │Has Metadata║\n╠═══════════╪═══════╪═════════╪════════════╣\n║Edges      │134,646│  4.1 MiB│          No║\n╟───────────┼───────┼─────────┼────────────╢\n║Individuals│ 24,000│656.3 KiB│          No║\n╟───────────┼───────┼─────────┼────────────╢\n║Migrations │      0│  8 Bytes│          No║\n╟───────────┼───────┼─────────┼────────────╢\n║Mutations  │      0│ 16 Bytes│          No║\n╟───────────┼───────┼─────────┼────────────╢\n║Nodes      │104,956│  2.8 MiB│          No║\n╟───────────┼───────┼─────────┼────────────╢\n║Populations│      4│341 Bytes│         Yes║\n╟───────────┼───────┼─────────┼────────────╢\n║Provenances│      1│  2.7 KiB│          No║\n╟───────────┼───────┼─────────┼────────────╢\n║Sites      │      0│ 16 Bytes│          No║\n╚═══════════╧═══════╧═════════╧════════════╝\n\n# Get the table of all recorded samples in the tree sequence\nts_samples(ts)\n\n# A tibble: 24,000 × 3\n   name      time pop  \n   &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;\n 1 CHIMP_1      0 AFR  \n 2 CHIMP_2      0 AFR  \n 3 CHIMP_3      0 AFR  \n 4 CHIMP_4      0 AFR  \n 5 CHIMP_5      0 AFR  \n 6 CHIMP_6      0 AFR  \n 7 CHIMP_7      0 AFR  \n 8 CHIMP_8      0 AFR  \n 9 CHIMP_9      0 AFR  \n10 CHIMP_10     0 AFR  \n# ℹ 23,990 more rows\n\n# Compute the count of individuals in different time points\nlibrary(dplyr)\n\nts_samples(ts) %&gt;% group_by(pop, present_day = time == 0) %&gt;% tally %&gt;% select(present_day, pop, n)\n\n# A tibble: 4 × 3\n# Groups:   pop [4]\n  present_day pop       n\n  &lt;lgl&gt;       &lt;chr&gt; &lt;int&gt;\n1 TRUE        AFR   15000\n2 TRUE        CHIMP  5000\n3 TRUE        EUR    3000\n4 TRUE        NEA    1000",
    "crumbs": [
      "Modeling fundamentals",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Demographic models</span>"
    ]
  },
  {
    "objectID": "popgen-stats.html",
    "href": "popgen-stats.html",
    "title": "Tree-sequence statistics",
    "section": "",
    "text": "Part 1: Computing nucleotide diversity\nIn this exercise, you will build on top of the results from the exercise on programming demographic models. Specifically, we will learn how to compute popgen statistics on slendr-simulated tree sequences using slendr’s interface to the tskit Python module.\nFirst, create a new R script stats.R and paste in the following code. This is one of the possible solutions to the Exercise 1, and it’s easier if we all use it to be on the same page from now on, starting from the same model and the same simulated tree sequence:\nAs a sanity check, let’s use a couple of tidyverse table-munging tricks to make sure the tree sequence does contain a set of sample which matches our intended sampling schedule (particularly the time series of European individuals and the two Neanderthals):\nEverything looks good! Having made sure that the ts object contains the individuals we want, let’s move to the exercise.\nThe toy model of ancient human history plotted above makes a fairly clear prediction of what would be the nucleotide diversity expected in the simulated populations. Compute the nucleotide diversity in all populations using the slendr function ts_diversity() in your tree sequence ts. Do you get numbers that (relatively between all populations) match what would expect from the model given the \\(N_e\\) that you programmed for each?\nHint: Nearly every slendr statistic function interfacing with tskit accepts a ts tree-sequence object as its first argument, with further arguments being either a vector of individual names representing a group of samples to compute a statistic on, or a (named) list of such vectors (each element of that list for a group of samples) – these lists are intended to be equivalent to the sample_sets = argument of many tskit Python methods (which you’ve learned about in your activity on tskit), except that they allow symbolic names of individuals, rather then integer indices of nodes in a tree sequence.\nAlthough you can get all the above information by processing the table produced by the ts_samples() function, slendr provides a useful helper function ts_names() which only returns the names of individuals as a vector (or a named list of such vectors, one vector per population as shown below).\nWhen you call it directly, you get a plain vector of individual names:\nts_names(ts)\n\n [1] \"NEA_1\"   \"EUR_1\"   \"NEA_2\"   \"EUR_2\"   \"EUR_3\"   \"EUR_4\"   \"EUR_5\"  \n [8] \"EUR_6\"   \"EUR_7\"   \"EUR_8\"   \"EUR_9\"   \"EUR_10\"  \"EUR_11\"  \"EUR_12\" \n[15] \"EUR_13\"  \"EUR_14\"  \"EUR_15\"  \"EUR_16\"  \"EUR_17\"  \"EUR_18\"  \"EUR_19\" \n[22] \"EUR_20\"  \"AFR_1\"   \"AFR_2\"   \"AFR_3\"   \"AFR_4\"   \"AFR_5\"   \"CHIMP_1\"\n[29] \"EUR_21\"  \"EUR_22\"  \"EUR_23\"  \"EUR_24\"  \"EUR_25\"  \"EUR_26\"  \"EUR_27\" \n[36] \"EUR_28\"  \"EUR_29\"  \"EUR_30\"\nThis is not super helpful, unless we want to compute some statistic for everyone in the tree sequence, regardless of their population assignment. Perhaps a bit more useful is to call the function like this, because it will produce a result which can be immediately used as the sample_sets = argument mentioned in the Hint above:\nts_names(ts, split = \"pop\")\n\n$AFR\n[1] \"AFR_1\" \"AFR_2\" \"AFR_3\" \"AFR_4\" \"AFR_5\"\n\n$CHIMP\n[1] \"CHIMP_1\"\n\n$EUR\n [1] \"EUR_1\"  \"EUR_2\"  \"EUR_3\"  \"EUR_4\"  \"EUR_5\"  \"EUR_6\"  \"EUR_7\"  \"EUR_8\" \n [9] \"EUR_9\"  \"EUR_10\" \"EUR_11\" \"EUR_12\" \"EUR_13\" \"EUR_14\" \"EUR_15\" \"EUR_16\"\n[17] \"EUR_17\" \"EUR_18\" \"EUR_19\" \"EUR_20\" \"EUR_21\" \"EUR_22\" \"EUR_23\" \"EUR_24\"\n[25] \"EUR_25\" \"EUR_26\" \"EUR_27\" \"EUR_28\" \"EUR_29\" \"EUR_30\"\n\n$NEA\n[1] \"NEA_1\" \"NEA_2\"\nAs you can see, this gave us a normal R list, with each element containing a vector of individual names in a population. Note that we can use standard R list indexing to get subsets of individuals:\nnames &lt;- ts_names(ts, split = \"pop\")\n\nnames[\"NEA\"]\n\n$NEA\n[1] \"NEA_1\" \"NEA_2\"\n\nnames[c(\"EUR\", \"NEA\")]\n\n$EUR\n [1] \"EUR_1\"  \"EUR_2\"  \"EUR_3\"  \"EUR_4\"  \"EUR_5\"  \"EUR_6\"  \"EUR_7\"  \"EUR_8\" \n [9] \"EUR_9\"  \"EUR_10\" \"EUR_11\" \"EUR_12\" \"EUR_13\" \"EUR_14\" \"EUR_15\" \"EUR_16\"\n[17] \"EUR_17\" \"EUR_18\" \"EUR_19\" \"EUR_20\" \"EUR_21\" \"EUR_22\" \"EUR_23\" \"EUR_24\"\n[25] \"EUR_25\" \"EUR_26\" \"EUR_27\" \"EUR_28\" \"EUR_29\" \"EUR_30\"\n\n$NEA\n[1] \"NEA_1\" \"NEA_2\"\netc.\nMany of the following exercises will use these kinds of tricks to instruct various slendr / tskit functions to compute statistics on subsets of all individuals sub-sampled in this way.\nAfter you computed nucleotide diversity per-population, compute it for each individual separately using the same function ts_diversity() (which, in this setting, gives you effectively the heterozygosity for each individual). If you are familiar with plotting in R, visualize the individual-based heterozygosities across all populations.\nHint: You can do this by giving a vector of names as sample_sets = (so not an R list of vectors). You could also use the data frame produced by ts_samples(ts) to get the names, just adding the heterozygosities to that data frame as a new column.",
    "crumbs": [
      "Modeling fundamentals",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree-sequence statistics</span>"
    ]
  },
  {
    "objectID": "popgen-stats.html#part-1-computing-nucleotide-diversity",
    "href": "popgen-stats.html#part-1-computing-nucleotide-diversity",
    "title": "Tree-sequence statistics",
    "section": "",
    "text": "Click to see the solution\n\n\n\n\n\nPopulation-based nucleotide diversity:\nLet’s first get a named list of individuals in each group we want to be working with (slendr tree-sequence statistic functions generally operate with this kind of structure):\n\nsample_sets &lt;- ts_names(ts, split = \"pop\")\nsample_sets\n\n$AFR\n[1] \"AFR_1\" \"AFR_2\" \"AFR_3\" \"AFR_4\" \"AFR_5\"\n\n$CHIMP\n[1] \"CHIMP_1\"\n\n$EUR\n [1] \"EUR_1\"  \"EUR_2\"  \"EUR_3\"  \"EUR_4\"  \"EUR_5\"  \"EUR_6\"  \"EUR_7\"  \"EUR_8\" \n [9] \"EUR_9\"  \"EUR_10\" \"EUR_11\" \"EUR_12\" \"EUR_13\" \"EUR_14\" \"EUR_15\" \"EUR_16\"\n[17] \"EUR_17\" \"EUR_18\" \"EUR_19\" \"EUR_20\" \"EUR_21\" \"EUR_22\" \"EUR_23\" \"EUR_24\"\n[25] \"EUR_25\" \"EUR_26\" \"EUR_27\" \"EUR_28\" \"EUR_29\" \"EUR_30\"\n\n$NEA\n[1] \"NEA_1\" \"NEA_2\"\n\n\nWe can use such sample_sets object to compute nucleotide diversity (pi)\nin each population, in a bit of a similar manner to how we would do it with the standard tskit in Python:\n\npi_pop &lt;- ts_diversity(ts, sample_sets = sample_sets)\narrange(pi_pop, diversity)\n\n# A tibble: 4 × 2\n  set   diversity\n  &lt;chr&gt;     &lt;dbl&gt;\n1 NEA   0.0000481\n2 CHIMP 0.000189 \n3 EUR   0.000543 \n4 AFR   0.000593 \n\n\nYou can see that this simple computation fits the extreme differences in long-term \\(N_e\\) encoded by your slendr demografr model.\nPer-individual heterozygosity:\nWe can do this by passing the vector of individual names directory as the sample_sets = argument, rather than in a list of groups as we did above.\nFor convenience, we first get a table of all individuals (which of course contains also their names) and in the next step, we’ll just add their heterozygosities as a new column:\n\npi_df &lt;- ts_samples(ts)\npi_df$name\n\n [1] \"NEA_1\"   \"EUR_1\"   \"NEA_2\"   \"EUR_2\"   \"EUR_3\"   \"EUR_4\"   \"EUR_5\"  \n [8] \"EUR_6\"   \"EUR_7\"   \"EUR_8\"   \"EUR_9\"   \"EUR_10\"  \"EUR_11\"  \"EUR_12\" \n[15] \"EUR_13\"  \"EUR_14\"  \"EUR_15\"  \"EUR_16\"  \"EUR_17\"  \"EUR_18\"  \"EUR_19\" \n[22] \"EUR_20\"  \"AFR_1\"   \"AFR_2\"   \"AFR_3\"   \"AFR_4\"   \"AFR_5\"   \"CHIMP_1\"\n[29] \"EUR_21\"  \"EUR_22\"  \"EUR_23\"  \"EUR_24\"  \"EUR_25\"  \"EUR_26\"  \"EUR_27\" \n[36] \"EUR_28\"  \"EUR_29\"  \"EUR_30\" \n\npi_df$diversity &lt;- ts_diversity(ts, sample_sets = pi_df$name)$diversity\npi_df\n\n# A tibble: 38 × 4\n   name   time pop   diversity\n   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 NEA_1 70000 NEA   0.0000446\n 2 EUR_1 40000 EUR   0.000594 \n 3 NEA_2 40000 NEA   0.0000416\n 4 EUR_2 38000 EUR   0.000488 \n 5 EUR_3 36000 EUR   0.000631 \n 6 EUR_4 34000 EUR   0.000544 \n 7 EUR_5 32000 EUR   0.000541 \n 8 EUR_6 30000 EUR   0.000555 \n 9 EUR_7 28000 EUR   0.000594 \n10 EUR_8 26000 EUR   0.000555 \n# ℹ 28 more rows\n\n\nLet’s plot the results using the ggplot2 package (because a picture is worth a thousand numbers!)\n\nlibrary(ggplot2)\n\nggplot(pi_df, aes(pop, diversity, color = pop, group = pop)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter() +\n  theme_bw()",
    "crumbs": [
      "Modeling fundamentals",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree-sequence statistics</span>"
    ]
  },
  {
    "objectID": "popgen-stats.html#part-2-computing-pairwise-divergence",
    "href": "popgen-stats.html#part-2-computing-pairwise-divergence",
    "title": "Tree-sequence statistics",
    "section": "Part 2: Computing pairwise divergence",
    "text": "Part 2: Computing pairwise divergence\nUse the function ts_divergence() to compute genetic divergence between all pairs of populations. Again, do you get results compatible with our demographic model in terms of expectation given the split times between populations as you programmed them for your model?\nHint: Again, you can use the same concept of sample_sets = we discussed in the previous part. In this case, the function computes pairwise divergence between each element of the list given as sample_sets = (i.e., for each vector of individual names).\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\nsample_sets &lt;- ts_names(ts, split = \"pop\")\n\ndiv_df &lt;- ts_divergence(ts, sample_sets)\narrange(div_df, divergence)\n\n# A tibble: 6 × 3\n  x     y     divergence\n  &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n1 AFR   EUR     0.000651\n2 EUR   NEA     0.000931\n3 AFR   NEA     0.000978\n4 CHIMP NEA     0.00416 \n5 CHIMP EUR     0.00416 \n6 AFR   CHIMP   0.00417 \n\n\nWe can see that the pairwise nucleotide divergences between populations recapitulate the known population/species relationships we would expect from our model.",
    "crumbs": [
      "Modeling fundamentals",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree-sequence statistics</span>"
    ]
  },
  {
    "objectID": "popgen-stats.html#part-3-detecting-neanderthal-admixture-in-europeans",
    "href": "popgen-stats.html#part-3-detecting-neanderthal-admixture-in-europeans",
    "title": "Tree-sequence statistics",
    "section": "Part 3: Detecting Neanderthal admixture in Europeans",
    "text": "Part 3: Detecting Neanderthal admixture in Europeans\nLet’s now pretend its about 2008, we’ve sequenced the first Neanderthal genome, and we are working on a project that will change human evolution research forever. We also have the genomes of a couple of people from Africa and Europe, which we want to use to answer the most burning question of all evolutionary anthropology: “Do some people living today carry Neanderthal ancestry?”\nEarlier you’ve learned about \\(f\\)-statistics of various kinds. You have also heard that an \\(f_4\\) statistic (or its equivalent \\(D\\) statistic) can be used as a test of “treeness”. Simply speaking, for some “quartet” of individuals or population samples, they can be used as a hypothesis test of whether the history of those samples is compatible with there not having been an introgression.\nCompute the \\(f_4\\) test of Neanderthal introgression in EUR individuals using the slendr function ts_f4(). When you’re running it, you will have to provide individuals to compute the statistic using a slightly different format. Take a look at the help page available as ?ts_f4 for more information. When you’re computing the \\(f_4\\), make sure to set mode = \"branch\" argument of the ts_f4() function (we will get to why a bit later).\n\n\nNote: By default, each slendr / tskit statistic function operates on mutations, and this will switch them to use branch length (as you might know, \\(f\\)-statistics are mathematically defined using branch lengths in trees and mode = \"branch\" does exactly that).\nHint: If you haven’t learned this in your \\(f\\)-statistics lecture, you want to compute (and compare) the values of these two statistics using the slendr function ts_f4():\n\n\\(f_4\\)(&lt;some African&gt;, &lt;another African&gt;; &lt;Neanderthal&gt;, &lt;Chimp&gt;)\n\\(f_4\\)(&lt;some African&gt;, &lt;a test European&gt;; &lt;Neanderthal&gt;, &lt;Chimp&gt;),\n\nhere &lt;individual&gt; can be the name of any individual recorded in your tree sequence, such as names you saw as name column in the table returned by ts_samples(ts) (i.e. \"NEA_1\" could be used as a “representative” &lt;Neanderthal&gt; in those equations, similarly for \"CHIMP_1\" as the fourth sample in the \\(f_4\\) quarted representing the outgroup).\nTo simplify things a lot, we can understand the above equations as comparing the counts of so-called BABA and ABBA allele patterns between the quarted of samples specified in the statistics:\n\\[\nf_4(AFR, X; NEA, CHIMP) = \\frac{\\#BABA - \\#ABBA}{\\#SNPs}\n\\]\nThe first \\(f_4\\) statistic above is not expected to give values “too different” from 0 (even in case of Neanderthal introgression into Europeans) because we don’t expect two African individuals to differ “significantly” in terms of how much alleles they share with a Neanderthal (because their ancestors never met Neanderthals!). The other should – if there was a Neanderthal introgression into Europeans some time in their history – be “significantly negative”.\nIs the second of those two statistics “much more negative” than the first, as expected assuming introgression from Neanderthals into Europeans?\nWhy am I putting “significantly” and “much more negative” in quotes in the previous sentence? What are we missing here for this being a true hypothesis test as you might be accustomed to from computing \\(f\\)-statistics using a tool such as ADMIXTOOLS? (We will get to this again in the following part of this exercise.)\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nCompute the difference in the amount of allele sharing between two African individuals and a Neanderthal:\n\nf4_null &lt;- ts_f4(ts, W = \"AFR_1\", X = \"AFR_2\", Y = \"NEA_1\", Z = \"CHIMP_1\", mode = \"branch\")\nf4_null\n\n# A tibble: 1 × 5\n  W     X     Y     Z          f4\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 AFR_1 AFR_2 NEA_1 CHIMP_1 -157.\n\n\nCompute the difference in the amount of allele sharing between an African individual vs European individual and a Neanderthal:\n\nf4_alt &lt;- ts_f4(ts, W = \"AFR_1\", X = \"EUR_1\", Y = \"NEA_1\", Z = \"CHIMP_1\", mode = \"branch\")\nf4_alt\n\n# A tibble: 1 × 5\n  W     X     Y     Z           f4\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;\n1 AFR_1 EUR_1 NEA_1 CHIMP_1 -3351.\n\n\nWe can see that the second test resulted in an f4 value about ~20 times more negative than the first test, indicating that a European in our test carries “significantly more” Neanderthal alleles compared to the baseline expectation of no introgression established by the comparison to an African …\n\nabs(f4_alt$f4 / f4_null$f4)\n\n[1] 21.36632\n\n\n… although this is not a real test of significance (we have no Z-score or standard error which would give us something like a p-value for the hypothesis test, as we get by jackknife procedure in ADMIXTOOLS)",
    "crumbs": [
      "Modeling fundamentals",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree-sequence statistics</span>"
    ]
  },
  {
    "objectID": "popgen-stats.html#part-4-detecting-neanderthal-admixture-in-europeans-v2.0",
    "href": "popgen-stats.html#part-4-detecting-neanderthal-admixture-in-europeans-v2.0",
    "title": "Tree-sequence statistics",
    "section": "Part 4: Detecting Neanderthal admixture in Europeans v2.0",
    "text": "Part 4: Detecting Neanderthal admixture in Europeans v2.0\nThe fact that we don’t get something equivalent to a p-value in these kinds of simulations is generally not a problem, because we’re often interested in establishing a trend of a statistic under various conditions, and understanding when and how its expected value behaves in a certain way. If statistical noise is a problem, we work around this by computing a statistic on multiple simulation replicates or even increasing the sample sizes.\n\n\nNote: To see this in practice, you can check out a paper in which I used this approach quite successfully on a related problem.\nOn top of that, p-value of something like an \\(f\\)-statistic (whether it’s significantly different from zero) is also strongly affected by quality of the data, sequencing errors, coverage, etc. (which can certainly be examined using simulations!). However, these are aspects of modeling which are quite orthogonal to the problem of investigating the expectations and trends of statistics given some underlying evolutionary model, which is what we’re after in these exercises.\nThat said, even in perfect simulated data, what exactly does “significantly different from zero compared to some baseline expectation” mean can be blurred by noise with simple single-individual comparisons that we did above. Let’s increase the sample size a bit to see if a statistical pattern expected in \\(f_4\\) statistic from our Neanderthal introgression model becomes more apparent.\nCompute the first \\(f_4\\) statistic (the baseline expectation between a pair of Africans) and the second \\(f_4\\) statistic (comparing an African and a European), but this time on all recorded Africans and all recorded Europeans, respectively. Plot the distributions of those two sets of statistics. This should remove lots of the uncertainty and make a statistical trend stand out much more clearly.\nHint: Whenever you need to compute something for many things in sequence, looping is very useful. One way to do compute, say, an \\(f_4\\) statistic over many individuals is by using this kind of pattern using R’s looping function lapply():\n\n# Loop over vector of individual names (variable x) and apply a given ts_f4()\n# expression on each individual (note the ts_f4(..., X = x, ...) in the code)\nlist_f4 &lt;- lapply(\n  c(\"ind_1\", \"ind_2\", ...),\n  function(x) ts_f4(ts, W = \"AFR_1\", X = x, Y = \"NEA_1\", Z = \"CHIMP_1\", mode = \"branch\")\n)\n\n# The above gives us a list of data frames, so we need to bind them all into a\n# single table for easier interpretation and visualization\ndf_f4 &lt;- do.call(rbind, list_f4)\n\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nThis gives us list of vectors of the names of all individuals in each population…\n\nsample_sets &lt;- ts_names(ts, split = \"pop\")\n# ... which we can then access like this\nsample_sets$AFR # all Africans\n\n[1] \"AFR_1\" \"AFR_2\" \"AFR_3\" \"AFR_4\" \"AFR_5\"\n\nsample_sets$EUR # all Europeans\n\n [1] \"EUR_1\"  \"EUR_2\"  \"EUR_3\"  \"EUR_4\"  \"EUR_5\"  \"EUR_6\"  \"EUR_7\"  \"EUR_8\" \n [9] \"EUR_9\"  \"EUR_10\" \"EUR_11\" \"EUR_12\" \"EUR_13\" \"EUR_14\" \"EUR_15\" \"EUR_16\"\n[17] \"EUR_17\" \"EUR_18\" \"EUR_19\" \"EUR_20\" \"EUR_21\" \"EUR_22\" \"EUR_23\" \"EUR_24\"\n[25] \"EUR_25\" \"EUR_26\" \"EUR_27\" \"EUR_28\" \"EUR_29\" \"EUR_30\"\n\n\nLet’s compute the f4 statistic for all Africans…\n\nf4_afr_list &lt;- lapply(\n  sample_sets$AFR,\n  function(x) ts_f4(ts, W = \"AFR_1\", X = x, Y = \"NEA_1\", Z = \"CHIMP_1\", mode = \"branch\")\n)\n# ... and Europeans\nf4_eur_list &lt;- lapply(\n  sample_sets$EUR,\n  function(x) ts_f4(ts, W = \"AFR_1\", X = x, Y = \"NEA_1\", Z = \"CHIMP_1\", mode = \"branch\")\n)\n\nBind each list of data frames into a single data frame:\n\nf4_afr &lt;- do.call(rbind, f4_afr_list)\nf4_eur &lt;- do.call(rbind, f4_eur_list)\n\n# add population columns to each of the two results for easier plotting\nf4_afr$pop &lt;- \"AFR\"\nf4_eur$pop &lt;- \"EUR\"\n\n# bind both tables together\nf4_results &lt;- rbind(f4_afr, f4_eur)\n\nNow we can visualize the results:\n\nf4_results %&gt;%\n  ggplot(aes(pop, f4, color = pop)) +\n  geom_boxplot() +\n  geom_jitter() +\n  geom_hline(yintercept = 0, linetype = 2) +\n  ggtitle(\"f4(AFR, EUR; NEA, CHIMP)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe can see that the \\(f_4\\) statistic test of Neanderthal introgression in Europeans indeed does give a much more negative distribution of values compared to the baseline expectation which compares two Africans to each other.",
    "crumbs": [
      "Modeling fundamentals",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree-sequence statistics</span>"
    ]
  },
  {
    "objectID": "popgen-stats.html#bonus-exercises",
    "href": "popgen-stats.html#bonus-exercises",
    "title": "Tree-sequence statistics",
    "section": "Bonus exercises",
    "text": "Bonus exercises\n\n\n\n\n\n\nBonus exercises\n\n\n\n\n\n\nBonus 1: mode = \"branch\" vs mode = \"site\"\nRepeat the previous part of the exercise by setting mode = \"site\" in the ts_f4() function calls (this is actually the default behavior of all slendr tree-sequence based tskit functions). This will switch the tskit computation to using mutation counts along each branch of the tree sequence, rather than using branch length themselves. Why might the branch-based computation be a bit better if what we’re after is investigating the expected values of statistics under some model?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nSee this tutorial (and particularly the directly linked section) for explanation.\n\n\n\n\n\nBonus 2: Outgroup \\(f_3\\) statistic\nUse the function ts_f3() to compute the outgroup \\(f_3\\) statistic between pairs of African-European, African-Neanderthal, and European-Neanderthal and a Chimpanzee outgroup.\nHint: The \\(f_3\\) statistic is traditionally expressed as \\(f_3(A, B; C)\\), where C represents the outgroup. Unfortunately, in tskit the outgroup is named A, with B and C being the pair of samples from which we trace the length of branches towards the outgroup, so the statistic is interpreted as \\(f_3(B, C; A)\\).\nHow do the outgroup f3 results compare to your expectation based on simple population relationships (and to the divergence computation above)?\nDo you see any impact of introgression on the \\(f_3\\) value when a Neanderthal is included in the computation?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\n# Standard formalism is this:\n#    f3(A, B; C) = E[ (A - C) * (B - C) ]\n# But in tskit, A is the outgroup (different from ADMIXTOOLS!), see below...\n\n# We can compute f3 for individuals...\nts_f3(ts, B = \"AFR_1\", C = \"EUR_1\", A = \"CHIMP_1\")\n\n# A tibble: 1 × 4\n  A       B     C          f3\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 CHIMP_1 AFR_1 EUR_1 0.00375\n\n# ... but also whole populations (or population samples)\nts_f3(ts, B = sample_sets[\"AFR\"], C = sample_sets[\"EUR\"], A = \"CHIMP_1\")\n\n# A tibble: 1 × 4\n  A       B     C          f3\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 CHIMP_1 AFR   EUR   0.00375\n\nts_f3(ts, B = sample_sets[\"AFR\"], C = sample_sets[\"NEA\"], A = \"CHIMP_1\")\n\n# A tibble: 1 × 4\n  A       B     C          f3\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 CHIMP_1 AFR   NEA   0.00358\n\nts_f3(ts, B = sample_sets[\"EUR\"], C = sample_sets[\"NEA\"], A = \"CHIMP_1\")\n\n# A tibble: 1 × 4\n  A       B     C          f3\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 CHIMP_1 EUR   NEA   0.00360\n\n\n\n\n\n\n\nBonus 3: Outgroup \\(f_3\\) statistic as a linear combination of \\(f_2\\) statistics\nYou might have learned that any complex \\(f\\)-statistic can be expressed as a linear combination of multiple \\(f_2\\) statistics (which represent simple branch length separating two lineages). Verify that this is the case by looking up equation (20b) in this amazing paper and compute an \\(f_3\\) statistic for any arbitrary trio of individuals of your choosing using this linear combination of \\(f_2\\) statistics.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\n# standard f3\nts_f3(ts, B = \"AFR_1\", C = \"AFR_2\", A = \"CHIMP_1\")\n\n# A tibble: 1 × 4\n  A       B     C          f3\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 CHIMP_1 AFR_1 AFR_2 0.00378\n\n# a \"homemade\" f3 statistic as a linear combination of f2 statistics\n# f3(A, B; C) = f2(A, C) + f2(B, C) - f2(A, B) / 2\nhomemade_f3 &lt;- (\n  ts_f2(ts, A = \"AFR_1\", B = \"CHIMP_1\")$f2 +\n  ts_f2(ts, A = \"AFR_2\", B = \"CHIMP_1\")$f2 -\n  ts_f2(ts, A = \"AFR_1\", B = \"AFR_2\")$f2\n) / 2\n\nhomemade_f3\n\n[1] 0.003779369\n\n\n\n\n\n\n\nBonus 4: Trajectory of Neanderthal ancestry in Europe over time\nThere used to be a lot of controversy about the question of whether or not did Neanderthal ancestry proportion in Europeans decline or not over the past 40 thousand years (see figure 1 in this paper figure 2 in this paper).\nYour simulated tree sequence contains a time-series of European individuals over time. Use the slendr function ts_f4ratio() to compute (and then plot) the proportion (commonly designated as alpha) of Neanderthal ancestry in Europe over time. Use \\(f_4\\)-ratio statistic of the following form:\n\nts_f4ratio(ts, X = &lt;vector of ind. names&gt;, A = \"NEA_1\", B = \"NEA_2\", C = \"AFR_1\", O = \"CHIMP_1\")\n\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\n# Extract table with names and times of sampled Europeans (ancient and present day)\neur_inds &lt;- ts_samples(ts) %&gt;% filter(pop == \"EUR\")\neur_inds\n\n# A tibble: 30 × 3\n   name    time pop  \n   &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;\n 1 EUR_1  40000 EUR  \n 2 EUR_2  38000 EUR  \n 3 EUR_3  36000 EUR  \n 4 EUR_4  34000 EUR  \n 5 EUR_5  32000 EUR  \n 6 EUR_6  30000 EUR  \n 7 EUR_7  28000 EUR  \n 8 EUR_8  26000 EUR  \n 9 EUR_9  24000 EUR  \n10 EUR_10 22000 EUR  \n# ℹ 20 more rows\n\n# Compute f4-ration statistic (this will take ~30s) -- note that we can provide\n# a vector of names for the X sample set to the `ts_f4ratio()` function\nnea_ancestry &lt;- ts_f4ratio(ts, X = eur_inds$name, A = \"NEA_1\", B = \"NEA_2\", C = \"AFR_1\", O = \"CHIMP_1\")\n\n# Add the age of each sample to the table of proportions\nnea_ancestry$time &lt;- eur_inds$time\nnea_ancestry\n\n# A tibble: 30 × 7\n   X      A     B     C     O        alpha  time\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 EUR_1  NEA_1 NEA_2 AFR_1 CHIMP_1 0.0741 40000\n 2 EUR_2  NEA_1 NEA_2 AFR_1 CHIMP_1 0.0202 38000\n 3 EUR_3  NEA_1 NEA_2 AFR_1 CHIMP_1 0.0731 36000\n 4 EUR_4  NEA_1 NEA_2 AFR_1 CHIMP_1 0.0354 34000\n 5 EUR_5  NEA_1 NEA_2 AFR_1 CHIMP_1 0.0132 32000\n 6 EUR_6  NEA_1 NEA_2 AFR_1 CHIMP_1 0.0401 30000\n 7 EUR_7  NEA_1 NEA_2 AFR_1 CHIMP_1 0.0295 28000\n 8 EUR_8  NEA_1 NEA_2 AFR_1 CHIMP_1 0.0476 26000\n 9 EUR_9  NEA_1 NEA_2 AFR_1 CHIMP_1 0.0261 24000\n10 EUR_10 NEA_1 NEA_2 AFR_1 CHIMP_1 0.0350 22000\n# ℹ 20 more rows\n\nnea_ancestry %&gt;%\n  ggplot(aes(time, alpha)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", linetype = 2, color = \"red\", linewidth = 0.5) +\n  xlim(40000, 0) +\n  coord_cartesian(ylim = c(0, 0.1)) +\n  labs(x = \"time [years ago]\", y = \"Neanderthal ancestry proportion\") +\n  theme_bw() +\n  ggtitle(\"Neanderthal ancestry proportion in Europeans over time\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# For good measure, let's test the significance of the decline using a linear model\nsummary(lm(alpha ~ time, data = nea_ancestry))\n\n\nCall:\nlm(formula = alpha ~ time, data = nea_ancestry)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.027358 -0.011309 -0.002650  0.007912  0.034803 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.569e-02  4.153e-03  11.001 1.13e-11 ***\ntime        -1.588e-07  2.123e-07  -0.748    0.461    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01589 on 28 degrees of freedom\nMultiple R-squared:  0.0196,    Adjusted R-squared:  -0.01541 \nF-statistic: 0.5598 on 1 and 28 DF,  p-value: 0.4606\n\n\n\n\n\n\n\nBonus 5: How many unique f4 quartets are there?\nIn your lecture about \\(f\\)-statistics, you’ve probably learned about various symmetries in \\(f_4\\) (but also other \\(f\\)-statistics) depending on the arrangement of the “quartet”. As a trivial example, an \\(f_3(A; B, C)\\) and \\(f_3(A; C, B)\\) will give you exactly the same value, and the same thing applies even to more complex \\(f\\)-statistics like \\(f_4\\).\nUse simulations to compute how manu unique \\(f_4\\) values involving a single quartet are there.\nHint: Draw some trees to figure out why could that be true. Also, when computing ts_f4(), set mode = \"branch\" to avoid the effect of statistical noise due to mutations.\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\n\n# # install a combinatorics R package\n# install.packages(\"combinat\")\n\nlibrary(combinat)\n\n\nAttaching package: 'combinat'\n\n\nThe following object is masked from 'package:utils':\n\n    combn\n\n# These are the four samples we can create quartet combinations from\nquartet &lt;- c(\"AFR_1\", \"EUR_1\", \"NEA_1\", \"CHIMP_1\")\nquartets &lt;- permn(quartet)\nquartets\n\n[[1]]\n[1] \"AFR_1\"   \"EUR_1\"   \"NEA_1\"   \"CHIMP_1\"\n\n[[2]]\n[1] \"AFR_1\"   \"EUR_1\"   \"CHIMP_1\" \"NEA_1\"  \n\n[[3]]\n[1] \"AFR_1\"   \"CHIMP_1\" \"EUR_1\"   \"NEA_1\"  \n\n[[4]]\n[1] \"CHIMP_1\" \"AFR_1\"   \"EUR_1\"   \"NEA_1\"  \n\n[[5]]\n[1] \"CHIMP_1\" \"AFR_1\"   \"NEA_1\"   \"EUR_1\"  \n\n[[6]]\n[1] \"AFR_1\"   \"CHIMP_1\" \"NEA_1\"   \"EUR_1\"  \n\n[[7]]\n[1] \"AFR_1\"   \"NEA_1\"   \"CHIMP_1\" \"EUR_1\"  \n\n[[8]]\n[1] \"AFR_1\"   \"NEA_1\"   \"EUR_1\"   \"CHIMP_1\"\n\n[[9]]\n[1] \"NEA_1\"   \"AFR_1\"   \"EUR_1\"   \"CHIMP_1\"\n\n[[10]]\n[1] \"NEA_1\"   \"AFR_1\"   \"CHIMP_1\" \"EUR_1\"  \n\n[[11]]\n[1] \"NEA_1\"   \"CHIMP_1\" \"AFR_1\"   \"EUR_1\"  \n\n[[12]]\n[1] \"CHIMP_1\" \"NEA_1\"   \"AFR_1\"   \"EUR_1\"  \n\n[[13]]\n[1] \"CHIMP_1\" \"NEA_1\"   \"EUR_1\"   \"AFR_1\"  \n\n[[14]]\n[1] \"NEA_1\"   \"CHIMP_1\" \"EUR_1\"   \"AFR_1\"  \n\n[[15]]\n[1] \"NEA_1\"   \"EUR_1\"   \"CHIMP_1\" \"AFR_1\"  \n\n[[16]]\n[1] \"NEA_1\"   \"EUR_1\"   \"AFR_1\"   \"CHIMP_1\"\n\n[[17]]\n[1] \"EUR_1\"   \"NEA_1\"   \"AFR_1\"   \"CHIMP_1\"\n\n[[18]]\n[1] \"EUR_1\"   \"NEA_1\"   \"CHIMP_1\" \"AFR_1\"  \n\n[[19]]\n[1] \"EUR_1\"   \"CHIMP_1\" \"NEA_1\"   \"AFR_1\"  \n\n[[20]]\n[1] \"CHIMP_1\" \"EUR_1\"   \"NEA_1\"   \"AFR_1\"  \n\n[[21]]\n[1] \"CHIMP_1\" \"EUR_1\"   \"AFR_1\"   \"NEA_1\"  \n\n[[22]]\n[1] \"EUR_1\"   \"CHIMP_1\" \"AFR_1\"   \"NEA_1\"  \n\n[[23]]\n[1] \"EUR_1\"   \"AFR_1\"   \"CHIMP_1\" \"NEA_1\"  \n\n[[24]]\n[1] \"EUR_1\"   \"AFR_1\"   \"NEA_1\"   \"CHIMP_1\"\n\n# How many permutations there are in total?\n#   4! = 4 * 3 * 2 * 1 = 24\nfactorial(4)\n\n[1] 24\n\n# We should therefore have 24 different quartet combinations of samples\nlength(quartets)\n\n[1] 24\n\n# Loop across all quartets, computing the corresponding f4 statistic (we want\n# to do this using branch lengths, not mutations, as the mutation-based computation\n# would involve statistical noise)\nall_f4s &lt;- lapply(quartets, function(q) ts_f4(ts, q[1], q[2], q[3], q[4], mode = \"branch\"))\n\n# Bind the list of f4 results into a single data frame and inspect the results\nall_f4s &lt;- bind_rows(all_f4s) %&gt;% arrange(abs(f4))\nprint(all_f4s, n = Inf)\n\n# A tibble: 24 × 5\n   W       X       Y       Z            f4\n   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n 1 AFR_1   EUR_1   NEA_1   CHIMP_1  -3351.\n 2 AFR_1   EUR_1   CHIMP_1 NEA_1     3351.\n 3 NEA_1   CHIMP_1 AFR_1   EUR_1    -3351.\n 4 CHIMP_1 NEA_1   AFR_1   EUR_1     3351.\n 5 CHIMP_1 NEA_1   EUR_1   AFR_1    -3351.\n 6 NEA_1   CHIMP_1 EUR_1   AFR_1     3351.\n 7 EUR_1   AFR_1   CHIMP_1 NEA_1    -3351.\n 8 EUR_1   AFR_1   NEA_1   CHIMP_1   3351.\n 9 AFR_1   NEA_1   CHIMP_1 EUR_1   -13496.\n10 AFR_1   NEA_1   EUR_1   CHIMP_1  13496.\n11 NEA_1   AFR_1   EUR_1   CHIMP_1 -13496.\n12 NEA_1   AFR_1   CHIMP_1 EUR_1    13496.\n13 EUR_1   CHIMP_1 NEA_1   AFR_1   -13496.\n14 CHIMP_1 EUR_1   NEA_1   AFR_1    13496.\n15 CHIMP_1 EUR_1   AFR_1   NEA_1   -13496.\n16 EUR_1   CHIMP_1 AFR_1   NEA_1    13496.\n17 AFR_1   CHIMP_1 EUR_1   NEA_1    16847.\n18 CHIMP_1 AFR_1   EUR_1   NEA_1   -16847.\n19 CHIMP_1 AFR_1   NEA_1   EUR_1    16847.\n20 AFR_1   CHIMP_1 NEA_1   EUR_1   -16847.\n21 NEA_1   EUR_1   CHIMP_1 AFR_1    16847.\n22 NEA_1   EUR_1   AFR_1   CHIMP_1 -16847.\n23 EUR_1   NEA_1   AFR_1   CHIMP_1  16847.\n24 EUR_1   NEA_1   CHIMP_1 AFR_1   -16847.\n\n# Narrow down the results to only unique f4 values\ndistinct(all_f4s, f4, .keep_all = TRUE)\n\n# A tibble: 6 × 5\n  W       X       Y       Z            f4\n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n1 AFR_1   EUR_1   NEA_1   CHIMP_1  -3351.\n2 AFR_1   EUR_1   CHIMP_1 NEA_1     3351.\n3 AFR_1   NEA_1   CHIMP_1 EUR_1   -13496.\n4 AFR_1   NEA_1   EUR_1   CHIMP_1  13496.\n5 AFR_1   CHIMP_1 EUR_1   NEA_1    16847.\n6 CHIMP_1 AFR_1   EUR_1   NEA_1   -16847.\n\ndistinct(all_f4s, abs(f4), .keep_all = TRUE)\n\n# A tibble: 3 × 6\n  W     X       Y       Z            f4 `abs(f4)`\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 AFR_1 EUR_1   NEA_1   CHIMP_1  -3351.     3351.\n2 AFR_1 NEA_1   CHIMP_1 EUR_1   -13496.    13496.\n3 AFR_1 CHIMP_1 EUR_1   NEA_1    16847.    16847.",
    "crumbs": [
      "Modeling fundamentals",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree-sequence statistics</span>"
    ]
  },
  {
    "objectID": "inference-afs.html",
    "href": "inference-afs.html",
    "title": "\\(N_e\\) inference with AFS",
    "section": "",
    "text": "Part 1: A self-contained slendr function of \\(N_e \\rightarrow \\textrm{AFS}\\)\nSo far we’ve learned how slendr provides an easy way to define demographic models in R and simulate (even very large!) tree sequences from them. This allows us to quickly verify our intuition about some popgen problem (things like “Hmmm, I wonder what would an \\(f_4\\) statistic look like if my model includes this particular gene-flow event?), in just a few lines of R. There have been instances in which we’ve been able to even answer questions like this directly in a meeting, pretty much on the spot! This makes slendr a very powerful “popgen calculator”.\nNow let’s take things one step further. Imagine you gathered some empirical data, like an allele frequency spectrum (AFS) from a population that you study. That data was, in the real world, produced by some (hidden) biological process (demographic history) that we want to learn about. For instance, the population we study had some \\(N_e\\), which we don’t know the value of (the only thing we have is the observed AFS) but we want to infer that value.\nSimulations can be a great tool to estimate the most likely value of such an unknown parameter. Briefly speaking, in this particular toy example, we can simulate a large number of AFS vectors (each resulting from a different assumed \\(N_e\\) value) and then pick just those \\(N_e\\) values (or just one \\(N_e\\) value) which produced a simulated AFS closest to the observed AFS.\nThis is exactly what you’ll be doing just now in Exercise 3.\nIn a new script afs.R write a custom R function called simulate_afs(), which will take Ne as its only parameter. Use this function to compute (and return) AFS vectors for a couple of Ne values of your choosing, but staying between Ne = 1000 and Ne = 30000 Plot those AFS vectors and observe how (and why?) do they differ based on Ne parameter you used in each respective simulation.\nHint: The function should create a one-population forward-time model (our population starting at time = 1, with the model simulation_length = 100000 and generation_time = 1 in compile_model()), simulate 10Mb tree sequence using msprime() (recombination rate 1e-8) and then overlay neutral mutations on it at mutation_rate = 1e-8), compute AFS for 10 samples and return the AFS vector as result of this custom function.\nHint: If you’ve never programmed before, the concept of a “custom function” might be very alien to you. Again, if you need help, feel free to start building your afs.R solution based on this “template” (just fill in missing relevant bits of slendr code that you should be already familiar with):\nlibrary(slendr)\ninit_env()\n\nsimulate_afs &lt;- function(Ne) {\n  # In here you should write code which will:\n  #   1. create one population with a given Ne (provided as a function argument)\n  #   2. compile a model using `simulation_length =` and `generation_time =`\n  #   3. simulate a tree sequence\n  #   4. select names of 10 samples (doesn't matter which, \"pop_1\", \"po2_\", ...)\n  #   5. compute AFS vector from those 10 individuals using `ts_afs()`\n  \n  # `result` is a variable with your 10-sample AFS vector (we remove the\n  # first element because it's not meaningful for our example)\n  return(result[-1]) \n}\n\nafs_1 &lt;- simulate_afs(Ne = 1000) # simulate AFS from a Ne = 1000 model...\nplot(afs_1, type =\"o\")           # ... and plot it\nHint: If the above still doesn’t make any sense to you, feel free to copy-paste the function from the solution below into your script and work with that function instead!\nWhen used in R, your custom function should work like this (the simulation is stochastic, so your numbers will be different, of course):\n# This gives us a vector of singletons, doubletons, etc., etc., all the way\n# to the number of fixed mutations in our sample of 10 individuals\nsimulate_afs(Ne = 1000)\n\n [1] 410 181 176  95  87  60  38  50  29  55  15  29  19  40  35  41  17  17  13\n[20]  24",
    "crumbs": [
      "Simulation-based inference",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>$N_e$ inference with AFS</span>"
    ]
  },
  {
    "objectID": "inference-afs.html#part-1-a-self-contained-slendr-function-of-n_e-rightarrow-textrmafs",
    "href": "inference-afs.html#part-1-a-self-contained-slendr-function-of-n_e-rightarrow-textrmafs",
    "title": "\\(N_e\\) inference with AFS",
    "section": "",
    "text": "Note: Remember that you should drop the first element of the AFS vector produced by ts_afs() (for instance with something like result[-1] if result contains the output of ts_afs()) technical reasons related to tskit. You don’t have to worry about that here, but you can read this for more detail.\n\n\n\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nA function can be understood as a independent unit of a computer program which executes a block of code inside the {…} brackets given some values of some parameters. In our example, we programmed a function simulate_sfs() which accepts a single parameter, Ne.\n\nsimulate_afs &lt;- function(Ne) {\n  # create a slendr model with a single population of size Ne = N\n  pop &lt;- population(\"pop\", N = Ne, time = 1)\n  model &lt;- compile_model(pop, generation_time = 1, simulation_length = 100000)\n\n  # simulate a tree sequence\n  ts &lt;-\n    msprime(model, sequence_length = 10e6, recombination_rate = 1e-8) %&gt;%\n    ts_mutate(mutation_rate = 1e-8)\n\n  # get a random sample of names of 10 individuals\n  samples &lt;- ts_names(ts) %&gt;% sample(10)\n\n  # compute the AFS vector (dropping the 0-th element added by tskit)\n  afs &lt;- ts_afs(ts, sample_sets = list(samples))[-1]\n\n  afs\n}\n\nOur functions is supposed to produce an AFS vector of counts of alleles observed at a given frequency a the population sample:\nLet’s use our custom function to simulate AFS vector for Ne = 1k, 10k, and 30k:\n\nafs_1k &lt;- simulate_afs(1000)\nafs_10k &lt;- simulate_afs(10000)\nafs_30k &lt;- simulate_afs(30000)\n\nHere’s one of those vectors. We can see that the function does, indeed, produce a result of the correct format:\n\nafs_1k\n\n [1] 376 220 154  98 102  93  71  46  52  42  44  25  53  25  35  14  15  10  15\n[20]  17\n\n\nTo see the results of this function in a clearer context, let’s visualize the vectors in the same plot:\n\nplot(afs_30k, type = \"o\", main = \"AFS, Ne = 30000\", col = \"cyan\",)\nlines(afs_10k, type = \"o\", main = \"AFS, Ne = 10000\", col = \"purple\")\nlines(afs_1k, type = \"o\", main = \"AFS, Ne = 1000\", col = \"blue\")\nlegend(\"topright\", legend = c(\"Ne = 1k\", \"Ne = 10k\", \"Ne = 30k\"),\n       fill = c(\"blue\", \"purple\", \"cyan\"))",
    "crumbs": [
      "Simulation-based inference",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>$N_e$ inference with AFS</span>"
    ]
  },
  {
    "objectID": "inference-afs.html#part-2-estimating-unknown-n_e-from-empirical-afs",
    "href": "inference-afs.html#part-2-estimating-unknown-n_e-from-empirical-afs",
    "title": "\\(N_e\\) inference with AFS",
    "section": "Part 2: Estimating unknown \\(N_e\\) from empirical AFS",
    "text": "Part 2: Estimating unknown \\(N_e\\) from empirical AFS\nImagine you sequenced 10 samples from a population and computed the following AFS vector (which contains, sequentially, the number of singletons, doubletons, etc., in your sample from a population):\n\n\nafs_observed &lt;- c(2520, 1449, 855, 622, 530, 446, 365, 334, 349, 244,\n                  264, 218,  133, 173, 159, 142, 167, 129, 125, 143)\n\nYou know (maybe from some fossil evidence) that the population probably had a constant \\(N_e\\) somewhere between 1000 and 30000 for the past 100,000 generations, and had mutation and recombination rates of 1e-8 (i.e., parameters already implemented by your simulate_afs() function – how convenient!).\nUse slendr simulations to guess the true (and hidden!) \\(N_e\\) given the observed AFS by running simulations for a range of \\(N_e\\) values and finding out which \\(N_e\\) produces the closest AFS vector to the afs_observed vector above using one of the following two approaches.\n\nOption 1 [easy]: Plot AFS vectors for various \\(N_e\\) values (i.e. simulate several of them using your function simulate_afs()), then eyeball which looks closest to the observed AFS based on the figures alone. (This is, of course, not how proper statistical inference is done, but it will be good enough for this exercie!)\nOption 2 [hard]: Simulate AFS vectors in steps of possible Ne (maybe lapply()?), and find the \\(N_e\\) which gives the closest AFS to the observed AFS based on Mean squared error.\n\n\n\n\n\n\n\nClick to see the solution to “Option 1”\n\n\n\n\n\nThis is the observed AFS with which we want to compare our simulated AFS vectors:\n\nafs_observed &lt;- c(2520, 1449, 855, 622, 530, 446, 365, 334, 349, 244,\n                  264, 218,  133, 173, 159, 142, 167, 129, 125, 143)\n\nWe know that the true \\(N_e\\) is supposed to be between 1000 and 30000, so let’s simulate a bunch of AFS vectors for different \\(N_e\\) values using our new AFS simulation function:\n\nafs_Ne1k &lt;- simulate_afs(Ne = 1000)\nafs_Ne5k &lt;- simulate_afs(Ne = 5000)\nafs_Ne6k &lt;- simulate_afs(Ne = 6000)\nafs_Ne10k &lt;- simulate_afs(Ne = 10000)\nafs_Ne20k &lt;- simulate_afs(Ne = 20000)\nafs_Ne30k &lt;- simulate_afs(Ne = 30000)\n\nNow let’s plot our simulated AFS vectors together with the observed AFS (highlighting it in black):\n\nplot(afs_observed, type = \"b\", col = \"black\", lwd = 3,\n     xlab = \"allele count bin\", ylab = \"count\", ylim = c(0, 13000))\nlines(afs_Ne1k, lwd = 2, col = \"blue\")\nlines(afs_Ne5k, lwd = 2, col = \"green\")\nlines(afs_Ne6k, lwd = 2, col = \"pink\")\nlines(afs_Ne10k, lwd = 2, col = \"purple\")\nlines(afs_Ne20k, lwd = 2, col = \"orange\")\nlines(afs_Ne30k, lwd = 2, col = \"cyan\")\nlegend(\"topright\",\n       legend = c(\"observed AFS\", \"Ne = 1000\", \"Ne = 5000\",\n                  \"Ne = 6000\", \"Ne = 10000\", \"Ne = 20000\", \"Ne = 30000\"),\n       fill = c(\"black\", \"blue\", \"green\", \"pink\", \"purple\", \"orange\", \"cyan\"))\n\n\n\n\n\n\n\n\nThe true \\(N_e\\) was 6543!\n\n\n\n\n\n\n\n\n\nClick to see the solution to “Option 2”\n\n\n\n\n\nThis is the observed AFS with which we want to compare our simulated AFS vectors:\n\nafs_observed &lt;- c(2520, 1449, 855, 622, 530, 446, 365, 334, 349, 244,\n                  264, 218,  133, 173, 159, 142, 167, 129, 125, 143)\n\nWe know that the true \\(N_e\\) is supposed to be between 1000 and 30000. Let’s generate regularly spaced values of potential Ne values whose AFS we want to investigate and compare to the obesrved AFS (our parameter grid):\n\nNe_grid &lt;- seq(from = 1000, to = 30000, by = 500)\nNe_grid\n\n [1]  1000  1500  2000  2500  3000  3500  4000  4500  5000  5500  6000  6500\n[13]  7000  7500  8000  8500  9000  9500 10000 10500 11000 11500 12000 12500\n[25] 13000 13500 14000 14500 15000 15500 16000 16500 17000 17500 18000 18500\n[37] 19000 19500 20000 20500 21000 21500 22000 22500 23000 23500 24000 24500\n[49] 25000 25500 26000 26500 27000 27500 28000 28500 29000 29500 30000\n\n\nWith the parameter grid Ne_grid set up, let’s simulate an AFS from each \\(N_e\\) model:\n\nlibrary(parallel)\n\nafs_grid &lt;- mclapply(Ne_grid, simulate_afs, mc.cores = detectCores())\nnames(afs_grid) &lt;- Ne_grid\n\n# show the first five simulated AFS vectors, for brevity, just to demonstrate\n# what the output of the grid simulations is supposed to look like\nafs_grid[1:5]\n\n$`1000`\n [1] 397 268 159 116  88  83  40  40  43  27  55  29  33  20  35  28  29  43  21\n[20]   6\n\n$`1500`\n [1] 659 295 190 189 110  93  71  58  48  54  54  40  39  35  52  52  26  38  43\n[20]  33\n\n$`2000`\n [1] 798 381 305 243 167 132 120  95  92  78  82  52  56  58  41  63  49  23  44\n[20]  44\n\n$`2500`\n [1] 902 437 264 247 192 175 164 124 105  74  72  74  78  67  73  39  48  56  72\n[20]  67\n\n$`3000`\n [1] 1288  608  368  318  247  150  215  157  156  111  139  120  101   56   81\n[16]   86   48   54   52   65\n\n\nPlot the observed AFS and overlay the simulated AFS vectors on top of it:\n\nplot(afs_observed, type = \"b\", col = \"black\", lwd = 3, xlab = \"allele count bin\", ylab = \"count\")\nfor (i in seq_along(Ne_grid)) {\n  lines(afs_grid[[i]], lwd = 0.5)\n}\nlegend(\"topright\", legend = c(\"observed AFS\", \"simulated AFS\"), fill = c(\"black\", \"gray\"))\n\n\n\n\n\n\n\n\nCompute mean-squared error of the AFS produced by each \\(N_e\\) value across the grid:\n\nerrors &lt;- sapply(afs_grid, function(sim_afs) {\n  sum((sim_afs - afs_observed)^2) / length(sim_afs)\n})\n\nplot(Ne_grid, errors, ylab = \"error\")\nabline(v = Ne_grid[which.min(errors)], col = \"red\")\nlegend(\"topright\", legend = paste(\"minimum error Ne =\", Ne_grid[which.min(errors)]), fill = \"red\")\n\n\n\n\n\n\n\n\nPlot the AFS again, but this time highlight the most likely spectrum (i.e. the one which gave the lowest RMSE value):\n\nplot(afs_observed, type = \"b\", col = \"black\", lwd = 3, xlab = \"allele count bin\", ylab = \"count\")\nfor (i in seq_along(Ne_grid)) {\n  color &lt;- if (i == which.min(errors)) \"red\" else \"gray\"\n  width &lt;- if (i == which.min(errors)) 2 else 0.75\n  lines(afs_grid[[i]], lwd = width, col = color)\n}\nlegend(\"topright\", legend = c(\"observed AFS\", paste(\"best fitting Ne =\", Ne_grid[which.min(errors)])),\n       fill = c(\"black\", \"red\"))\n\n\n\n\n\n\n\n\nThe true \\(N_e\\) was 6543!\n\n\n\nCongratulations, you now know how to infer parameters of evolutionary models using simulations! What you just did is really very similar to how simulation-based inference is done in practice (even with methods such as ABC). Hopefully you now also see how easy slendr makes it to do this (normally a rather laborious) process.\nThis kind of approach can be used to infer all sorts of demographic parameters, even using other summary statistics that you’ve also learned to compute… including selection parameters, which we delve into in another exercise.",
    "crumbs": [
      "Simulation-based inference",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>$N_e$ inference with AFS</span>"
    ]
  },
  {
    "objectID": "handouts.html",
    "href": "handouts.html",
    "title": "Cheatsheets and handouts",
    "section": "",
    "text": "PDF cheatsheets\nThis section contains a single-page rendering of slides introducing some chapters in form of handouts. These are intended for easier reference during practical exercise sessions for each relevant chapter.\nAdditionally, exercises for some chapters (namely on fundamentals of R programming, tidyverse, and ggplot2) are supported by a set of beautiful cheatsheets. These contain everything needed as a reference for all exercises, but also much, much more than that. Keep them on your desk as you beging working on your own projects!",
    "crumbs": [
      "Cheatsheets and handouts"
    ]
  },
  {
    "objectID": "handouts.html#pdf-cheatsheets",
    "href": "handouts.html#pdf-cheatsheets",
    "title": "Cheatsheets and handouts",
    "section": "",
    "text": "“R bootcamp session” – cheatsheet on base R, cheatsheet on RStudio\n“Introduction to tidyverse” and “More tidyverse practice” – cheatsheet on the dplyr package\n“Data visualization” – cheatsheet on the ggplot2 package\n“Quarto reports and slides” – cheatsheet on the Quarto framework",
    "crumbs": [
      "Cheatsheets and handouts"
    ]
  },
  {
    "objectID": "handouts.html#slide-handouts",
    "href": "handouts.html#slide-handouts",
    "title": "Cheatsheets and handouts",
    "section": "Slide handouts",
    "text": "Slide handouts\n\n“R bootcamp” – one-page handouts\n“Introduction to tidyverse” – one-page handouts\n“Introduction to slendr” – one-page handouts",
    "crumbs": [
      "Cheatsheets and handouts"
    ]
  },
  {
    "objectID": "handouts_r-bootcamp.html",
    "href": "handouts_r-bootcamp.html",
    "title": "R bootcamp",
    "section": "",
    "text": "R is the best technology for doing computational science\n(A few remarks and tips before the practical session)",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "handouts_r-bootcamp.html#r-has-an-incredible-wealth-of-toolkits",
    "href": "handouts_r-bootcamp.html#r-has-an-incredible-wealth-of-toolkits",
    "title": "R bootcamp",
    "section": "R has an incredible wealth of toolkits",
    "text": "R has an incredible wealth of toolkits\nThe most famous is the tidyverse ecosystem for data science:\n\n\n\nThere are packages for machine learning (Keras, Tensorflow), spatial packages (sf, stars), packages specific to research fields (genomics, ecology, etc.). More than 23000 packages total.",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "handouts_r-bootcamp.html#r-has-awesome-easy-to-use-tools-for-reproducibility",
    "href": "handouts_r-bootcamp.html#r-has-awesome-easy-to-use-tools-for-reproducibility",
    "title": "R bootcamp",
    "section": "R has awesome easy-to-use(!) tools for reproducibility",
    "text": "R has awesome easy-to-use(!) tools for reproducibility\n\nQuarto “authoring system” for writing automated reports, slides, PDF documents, etc. (our “Topic #4!”)\ntargets pipelining framework (possibly the most powerful and flexible of its kind)\ntidyverse framework (particularly the dplyr R package introduced as “Topic 2/3”) is designed to facilitate building readable, easy-to-write processing pipelines\nR itself is a very powerful, flexible programming language",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "handouts_r-bootcamp.html#the-unfortunate-way-r-is-taught",
    "href": "handouts_r-bootcamp.html#the-unfortunate-way-r-is-taught",
    "title": "R bootcamp",
    "section": "The unfortunate way R is taught…",
    "text": "The unfortunate way R is taught…\n\nSome slides on “R as a calculator” (only half joking)\nThen straight into plotting histograms and computing t-tests\n\nEffectively treats computation / data science as black box\n\n\n. . .\n\nR was first created “by statisticians for statisticians” (1991)\n\nSo this way of teaching R makes sense historically\n\n\n. . .\n\nBut teaching needs change in modern times:\n\nOur data is larger and more complex than in 1990s\nReproducibility requires proper programming skills",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "handouts_r-bootcamp.html#challenge-of-teaching-programming",
    "href": "handouts_r-bootcamp.html#challenge-of-teaching-programming",
    "title": "R bootcamp",
    "section": "Challenge of teaching programming",
    "text": "Challenge of teaching programming\n\nProgramming is a skill, not a knowledge to transfer\nTeaching R in a lecture format would mean 3 hours of torture\n\n. . .\n\nToday’s “R bootcamp” session is designed to walk you through fundamentals of R in an interactive form.\n\n. . .\nA series of problems-solutions to develop understanding of:\n\nWhat happens behind the scenes of data-science operations.\nWhich will give you tools and confidence to build “mental models”",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "handouts_r-bootcamp.html#knowing-rstudio-well-is-like-having-a-superpower",
    "href": "handouts_r-bootcamp.html#knowing-rstudio-well-is-like-having-a-superpower",
    "title": "R bootcamp",
    "section": "Knowing RStudio well is like having a superpower",
    "text": "Knowing RStudio well is like having a superpower\nDon’t take it as nothing but a text editor like Notepad.\nIt’s a starship Enterprise of data science at your fingertips. It’s incredible powerful and has a lot of features.\n. . .\nThis cheatsheet has a lot of information, but try to internalize  keyboard shortcuts which I highlighted in yellow in the PDF.\nAt first it will be annoying and slower to use keyboard and not a mouse, but trust me. It will pay of in the long run.",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "handouts_r-bootcamp.html#read-eval-print-loop-repl",
    "href": "handouts_r-bootcamp.html#read-eval-print-loop-repl",
    "title": "R bootcamp",
    "section": "Read-Eval-Print Loop (REPL)",
    "text": "Read-Eval-Print Loop (REPL)\n\n[…] the user enters expressions (rather than an entire [computer program]), the REPL evaluates them and displays the results […] – Wikipedia\n\nAn idea from ancient computers (1964!) with these functions:\n\nread — accepts a bit of code from a user (1 + 2)\neval — evaluates the code (applies + on 1 and 2, yielding 3)\nprint — prints the result 6 on the screen\n\nSteps 1.-3. repeat in an infinite loop, until the program closes.\n. . .\n\n\nR console is a powerful REPL!",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "handouts_r-bootcamp.html#r-console-is-like-an-ultimate-experimental-lab-equipment",
    "href": "handouts_r-bootcamp.html#r-console-is-like-an-ultimate-experimental-lab-equipment",
    "title": "R bootcamp",
    "section": "R console is like an ultimate experimental lab equipment",
    "text": "R console is like an ultimate experimental lab equipment\nR encourages a highly interactive workflow.\nWhen I don’t understand something, some code I don’t get, etc., I always type it in the REPL to build an intuition.\n. . .\nDoing data analysis is like playing a detective, especially when figuring out bugs and problems.\n. . .\nForm a hypothesis, run a tiny bit of R code to test the hypothesis. Move forward based on the result you got.\n. . .\nI see a lot of experienced PhD students writing and running long code top-to-bottom, instead of thinking methodically.",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "handouts_r-bootcamp.html#built-in-r-help-always-has-an-answer",
    "href": "handouts_r-bootcamp.html#built-in-r-help-always-has-an-answer",
    "title": "R bootcamp",
    "section": "Built-in R help always has an answer!",
    "text": "Built-in R help always has an answer!\nAll languages (and their packages) have documentation, sure.\nBut it’s mostly scattered on the internet, often hard to find.\n. . .\nR packages have a standardized documentation inside R!\n\nEvery func has a manual page available at command ?func\n\n. . .\nEvery single such help page describes:\n\nBasic usage of the function\nWhich optional parameters can be given\nDescription of what the function does\nRunnable example code (!!!)",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "handouts_r-bootcamp.html#these-manuals-are-amazingly-helpful",
    "href": "handouts_r-bootcamp.html#these-manuals-are-amazingly-helpful",
    "title": "R bootcamp",
    "section": "These manuals are amazingly helpful",
    "text": "These manuals are amazingly helpful\n\n\n\n\n(Help for a function ts_tajima() from my R package.)",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "handouts_r-bootcamp.html#consider-switching-the-pane-layout",
    "href": "handouts_r-bootcamp.html#consider-switching-the-pane-layout",
    "title": "R bootcamp",
    "section": "Consider switching the pane layout",
    "text": "Consider switching the pane layout\nIn the RStudio menu Global Options -&gt; Pane Layout set:\n\n\n\nMaximum vertical space for code and easy switching between script and R console (particularly with keyboard shortcuts).",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>R bootcamp</span>"
    ]
  },
  {
    "objectID": "handouts_tidy-basics.html",
    "href": "handouts_tidy-basics.html",
    "title": "Introduction to tidyverse",
    "section": "",
    "text": "Quick recap from our R bootcamp yesterday\n(A few remarks and tips before the practical session)",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "handouts_tidy-basics.html#vectors-and-lists",
    "href": "handouts_tidy-basics.html#vectors-and-lists",
    "title": "Introduction to tidyverse",
    "section": "Vectors and lists",
    "text": "Vectors and lists\n\nVectors are collections of values of the same type:\n\n\nsample   &lt;- c(\"Loschbour\", \"UstIshim\", \"Saqqaq\", \"AltaiNeandertal\")\ncoverage &lt;- c(18.2,        35.2,       13.4,     44.8)\narchaic  &lt;- c(FALSE,       FALSE,      FALSE,    TRUE)\n\n. . .\n\nLists are collections of anything:\n\n\nlist(\"Hello\", TRUE, 123)\n\n[[1]]\n[1] \"Hello\"\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] 123\n\n\n. . .\n\n… and that “anything” can also include other vectors!",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "handouts_tidy-basics.html#an-example-of-such-a-list-of-vectors",
    "href": "handouts_tidy-basics.html#an-example-of-such-a-list-of-vectors",
    "title": "Introduction to tidyverse",
    "section": "An example of such a list of vectors…",
    "text": "An example of such a list of vectors…\n\n\nFrom vectors stored as individual variables…\n\n\n\nsample   &lt;- c(\"Loschbour\", \"UstIshim\", \"Saqqaq\", \"AltaiNeandertal\")\ncoverage &lt;- c(18.2,        35.2,       13.4,     44.8)\nage      &lt;- c(8050,        45020,      3885,     125000)",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "handouts_tidy-basics.html#an-example-of-such-a-list-of-vectors-1",
    "href": "handouts_tidy-basics.html#an-example-of-such-a-list-of-vectors-1",
    "title": "Introduction to tidyverse",
    "section": "An example of such a list of vectors…",
    "text": "An example of such a list of vectors…\n\n\nTo those vectors stored as (named) list…\n\n\n\nlist(\n  sample   = c(\"Loschbour\", \"UstIshim\", \"Saqqaq\", \"AltaiNeandertal\"),\n  coverage = c(18.2,        35.2,       13.4,     44.8),\n  age      = c(8050,        45020,      3885,     125000)\n)",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "handouts_tidy-basics.html#data-frame-is-just-that",
    "href": "handouts_tidy-basics.html#data-frame-is-just-that",
    "title": "Introduction to tidyverse",
    "section": "Data frame is just that",
    "text": "Data frame is just that\n\n\nA list of vectors…\n\n\n\nlist(\n  sample   = c(\"Loschbour\", \"UstIshim\", \"Saqqaq\", \"AltaiNeandertal\"),\n  coverage = c(18.2,        35.2,       13.4,     44.8),\n  age      = c(8050,        45020,      3885,     125000)\n)",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "handouts_tidy-basics.html#data-frame-is-just-that-1",
    "href": "handouts_tidy-basics.html#data-frame-is-just-that-1",
    "title": "Introduction to tidyverse",
    "section": "Data frame is just that",
    "text": "Data frame is just that\n\n\n… which is just printed as a table.\n\n\n\ndata.frame(\n  sample   = c(\"Loschbour\", \"UstIshim\", \"Saqqaq\", \"AltaiNeandertal\"),\n  coverage = c(18.2,        35.2,       13.4,     44.8),\n  age      = c(8050,        45020,      3885,     125000)\n)\n\n           sample coverage    age\n1       Loschbour     18.2   8050\n2        UstIshim     35.2  45020\n3          Saqqaq     13.4   3885\n4 AltaiNeandertal     44.8 125000",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "handouts_tidy-basics.html#indexing-into-tables-dfrows-cols",
    "href": "handouts_tidy-basics.html#indexing-into-tables-dfrows-cols",
    "title": "Introduction to tidyverse",
    "section": "Indexing into tables: df[rows, cols]",
    "text": "Indexing into tables: df[rows, cols]\n Indexing by columns (“selecting columns”)  \n\ndf[, c(\"sample\", \"coverage\")]\n\n           sample coverage\n1       Loschbour     18.2\n2        UstIshim     35.2\n3          Saqqaq     13.4\n4 AltaiNeandertal     44.8",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "handouts_tidy-basics.html#indexing-into-tables-dfrows-cols-1",
    "href": "handouts_tidy-basics.html#indexing-into-tables-dfrows-cols-1",
    "title": "Introduction to tidyverse",
    "section": "Indexing into tables: df[rows, cols]",
    "text": "Indexing into tables: df[rows, cols]\n Indexing by rows (“filtering rows”) \n\nusing row numbers:\n\n\ndf[c(2, 3), ]\n\n    sample coverage   age\n2 UstIshim     35.2 45020\n3   Saqqaq     13.4  3885\n\n\n. . .\n\nusing TRUE/FALSE for each row:\n\n\n\n\ndf[c(FALSE, TRUE, FALSE, TRUE), ]\n\n           sample coverage    age\n2        UstIshim     35.2  45020\n4 AltaiNeandertal     44.8 125000\n\n\n\n\n\ndf[df$coverage &gt; 30, ] # same thing!\n\n           sample coverage    age\n2        UstIshim     35.2  45020\n4 AltaiNeandertal     44.8 125000",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "handouts_tidy-basics.html#we-can-also-extract-columns-with",
    "href": "handouts_tidy-basics.html#we-can-also-extract-columns-with",
    "title": "Introduction to tidyverse",
    "section": "We can also extract columns with $",
    "text": "We can also extract columns with $\n\n\nIf df is our data frame:\n\n\n\n           sample coverage    age\n1       Loschbour     18.2   8050\n2        UstIshim     35.2  45020\n3          Saqqaq     13.4   3885\n4 AltaiNeandertal     44.8 125000\n\n\n\n\n. . .\n\nWe can do this:\n\n\ndf$age\n\n[1]   8050  45020   3885 125000\n\n\n. . .\n\nAnd also this:\n\n\nmean(df$age)\n\n[1] 45488.75\n\n\n. . .\n\nOr maybe this, etc.:\n\n\nis.na(df$age)\n\n[1] FALSE FALSE FALSE FALSE",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "handouts_tidy-basics.html#what-is-tidyverse",
    "href": "handouts_tidy-basics.html#what-is-tidyverse",
    "title": "Introduction to tidyverse",
    "section": "What is tidyverse?",
    "text": "What is tidyverse?\n\n\n\n\nThe tidyverse is a language for solving data science challenges with R code. Its primary goal is to facilitate a conversation between a human and a computer about data. Less abstractly, the tidyverse is a collection of R packages that share a high-level design philosophy […] so that learning one package makes it easier to learn the next. The tidyverse encompasses the repeated tasks at the heart of every data science project: data import, tidying, manipulation, visualisation, and programming.",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "handouts_tidy-basics.html#further-companion-study-material",
    "href": "handouts_tidy-basics.html#further-companion-study-material",
    "title": "Introduction to tidyverse",
    "section": "Further companion study material",
    "text": "Further companion study material\n\n\nhttps://r4ds.hadley.nz",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "handouts_tidy-basics.html#why-those-two-data-sets",
    "href": "handouts_tidy-basics.html#why-those-two-data-sets",
    "title": "Introduction to tidyverse",
    "section": "Why those two data sets?",
    "text": "Why those two data sets?\n\nTable of metadata information associated with each sample\nGenome-wide data set of Identity-by-Descent segments\n\n\n\nBest representatives of modern population genetic data\nLots of opportunities to practice tidyverse data processing\nEven more opportunities to showcase ggplot2 possibilities",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to _tidyverse_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html",
    "href": "handouts_slendr.html",
    "title": "Introduction to slendr",
    "section": "",
    "text": "Why use simulations?",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#developing-intuition-into-statistics",
    "href": "handouts_slendr.html#developing-intuition-into-statistics",
    "title": "Introduction to slendr",
    "section": "Developing intuition into statistics",
    "text": "Developing intuition into statistics\n\n\n\n\n\nImage from Peter (2016)",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#developing-intuition-into-statistics-1",
    "href": "handouts_slendr.html#developing-intuition-into-statistics-1",
    "title": "Introduction to slendr",
    "section": "Developing intuition into statistics",
    "text": "Developing intuition into statistics\n\n\n\n\n\nImage from Lawson et al. (2018)",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#estimating-model-parameters-i.e.-abc",
    "href": "handouts_slendr.html#estimating-model-parameters-i.e.-abc",
    "title": "Introduction to slendr",
    "section": "Estimating model parameters (i.e. ABC)",
    "text": "Estimating model parameters (i.e. ABC)\n\n\n\n\n\nImage from Wikipedia on ABC",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#ground-truth-for-method-development",
    "href": "handouts_slendr.html#ground-truth-for-method-development",
    "title": "Introduction to slendr",
    "section": "Ground truth for method development",
    "text": "Ground truth for method development\n\n\n\n\n\nImage from Schiffels and Durbin (2014)",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#simulation-software",
    "href": "handouts_slendr.html#simulation-software",
    "title": "Introduction to slendr",
    "section": "Simulation software",
    "text": "Simulation software\nThe most famous and widely used are SLiM and msprime.\n\nThey are very powerful and (nearly) infinitely flexible.\n\n\nHowever, they both require:\n\nquite a bit of code for complex simulations (“complex” is relative, of course)\nrelatively high confidence in programming\n\n\n\n\n\n\nOur exercises will focus on the slendr simulation toolkit for population genetics in R.\n\n\n\n\n\nBut, as a recap, let’s look at msprime and SLiM a little bit…",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#section-1",
    "href": "handouts_slendr.html#section-1",
    "title": "Introduction to slendr",
    "section": "",
    "text": "What is msprime?\n\n\nA Python module for writing coalescent simulations\nExtremely fast (genome-scale, population-scale data!)\nYou should know Python fairly well to build complex models\n\n\n\n\n\n\nImage modified from Alexei Drummond",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#simple-simulation-using-msprime",
    "href": "handouts_slendr.html#simple-simulation-using-msprime",
    "title": "Introduction to slendr",
    "section": "Simple simulation using msprime",
    "text": "Simple simulation using msprime\n\n\nimport msprime\n\ndemography = msprime.Demography()\ndemography.add_population(name=\"A\", initial_size=10_000)\ndemography.add_population(name=\"B\", initial_size=5_000)\ndemography.add_population(name=\"C\", initial_size=1_000)\ndemography.add_population_split(time=1000, derived=[\"A\", \"B\"], ancestral=\"C\")\n\nts = msprime.sim_ancestry(\n  sequence_length=10e6,\n  recombination_rate=1e-8,\n  samples={\"A\": 100, \"B\": 100},\n  demography=demography\n)",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#section-2",
    "href": "handouts_slendr.html#section-2",
    "title": "Introduction to slendr",
    "section": "",
    "text": "What is SLiM?\n\n\nA forward-time simulator\nHas its own programming language\nMassive library of functions for:\n\ndemographic events\nvarious mating systems\nnatural selection\n\nMore than 700 pages long manual!\n\n\n\n\n\n\nImage modified from Alexei Drummond",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#simple-neutral-simulation-in-slim",
    "href": "handouts_slendr.html#simple-neutral-simulation-in-slim",
    "title": "Introduction to slendr",
    "section": "Simple neutral simulation in SLiM",
    "text": "Simple neutral simulation in SLiM\n\n\ninitialize() {\n    // create a neutral mutation type\n    initializeMutationType(\"m1\", 0.5, \"f\", 0.0);\n\n    // initialize 1Mb segment\n    initializeGenomicElementType(\"g1\", m1, 1.0);\n    initializeGenomicElement(g1, 0, 999999);\n\n    // set mutation rate and recombination rate of the segment\n    initializeMutationRate(1e-8);\n    initializeRecombinationRate(1e-8);\n}\n\n// create an ancestral population p1 of 10000 diploid individuals\n1 early() { sim.addSubpop(\"p1\", 10000); }\n\n// in generation 1000, create two daughter populations p2 and p3\n1000 early() {\n    sim.addSubpopSplit(\"p2\", 5000, p1);\n    sim.addSubpopSplit(\"p3\", 1000, p1);\n}\n\n// in generation 10000, stop the simulation and save 100 individuals\n// from p2 and p3 to a VCF file\n10000 late() {\n    p2_subset = sample(p2.individuals, 100);\n    p3_subset = sample(p3.individuals, 100);\n    c(p2_subset, p3_subset).genomes.outputVCF(\"/tmp/slim_output.vcf.gz\");\n\n    sim.simulationFinished();\n}",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#so-why-slendr",
    "href": "handouts_slendr.html#so-why-slendr",
    "title": "Introduction to slendr",
    "section": "… so why slendr?",
    "text": "… so why slendr?\n\n\n\nwww.slendr.net",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#first-motivation-spatial-simulations",
    "href": "handouts_slendr.html#first-motivation-spatial-simulations",
    "title": "Introduction to slendr",
    "section": "First motivation: spatial simulations!",
    "text": "First motivation: spatial simulations!",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#a-broader-motivation-for-slendr",
    "href": "handouts_slendr.html#a-broader-motivation-for-slendr",
    "title": "Introduction to slendr",
    "section": "A broader motivation for slendr",
    "text": "A broader motivation for slendr\n\nMost researchers are not expert programmers\nAll but the most trivial simulations require lots of code\n\n\n\nYet, 90% [citation needed] of simulations are basically the same!\n\ncreate populations (splits and \\(N_e\\) changes)\nspecify admixture rates and admixture times\n\n… all this means duplication of code across many projects\n\n\n\n\nComputing statistics presents even more hurdles\n\n\n\n\n\nslendr makes this very easy, even for “complex models”",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#typical-slendr-workflow",
    "href": "handouts_slendr.html#typical-slendr-workflow",
    "title": "Introduction to slendr",
    "section": "Typical slendr workflow",
    "text": "Typical slendr workflow\nWe will always start our R scripts with this:\n\nlibrary(slendr) # You can safely ignore any potential warnings!\ninit_env()      # This activates the internal Python environmet\n\nFollowed by some combination of the following:\n\ncreating populations\nprogramming \\(N_e\\) size changes\nencoding gene-flow events\nsimulating genomic data\ncomputing popgen statistics",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#creating-populations",
    "href": "handouts_slendr.html#creating-populations",
    "title": "Introduction to slendr",
    "section": "Creating populations",
    "text": "Creating populations\nAt minimum, we need its name, size and “time of appearance”:\n\npop1 &lt;- population(\"pop1\", N = 1000, time = 1)\n\n\nThis creates a normal R object! Typing it out gives a summary:\n\npop1\n\nslendr 'population' object \n-------------------------- \nname: pop1 \nnon-spatial population\nstays until the end of the simulation\n\npopulation history overview:\n  - time 1: created as an ancestral population (N = 1000)\n\n\n\n\n\nNote: Because slendr uses either msprime or SLiM internally for simulation of genomic data, all individuals are assumed to be diploid.",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#programming-population-splits",
    "href": "handouts_slendr.html#programming-population-splits",
    "title": "Introduction to slendr",
    "section": "Programming population splits",
    "text": "Programming population splits\nSplits are defined by providing a parent = &lt;pop&gt; argument:\n\npop2 &lt;- population(\"pop2\", N = 100, time = 50, parent = pop1)\n\n\n\nNote: Here pop1 is an R object created above, not a string \"pop1\"!\n\nThe split is again reported in the “historical summary”:\n\npop2\n\nslendr 'population' object \n-------------------------- \nname: pop2 \nnon-spatial population\nstays until the end of the simulation\n\npopulation history overview:\n  - time 50: split from pop1 (N = 100)",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#scheduling-resize-events",
    "href": "handouts_slendr.html#scheduling-resize-events",
    "title": "Introduction to slendr",
    "section": "Scheduling resize events",
    "text": "Scheduling resize events\n\nStep size decrease:\n\n\npop1 &lt;- population(\"pop1\", N = 1000, time = 1)\npop1_step &lt;- resize(pop1, N = 100, time = 500, how = \"step\")\n\n\n\nExponential increase:\n\n\npop2 &lt;- population(\"pop2\", N = 100, time = 50, parent = pop1)\npop2_exp &lt;- resize(pop2, N = 10000, time = 500, end = 2000, how = \"exponential\")",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#tidyverse-style-pipe-interface",
    "href": "handouts_slendr.html#tidyverse-style-pipe-interface",
    "title": "Introduction to slendr",
    "section": "Tidyverse-style pipe %>% interface",
    "text": "Tidyverse-style pipe %&gt;% interface\nThe following leads to a more concise (and “elegant”) code.\n\nStep size decrease:\n\n\npop1 &lt;-\n  population(\"pop1\", N = 1000, time = 1) %&gt;%\n  resize(N = 100, time = 500, how = \"step\")\n\n\nExponential increase:\n\n\npop2 &lt;-\n  population(\"pop2\", N = 1000, time = 1) %&gt;%\n  resize(N = 10000, time = 500, end = 2000, how = \"exponential\")\n\n\n\nNote: You can read (and understand) a() %&gt;% b() %&gt;% c() as “take the result of the function a, pipe it into function b, and then pipe that to function c”.",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#a-more-complex-model",
    "href": "handouts_slendr.html#a-more-complex-model",
    "title": "Introduction to slendr",
    "section": "A more complex model",
    "text": "A more complex model\nUsing just the two functions introduced so far:\n\npop1 &lt;- population(\"pop1\", N = 1000, time = 1)\n\npop2 &lt;-\n  population(\"pop2\", N = 1000, time = 300, parent = pop1) %&gt;%\n  resize(N = 100, how = \"step\", time = 1000)\n\npop3 &lt;-\n  population(\"pop3\", N = 1000, time = 400, parent = pop2) %&gt;%\n  resize(N = 2500, how = \"step\", time = 800)\n\npop4 &lt;-\n  population(\"pop4\", N = 1500, time = 500, parent = pop3) %&gt;%\n  resize(N = 700, how = \"exponential\", time = 1200, end = 2000)\n\npop5 &lt;-\n  population(\"pop5\", N = 100, time = 600, parent = pop4) %&gt;%\n  resize(N = 50, how = \"step\", time = 900) %&gt;%\n  resize(N = 1000, how = \"exponential\", time = 1600, end = 2200)",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#again-each-object-carries-its-history",
    "href": "handouts_slendr.html#again-each-object-carries-its-history",
    "title": "Introduction to slendr",
    "section": "Again, each object carries its history!",
    "text": "Again, each object carries its history!\nFor instance, this is the summary you will get from the last population from the previous code chunk:\n\npop5\n\nslendr 'population' object \n-------------------------- \nname: pop5 \nnon-spatial population\nstays until the end of the simulation\n\npopulation history overview:\n  - time 600: split from pop4 (N = 100)\n  - time 900: resize from 100 to 50 individuals\n  - time 1600-2200: exponential resize from 50 to 1000 individuals\n\n\n\nThis way, you can build up complex models step by step, checking things as you go by interacting with the R console.",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#gene-flow-admixture",
    "href": "handouts_slendr.html#gene-flow-admixture",
    "title": "Introduction to slendr",
    "section": "Gene flow / admixture",
    "text": "Gene flow / admixture\nWe can schedule gene flow from pop1 into pop2 with:\n\ngf &lt;- gene_flow(from = pop1, to = pop2, start = 2000, end = 2200, rate = 0.13)\n\n\n\nNote: Here rate = 0.13 means 13% migrants over the given time window will come from “pop1” into “pop2”.\n\n\nMultiple gene-flow events can be gathered in a list:\n\ngf &lt;- list(\n  gene_flow(from = pop1, to = pop2, start = 500, end = 600, rate = 0.13),\n  gene_flow(from = ..., to = ..., start = ..., end = ..., rate = ...),\n  ...\n)",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#model-compilation",
    "href": "handouts_slendr.html#model-compilation",
    "title": "Introduction to slendr",
    "section": "Model compilation",
    "text": "Model compilation\n\nThis is the final step before we can simulate data.\n\n\nmodel &lt;- compile_model(\n  populations = list(pop1, pop2, pop3, pop4, pop5),\n  generation_time = 1,       # (converts the all times into generations)\n  simulation_length = 3000,  # (number of generations to run the simulation for)\n  direction = \"forward\"      # (not totally necessary but good practice)\n)\n\n\n\ncompile_model() takes a list of components, performs some consistency checks, and returns a single R object",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#model-compilation-1",
    "href": "handouts_slendr.html#model-compilation-1",
    "title": "Introduction to slendr",
    "section": "Model compilation",
    "text": "Model compilation\n\nThis is the final step before we can simulate data.\n\n\nmodel &lt;- compile_model(\n  populations = list(pop1, pop2, pop3, pop4, pop5),\n  gene_flow = gf,      # &lt;----- in case our model includes gene flow(s)\n  generation_time = 1,\n  simulation_length = 3000,\n  direction = \"forward\"\n)\n\n\n\nGene flow(s) can be included via the gene_flow argument.",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#model-summary",
    "href": "handouts_slendr.html#model-summary",
    "title": "Introduction to slendr",
    "section": "Model summary",
    "text": "Model summary\nTyping the compiled model into R prints a brief summary:\n\nmodel\n\nslendr 'model' object \n--------------------- \npopulations: pop1, pop2, pop3, pop4, pop5 \ngeneflow events: 1 \ngeneration time: 1 \ntime direction: forward \ntime units: generations \ntotal running length: 3000 time units\nmodel type: non-spatial\n\nconfiguration files in: /private/var/folders/h2/qs0z_44x2vn2sskqc0cct7540000gn/T/RtmpZ576Uk/filecdfb6e69f421_slendr_model \n\n\nThis can be useful as a quick overview of the model we are working with. However, a better way to check a model is…",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#model-visualization",
    "href": "handouts_slendr.html#model-visualization",
    "title": "Introduction to slendr",
    "section": "Model visualization",
    "text": "Model visualization\n\nplot_model(model)",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#forward-time-units",
    "href": "handouts_slendr.html#forward-time-units",
    "title": "Introduction to slendr",
    "section": "“Forward time units”",
    "text": "“Forward time units”\n\npop1 &lt;- population(\"pop1\", N = 1000, time = 1)\n\npop2 &lt;-\n  population(\"pop2\", N = 1000, time = 300, parent = pop1) %&gt;%\n  resize(N = 100, how = \"step\", time = 1000)\n\npop3 &lt;-\n  population(\"pop3\", N = 1000, time = 400, parent = pop2) %&gt;%\n  resize(N = 2500, how = \"step\", time = 800)\n\npop4 &lt;-\n  population(\"pop4\", N = 1500, time = 500, parent = pop3) %&gt;%\n  resize(N = 700, how = \"exponential\", time = 1200, end = 2000)\n\npop5 &lt;-\n  population(\"pop5\", N = 100, time = 600, parent = pop4) %&gt;%\n  resize(N = 50, how = \"step\", time = 900) %&gt;%\n  resize(N = 1000, how = \"exponential\", time = 1600, end = 2200)\n\nmodel &lt;- compile_model(\n  populations = list(pop1, pop2, pop3, pop4, pop5),\n  generation_time = 1,\n  simulation_length = 3000, # forward-time sims need an explicit end\n  direction = \"forward\"\n)\n\n\n\nWe started with pop1 in generation 1, with later events at an increasing time value.",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#forward-time-units-1",
    "href": "handouts_slendr.html#forward-time-units-1",
    "title": "Introduction to slendr",
    "section": "“Forward time units”",
    "text": "“Forward time units”\n\nplot_model(model) # see time progressing from generation 1 forwards\n\n\n\n\n\n\n\n\n\n\nWe started with pop1 in generation 1, with later events at an increasing time value.",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#backward-time-units",
    "href": "handouts_slendr.html#backward-time-units",
    "title": "Introduction to slendr",
    "section": "“Backward time units”",
    "text": "“Backward time units”\n\npop1 &lt;- population(\"pop1\", N = 1000, time = 30000)\n\npop2 &lt;-\n  population(\"pop2\", N = 1000, time = 27000, parent = pop1) %&gt;%\n  resize(N = 100, how = \"step\", time = 20000)\n\npop3 &lt;-\n  population(\"pop3\", N = 1000, time = 26000, parent = pop2) %&gt;%\n  resize(N = 2500, how = \"step\", time = 22000)\n\npop4 &lt;-\n  population(\"pop4\", N = 1500, time = 25000, parent = pop3) %&gt;%\n  resize(N = 700, how = \"exponential\", time = 18000, end = 10000)\n\npop5 &lt;-\n  population(\"pop5\", N = 100, time = 24000, parent = pop4) %&gt;%\n  resize(N = 50, how = \"step\", time = 21000) %&gt;%\n  resize(N = 1000, how = \"exponential\", time = 14000, end = 8000)\n\nmodel &lt;- compile_model(\n  populations = list(pop1, pop2, pop3, pop4, pop5),\n  generation_time = 10 # (10 time units for each generation)\n  # (we don't need to provide `simulation_length =` because\n  # \"backwards\" models end at time 0 by default, i.e. \"present-day\")\n)\n\n\n\nSame model as before, except now expressed in units of “years before present”.",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#backward-time-units-1",
    "href": "handouts_slendr.html#backward-time-units-1",
    "title": "Introduction to slendr",
    "section": "“Backward time units”",
    "text": "“Backward time units”\n\nplot_model(model) # see time progressing from \"year\" 30000 backwards\n\n\n\n\n\n\n\n\n\n\nSame model as before, except now expressed in units of “years before present”.",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#built-in-simulation-engines",
    "href": "handouts_slendr.html#built-in-simulation-engines",
    "title": "Introduction to slendr",
    "section": "Built-in simulation “engines”",
    "text": "Built-in simulation “engines”\nslendr has two simulation “engine scripts” built-in:\n\nmsprime engine (slendr source) – R function msprime()\nSLiM engine (slendr source) – R function slim()\n\n\nThey are designed to “understand” slendr models, meaning that you can simulate data just with this command:\n\nts &lt;- msprime(model, sequence_length = 10e6, recombination_rate = 1e-8)\n\n\n\n\n\n\nNo need to write any msprime or SLiM code!",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#what-is-tree-sequence",
    "href": "handouts_slendr.html#what-is-tree-sequence",
    "title": "Introduction to slendr",
    "section": "What is tree sequence?",
    "text": "What is tree sequence?\n\n\n\n\n\n\na record of full genetic ancestry of a set of samples\nan encoding of DNA sequence carried by those samples\nan efficient analysis framework",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#what-we-usually-have",
    "href": "handouts_slendr.html#what-we-usually-have",
    "title": "Introduction to slendr",
    "section": "What we usually have",
    "text": "What we usually have",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#what-we-usually-want",
    "href": "handouts_slendr.html#what-we-usually-want",
    "title": "Introduction to slendr",
    "section": "What we usually want",
    "text": "What we usually want\nAn understanding of our samples’ evolutionary history:\n\n\n\n\n\n\nThis is exactly what a tree sequence is!\n\n\n\n\n\nImage from the tskit documentation",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#the-magic-of-tree-sequences",
    "href": "handouts_slendr.html#the-magic-of-tree-sequences",
    "title": "Introduction to slendr",
    "section": "The magic of tree sequences",
    "text": "The magic of tree sequences\nThey allow us to compute statistics without genotypes!\n\n\n\nThere is a “duality” between mutations and branch lengths.\n\n\nNote: See an amazing paper by Ralph et al. (2020) for more detail.",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#what-if-we-need-mutations-though",
    "href": "handouts_slendr.html#what-if-we-need-mutations-though",
    "title": "Introduction to slendr",
    "section": "What if we need mutations though?",
    "text": "What if we need mutations though?",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#what-if-we-need-mutations-though-1",
    "href": "handouts_slendr.html#what-if-we-need-mutations-though-1",
    "title": "Introduction to slendr",
    "section": "What if we need mutations though?",
    "text": "What if we need mutations though?\nCoalescent and mutation processes can be decoupled!\n\n\n\n\nThis means we can add mutations to ts after the simulation using ts_mutate().",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#lets-go-back-to-our-example-model",
    "href": "handouts_slendr.html#lets-go-back-to-our-example-model",
    "title": "Introduction to slendr",
    "section": "Let’s go back to our example model…",
    "text": "Let’s go back to our example model…\n\nplot_model(model)",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#simulate-a-tree-sequence",
    "href": "handouts_slendr.html#simulate-a-tree-sequence",
    "title": "Introduction to slendr",
    "section": "… simulate a tree sequence…",
    "text": "… simulate a tree sequence…\n\nIn our script we’ll have something like this:\n\nlibrary(slendr)\ninit_env()\n\n# &lt;... population() definitions ...&gt;\n\n# &lt;... gene_flow() definition ...&gt;\n\n# &lt;... compile_model(...) ...&gt;\n  \nts &lt;-\n  msprime(model, sequence_length = 50e6, recombination_rate = 1e-8)",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#and-overlay-mutations-on-it",
    "href": "handouts_slendr.html#and-overlay-mutations-on-it",
    "title": "Introduction to slendr",
    "section": "… and overlay mutations on it",
    "text": "… and overlay mutations on it\n\nIn our script we’ll have something like this:\n\nlibrary(slendr)\ninit_env()\n\n# &lt;... population() definitions ...&gt;\n\n# &lt;... gene_flow() definition ...&gt;\n\n# &lt;... compile_model(...) ...&gt;\n  \nts &lt;-\n  msprime(model, sequence_length = 50e6, recombination_rate = 1e-8) %&gt;%\n  ts_mutate(mutation_rate = 1e-8)\n\n\n\nNote: In some exercises, mutations won’t be necessary. Where we will need them, you can use ts_mutate() using the pattern shown here.",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#slendrs-r-interface-to-tskit-statistics",
    "href": "handouts_slendr.html#slendrs-r-interface-to-tskit-statistics",
    "title": "Introduction to slendr",
    "section": "slendr’s R interface to tskit statistics",
    "text": "slendr’s R interface to tskit statistics\n\n\n\nAllele-frequecy spectrum, \\(\\pi\\), \\(f\\)-statistics, \\(F_{ST}\\), Tajima’s D, etc.\nFind help at slendr.net/reference or in R under ?ts_fst etc.",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#extracting-names-of-recorded-samples",
    "href": "handouts_slendr.html#extracting-names-of-recorded-samples",
    "title": "Introduction to slendr",
    "section": "Extracting names of recorded samples",
    "text": "Extracting names of recorded samples\n\nWe can get individuals recorded in ts with ts_samples():\n\n\nts_samples(ts) %&gt;% head(1) # returns a data frame (one row here, for brevity)\n\n\n\n    name time  pop\n1 pop1_1 3001 pop1\n\n\n\n\nA shortcut ts_names() can also be useful:\n\n\nts_names(ts) %&gt;% head(5) # returns a vector of individuals' names\n\n[1] \"pop1_1\" \"pop1_2\" \"pop1_3\" \"pop1_4\" \"pop1_5\"\n\n\n\n\n\nWe can get a per-population list of individuals like this:\n\n\nts_names(ts, split = \"pop\") # returns a named list of such vectors\n\n\n\n$pop1\n[1] \"pop1_969\" \"pop1_803\" \"pop1_611\" \"pop1_427\" \"pop1_468\"",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#tskit-computation-option-1",
    "href": "handouts_slendr.html#tskit-computation-option-1",
    "title": "Introduction to slendr",
    "section": "tskit computation – option #1",
    "text": "tskit computation – option #1\nFor a function which operates on one set of individuals, we can first get a vector of names to compute on like this:\n\n# a random selection of names of three individuals in a tree sequence\nsamples &lt;- c(\"popX_1\", \"popX_2\", \"popY_42\")\n\n\n\nThen we can calculate the statistic of interest like this:\n\n# this computes nucleotide diversity in our set of individuals\ndf_result &lt;- ts_diversity(ts, sample_sets = list(samples))\n\n\n\n\nNote: Wherever you see list(&lt;vector of names&gt;), you can think of it as “compute a statistic for the entire group of individuals” (you get a single number). Without the list(), it would mean “compute the statistic for each individual separately” (and get a value for each of them individually).",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#tskit-computation-option-2",
    "href": "handouts_slendr.html#tskit-computation-option-2",
    "title": "Introduction to slendr",
    "section": "tskit computation – option #2",
    "text": "tskit computation – option #2\nFor a function operating on multiple sets of individuals, we want a list of vectors of names (one such vector per group):\n\n# when we compute on multiple groups, it's a good idea to name them\nsamples &lt;- list(\n  popX = c(\"popX_1\", \"popX_2\", \"popX_3\"),\n  popY = c(\"popY_1\", \"popY_2\", \"popY_3\"),\n  popZ = c(\"popZ_1\", \"popZ_2\")\n)\n\n\nThen we use this list of vectors in the same way as before:\n\n# this computes a pairwise divergence between all three groups\ndf_result &lt;- ts_divergence(ts, sample_sets = samples)",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#tskit-computation-option-3",
    "href": "handouts_slendr.html#tskit-computation-option-3",
    "title": "Introduction to slendr",
    "section": "tskit computation – option #3",
    "text": "tskit computation – option #3\nFor something like \\(f\\) statistics, the function arguments must be more precisely specified (here A, B, C, not sample_sets):\n\ndf_result &lt;- ts_f3(\n  ts,\n  A = c(\"popX_1\", \"popX_2\", \"popX_3\"),\n  B = c(\"popY_1\", \"popY_2\", \"popY_3\"),\n  C = c(\"popZ_1\", \"popZ_2\")\n)\n\n\nDoing this manually can be annoying — ts_names() helps by preparing the list of names in the correct format:\n\n# get names of individuals in each population as a named list of vectors\nsamples &lt;- ts_names(ts, split = \"pop\")\n\n# use this list directly by specifying which vectors to take out\nts_f3(ts, A = samples$popX, B = samples$popY, C = samples$popZ)",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#example-nucleotide-diversity",
    "href": "handouts_slendr.html#example-nucleotide-diversity",
    "title": "Introduction to slendr",
    "section": "Example: nucleotide diversity",
    "text": "Example: nucleotide diversity\n\n\nGet a list of individuals in each population:\n\nsamples &lt;- ts_names(ts, split = \"pop\")\n\nnames(samples)\n\n[1] \"pop1\" \"pop2\"\n\n\n\n\nWe can index into the list via population name:\n\nsamples$pop1\n\n\n\n[1] \"pop1_1\" \"pop1_2\" \"pop1_3\"\n\n\n\nsamples$pop2\n\n\n\n[1] \"pop2_1\" \"pop2_2\" \"pop2_3\"\n\n\n\n\n \n\n\nCompute nucleotide diversity (note the list samples):\n\nts_diversity(ts, sample_sets = samples)\n\n# A tibble: 2 × 2\n  set   diversity\n  &lt;chr&gt;     &lt;dbl&gt;\n1 pop1  0.000400 \n2 pop2  0.0000654\n\n\n\nOur tree sequence had two populations, pop1 and pop2, which is why we get a data frame with diversity in each of them.",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#example-allele-frequency-spectrum",
    "href": "handouts_slendr.html#example-allele-frequency-spectrum",
    "title": "Introduction to slendr",
    "section": "Example: allele frequency spectrum",
    "text": "Example: allele frequency spectrum\n\n\nGet names of individuals:\n\nsamples &lt;- ts_names(ts)[1:5]\nsamples\n\n[1] \"pop_1\" \"pop_2\" \"pop_3\" \"pop_4\" \"pop_5\"\n\n\n\nCompute the AFS:\n\nafs &lt;- ts_afs(ts, sample_sets = list(samples))\n\n# we skip the 1st item because it has a special meaning in tskit\nafs[-1]\n\n [1] 3917 2151 1432  941  740  624  607  587  416  385\n\n\n\n\n \n\n\n\nplot(afs[-1], type = \"b\",\n     xlab = \"allele count bin\",\n     ylab = \"frequency\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: One of the rare examples when a slendr / tskit statistical function does not return a data frame (ts_afs() returns a numerical vector, not a data frame).",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#tree-sequence-tables",
    "href": "handouts_slendr.html#tree-sequence-tables",
    "title": "Introduction to slendr",
    "section": "Tree sequence tables",
    "text": "Tree sequence tables\n\nThis simulates 2 \\(\\times\\) 10000 chromosomes of 100 Mb:\n\nlibrary(slendr)\ninit_env(quiet = FALSE)\n\npop &lt;- population(\"pop\", time = 100e6, N = 10000)\nmodel &lt;- compile_model(pop, generation_time = 30, direction = \"backward\")\nts &lt;- msprime(model, sequence_length = 1e6, recombination_rate = 1e-8)\n\n\n\nThe interface to all required Python modules has been activated.\n\n\n\nRuns in less than 30 seconds on my laptop!\nTakes only about 66 Mb of memory!",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#how-is-this-even-possible",
    "href": "handouts_slendr.html#how-is-this-even-possible",
    "title": "Introduction to slendr",
    "section": "How is this even possible?!",
    "text": "How is this even possible?!",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#tree-sequence-tables-1",
    "href": "handouts_slendr.html#tree-sequence-tables-1",
    "title": "Introduction to slendr",
    "section": "Tree-sequence tables",
    "text": "Tree-sequence tables\n\n\n\nA tree can be represented by\n\n\na table of nodes,\na table of edges between nodes,\na table of mutations on edges\n\n\n\n\n\n\n\n\n\n\n\nA collection of such tables is a tree sequence.\n\n\n\nNote: This is a huge oversimplification. Find more information in tskit docs.",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#tree-sequence-tables-in-practice",
    "href": "handouts_slendr.html#tree-sequence-tables-in-practice",
    "title": "Introduction to slendr",
    "section": "Tree-sequence tables in practice",
    "text": "Tree-sequence tables in practice\n\n\n\n\n\n\n\n\n\n\n\n\nNodes:\n\n\n  node     time\n1  360 871895.1\n2  256 475982.3\n3  255 471179.5\n\n\n\n\n\nEdges:\n\n\n  child parent\n1   256    360\n2   255    256\n3    69    256\n\n\n\n\n\nMutations:\n\n\n   id node     time\n1  69   74 125539.4\n2 272   22 242337.9\n3 277   22 129474.1",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "handouts_slendr.html#standard-genotype-formats",
    "href": "handouts_slendr.html#standard-genotype-formats",
    "title": "Introduction to slendr",
    "section": "Standard genotype formats",
    "text": "Standard genotype formats\nIf a tree sequence doesn’t cut it, you can always…\n\n\nexport genotypes to a VCF file:\n\n\nts_vcf(ts, path = \"path/to/a/file.vcf.gz\")\n\n\n\n\nexport genotypes in the EIGENSTRAT format:\n\n\nts_eigenstrat(ts, prefix = \"path/to/eigenstrat/prefix\")\n\n\n\n\naccess genotypes as a data frame:\n\n\nts_genotypes(ts)\n\n\n\n    pos pop_1_chr1 pop_1_chr2 pop_2_chr1 pop_2_chr2 pop_3_chr1 pop_3_chr2\n1 12977          0          1          0          0          0          0\n2 15467          1          0          1          1          1          1",
    "crumbs": [
      "Cheatsheets and handouts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to _slendr_</span>"
    ]
  },
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "Big parts of the first draft of this course have been developed primarily as part of a collaboration between University of Tartu and University of Copenhagen (Twinning project ECHO: Expanding Concept and Methodology for Human Past Studies in the Eastern Baltic Region).\nHuge thanks go to my students and mentees for showing me how focusing on the essentials and fundamentals (rather than perfection and theory) is the most important thing in pushing research project forward.",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "setup-tidyverse.html",
    "href": "setup-tidyverse.html",
    "title": "Appendix A — Installation – tidyverse",
    "section": "",
    "text": "Installation steps\nThe chapters of the workbook focused on data science rely exclusively on the ecosystem of R packages called tidyverse (plus a set of functionality available in any basic R installation without any additional setup needed). All of these packages are so mainstream that it’s completely trivial to install them on practically any machine and any operating system.\nPlease don’t hesitate to get in touch with me over email at contact [snail sign] bodkan.net if you run into any problems with this setup procedure or with anything else related to the workshop! You can also catch me whenever you see me in the teaching venue.\ninstall.packages(c(\"dplyr\", \"ggplot2\", \"readr\", \"forcats\", \"cowplot\"))\ninstall.packages(c(\"rnaturalearth\", \"sf\"))\nThis installs two very useful R packages for working with geospatial data. Depending on how much time we have, there will be a session on the basics of visualization of spatial information using R. These two packages are all that are necessary for that.\nIf you’ve made it this far, you’re good to go!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Installation -- _tidyverse_</span>"
    ]
  },
  {
    "objectID": "setup-tidyverse.html#installation-steps",
    "href": "setup-tidyverse.html#installation-steps",
    "title": "Appendix A — Installation – tidyverse",
    "section": "",
    "text": "Note: It will be great if you do this setup on your personal laptop. Getting everything to run locally is often much easier than trying to install things on shared clusters or other HPC environments.\n\nI assume you already have R and RStudio installed on your computer (any version will do).\nOpen RStudio and copy this into the R console to install the tidyverse packages that we will actually be using in the workshop:\n\n\n\nThen, in the same R console, run the following installation command:\n\n\n\n\n\nNote: If you get an error during the installation, please copy-and-paste the entire output from this command and send it to me via email. Or just talk to me in person! The point 2. should work without any issues, but point 3. might require a bit more setup, particularly on Linux or Mac. Don’t worry if things don’t work 100%! Just reach out to me.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Installation -- _tidyverse_</span>"
    ]
  },
  {
    "objectID": "setup-slendr.html",
    "href": "setup-slendr.html",
    "title": "Appendix B — Installation – slendr",
    "section": "",
    "text": "Installation steps\nThe chapters of the workbook focused on population genomics and population genomic simulations rely exclusive on the R package called slendr (plus an assortment of other useful R packages for data science, all of which will be automatically installed alongside slendr).\nPlease don’t hesitate to get in touch with me over email at contact [snail sign] bodkan.net if you run into any problems with this setup procedure or with anything else related to the workshop! You can also catch me whenever you see me in the teaching venue.\ninstall.packages(\"slendr\")\nSys.setenv(CONDA_PLUGINS_AUTO_ACCEPT_TOS = \"yes\")\n\nslendr::setup_env(agree = TRUE)\nIf everything worked, you should get an optimistic message at the end of the entire procedure saying:\nlibrary(slendr)\ninit_env()\n## The interface to all required Python modules has been activated.\n\nA &lt;- population(\"popA\", time = 8000, N = 1000)\nB &lt;- population(\"popB\", time = 4000, N = 1000, parent = A)\nC &lt;- population(\"popC\", time = 4000, N = 1000, parent = A)\nD &lt;- population(\"popD\", time = 3000, N = 1000, parent = C)\n\ngf &lt;- list(\n  gene_flow(from = B, to = D, start = 1000, end = 800, rate = 0.2),\n  gene_flow(from = C, to = D, start = 1000, end = 800, rate = 0.1)\n)\n\nmodel &lt;- compile_model(\n  populations = list(A, B, C, D),\n  gene_flow = gf, generation_time = 30\n)\n\nts &lt;- msprime(model, sequence_length = 1e6, recombination_rate = 1e-8)\nts\n## ╔═══════════════════════════╗\n## ║TreeSequence               ║\n## ╠═══════════════╤═══════════╣\n## ║Trees          │        867║\n## ╟───────────────┼───────────╢\n## ║Sequence Length│  1,000,000║\n## ╟───────────────┼───────────╢\n## ║Time Units     │generations║\n## ╟───────────────┼───────────╢\n## ║Sample Nodes   │      8,000║\n## ╟───────────────┼───────────╢\n## ║Total Size     │    1.4 MiB║\n## ╚═══════════════╧═══════════╝\n## ╔═══════════╤══════╤═════════╤════════════╗\n## ║Table      │Rows  │Size     │Has Metadata║\n## ╠═══════════╪══════╪═════════╪════════════╣\n## ║Edges      │20,031│626.0 KiB│          No║\n## ╟───────────┼──────┼─────────┼────────────╢\n## ║Individuals│ 4,000│109.4 KiB│          No║\n## ╟───────────┼──────┼─────────┼────────────╢\n## ║Migrations │     0│  8 Bytes│          No║\n## ╟───────────┼──────┼─────────┼────────────╢\n## ║Mutations  │     0│ 16 Bytes│          No║\n## ╟───────────┼──────┼─────────┼────────────╢\n## ║Nodes      │17,442│476.9 KiB│          No║\n## ╟───────────┼──────┼─────────┼────────────╢\n## ║Populations│     4│343 Bytes│         Yes║\n## ╟───────────┼──────┼─────────┼────────────╢\n## ║Provenances│     1│  2.8 KiB│          No║\n## ╟───────────┼──────┼─────────┼────────────╢\n## ║Sites      │     0│ 16 Bytes│          No║\n## ╚═══════════╧══════╧═════════╧════════════╝\nThe outcome of this script should approximately match what you see above (minus some statistical noise manifesting in your numbers in the summary being slightly different).\nIn order to be able to run my own solutions to some of the exercises (particularly some more advanced bonus exercises which will be entirely optional, and we very likely won’t have time to go through during the short span of the worksop), a couple of additional R packages might be useful.\nYou can install those like this:\ninstall.packages(c(\"combinat\", \"cowplot\"))\nIf you’ve made it this far, you’re good to go!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Installation -- _slendr_</span>"
    ]
  },
  {
    "objectID": "setup-slendr.html#installation-steps",
    "href": "setup-slendr.html#installation-steps",
    "title": "Appendix B — Installation – slendr",
    "section": "",
    "text": "Note: It will be great if you do this setup on your personal laptop. Getting everything to run locally is often much easier than trying to install things on shared clusters or other HPC environments.\n\nI assume you already have R and RStudio installed on your computer (any version will do).\nOpen RStudio and copy this into the R console to install the slendr package:\n\n\n\n\nNote: If you get an error during the installation, please copy-and-paste the entire output from this command and send it to me via email.\n\nAfter you have slendr successfuly installed, create a dedicated Python environment which it internally uses for simulation and tree-sequence analysis by typing this into the R console again:\n\n\n\n======================================================================\nPython environment for slendr has been successfuly created, and the R\ninterface to msprime, tskit, and pyslim modules has been activated.\n\nIn future sessions, activate this environment by calling init_env().\n=======================================================================\n\n\n\n\n\n\nClick here if running setup_env() fails\n\n\n\n\n\nIf the setup_env() installation procedure above fails, try the following:\n\nDelete the failed installation attempt:\n\n\nslendr::clear_env(force = TRUE, all = TRUE)\n\n\nRestart R by clicking (in RStudio) on Session -&gt; Restart R.\nTry installing it again, this time using pip as a Python installation method (the default is conda which unfortunately fails fairly often):\n\n\nslendr::setup_env(agree = TRUE, pip = TRUE)\n\n\n\n\n\n\nNote: If you still get an error during the installation, please copy-and-paste the entire output from this command and send it to me via email. Or just grab me whenever you see me in person!\n\nPaste this little testing slendr simulation script into the R console to make sure that everything works correctly. Don’t read too much into the meaning of the code, understanding (and being able to program) all this and more will be the point of our workshop.\n\n\n\nNote: If you managed to successfully run the installation steps 1-3 above, this simulation should finish running without any problems. If it does not work, then there’s something strange going on. Please copy-and-paste the entire output this script produces in your R console (including the error) and send it to me via email.\n\n\n\nInstalling a couple of bonus R packages",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Installation -- _slendr_</span>"
    ]
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Appendix C — Slides",
    "section": "",
    "text": "Here is a list of links to online version of slides we went through when introducing each chapter before moving to the exercises. This is provided mostly for completeness – when working on the exercises, the single-page versions of these slides (available under the Cheatsheets and handouts heading will be probably more useful.\n\nAbout this workshop – link\nR bootcamp – link\nIntroduction to tidyverse — link\nIntroduction to slendr – link",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Slides</span>"
    ]
  }
]