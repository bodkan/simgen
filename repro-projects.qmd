# Structuring R projects

## Introduction

You've done a huge amount of programming already. Processing data,
filtering data, annotating data, summarizing data, but also plotting data
using the most powerful tool for scientific visualization _ggplot2_! I
said it before but it's worth repeating again---having been introduced to
_tidyverse_ table manipulation and _ggplot2_ over the past couple of days,
you've learned 80% of tools you might need to do data science on a day-to-day
basis. Sure, you might not remember everything, but remembering comes from
repetition and practice. You've been exposed to the most important concepts,
you know where to come back for more information when needed later.

**In this session on reproducibility, we will take things a bit easier. Instead
of focusing on programming techniques and various functions, we'll go through
guidelines on how to organize (and run) computational projects on a practical
basis--taking _what_ you've learned, and learning _how_ to do it in practice.**

Rather than going through building _tidyverse_ and _ggplot2_ pipelines on
new data (again), you will be practicing reproducibility concepts in one of the
following two ways, **depending on the level of your confidence and comfort:**

1. **You can take already established bits of code (currently scattered
over multiple separate scripts and files or even nowhere because you
experimented in the R console -- all these approaches were OK so far!), or**

2. **you can take your own data, and your own scripts...**

**... and using code and data from 1. or 2., you will learn about the
possibilities to organize your computational projects in the best possible
way, both for yourself now, yourself in the future, or others who might
pick up your projects at a later point.**

**In the next session, we will explore two possibilities of how to _present_
your results in a 100% reproducible, reliable way, either in form of slides
(such as for group meeting updates) or reports (as your research notebooks).**

## Exercise 1: Creating a formal R project

1. Click `File` `->` `New project...`
2. In the new window, click on `New Directory` `->` `New Project`
3. Under `"Directory name"` type in `"simgen-project"` (the name of our course,
and the directory where all project files will be stored). Pick where you
would like to save that directory (this doesn't matter, just put the project
somewhere you can later find it :)). Check the `"Open in new session"`, leave
the other options related to "git" and "renv" unchecked.

4. Then click on the "Create Project" button.

**This will create a new RStudio window. Your original RStudio window
(where you worked so far is still open). You task now is to convert the
(probably disorganized) bits of code into a "proper reproducible R project".**

**Notice the new `simgen-project.Rproj` file that is currently the only file
in your project directory! We will come back to it soon.**


## Exercise 2: Seting up a project file structure

What makes for a good computational project structure? There are endless
possibilities but a **good guidelines are**:

1. **Be consistent** -- put files that "belong together" in the same place. For
example, an Excel table shouldn't be saved in a directory with scripts.

2. **Be predictable** -- even a person unfamiliar with your project should be
able to guess where is what just by looking at your folder. Remember, when
you publish your paper or your thesis, you will have to provide all your
data and scripts as supplementary materials, so others should be able to
understand all of them!

**Let's set up an example computational project structure for our IBD data,
our ancient DNA metadata, and our $f_4$-ratio Neanderthal estimates data, just
like you would do this for a real study.**

**Note:** This is all just an example! Again, as long as you follow the guidelines
numbered above, everything works!

---

**In the `Files` pane of RStudio, click on `"New Folder"` and create the
following directories in the "root of your project directory
`simgen-project/`":**

1. `code/`
2. `data/`, and within it create directories `raw/` and `processed/`
3. `figures/`
4. `reports/`

**Note:** You can do all this manually, or you can play around with doing it
using code with the incredibly useful function `?dir.create`! For instance, a
single command to do all of the above could be the following.
**Why is the `recursive = TRUE` needed sometimes? Look at help of `?dir.create`.**

```{r}
#| eval: false
dir.create("code/")
dir.create("data/raw", recursive = TRUE)
dir.create("data/processed", recursive = TRUE)
dir.create("figures/")
dir.create("reports/")
```

**When you hit the `"Refresh file listing"` circular arrow button in the top-right
of the `"Files"` pane, you should see all the directories you just created.**

## Exercise 3: Building an example reproducible pipeline

**Let's get something out of the way first.**
If there are a million possible ways how to properly structure a computational
projects (as mentioned above), there are infinite ways how a "reproducible
research pipeline" should be actually organized.

**Obviously, all research projects are different**, they focus on different kinds
of data (archaeological data, ancient DNA sequences, linguistic data, field
observation data, etc.), so naturally they will require different code which
will have to be structured in different ways.

**Still, there are some common workflows which practically every single computational
scientific research study does:**

1. **Data gathering** -- in our IBD and metadata examples, this involved downloading
data from the internet.

2. **Data processing** -- in our case, this involved filtering the data, processing
it to bin individuals based on their ages, joining IBD data with metadata, etc.

3. **Data analysis** -- this involves computing summary statistics, creating figures,
etc.

The dirty secret of many scientific research studies (especially in the "old days")
is that all of these steps are often clumped together in huge scripts, and
its very difficult (even for the author) to sometimes tell where is what.
This can be a big problem, especially if a bug needs to be fixed, a new step
of a processing pipeline needs to be added, etc.

**Let's demonstrate how you could organize your code in practice, and hopefully
you'll see how investing a bit more time and thinking into properly organizing
code in your research project makes your life a lot easier in the long run**
(and much easier for everyone who might pick up your project later too).

---

### 1. Data gathering

**Create a new R script `File` `->` `New File` `->` `R Script`. Paste the
following code to that script, and then save it as `01_download_data.R` in the
root of your `simgen-project` project directory.**

**Don't copy it without skimming through it! Do you recognize these commands from our
earlier exercises on _tidyverse_? That example code was a little messy and
random, because it was structured as an _ad hoc_ tutorial. What we're doing
here is showing how to organize computational research properly.**

**When you have your script, run it by calling
`source("01_download_data.R")` and observe what happens in your R console
(you can also press Cmd/Ctrl + Shift + S)! Then click through your `"Files"`
panel and look in `data/raw` -- do you see your files?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the contents of the script you should create

```{bash}
#| code-fold: true
#| echo: false
cat files/repro/01_download_data.R
```

:::

**Note:** Notice the `cat("some kind of message\n")` command. This is 
extremely useful for printing log information about a (potentially)
log running process. If you're confused about what it does, write this
into your R console: `cat("Hello friend!\n")`.

This is a first step towards reproducibility. Downloading of all data set
now happens in a dedicated script, which means:

1. We only have to run it once, and have all data available in `data/raw`
for later use.
2. If we have to include a new data set to be included, we just edit that
script `01_download_data.R` and run it again!

**This doesn't sound like much, but it's absolutely crucial. Automation
is the most important aspect of reproducible research.**

Now let's move to the next step!

---

### 2. Data processing

**Create a new R script `File` `->` `New File` `->` `R Script`. Add the
following code to that script, and then save it as `02_process_data.R` in the
root of your `simgen-project` project directory.**

**Again, don't just blindly copy it! Do you recognize these commands from our
earlier exercises on _tidyverse_? Try to connect these bits of code to what
we did earlier in the (admittedly a bit chaotic) tutorial session on _tidyverse_
and _ggplot2_.**

**When you have your script, close your RStudio.**

...

_Let's pretend that some time passed, you're done for the day and went home._

...

**Find the location of your `simgen-project` directory and double-click on
the R project file `simgen-project.Rproj`. You'll get your old session back!**

**Now run run the processing script by calling `source("02_process_data.R")`
and observe what happens in your R console (you can also press Cmd/Ctrl +
Shift + S)! Then click through your `"Files"` panel and look in `data/processed`
-- do you see your files?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the contents of the script you should create

```{bash}
#| code-fold: true
#| echo: false
cat files/repro/02_process_data.R
```

:::


---

### 3. Data analysis

Hopefully you're now starting to get an idea about what a well-organized,
reproducible pipeline means. It's about properly structuring the directory
where your project lives, and about partitioning your code into scripts
which represent logical components of your project -- from downloading
data (and saving it to a particular storage location), and processing it
(and again saving it to a proper storage location), and, finally, to answering
research questions based on this data.

**This is where you'll come in!**




