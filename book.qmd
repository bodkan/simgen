---
title: "Simulation-Based Population Genomics and Inference in R"
subtitle: "A course on the fundamentals of population genomics and statistical inference, with a strong focus on good practices of reproducible research"
author:
  - "Martin Petr"
  - "[CC BY SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)"
#date: "January 2025"
#date-format: "MMMM YYYY"
format:
  html:
    toc-title: Contents
    toc: true
    echo: true
    code-line-numbers: false
    fig-align: center
    self-contained: true
    callout-appearance: minimal
filters: 
 - collapse-callout.lua
---

<!-- TODO: Looks at these links to define a custom "Show the solution" callout -->

<!-- https://www.youtube.com/watch?v=DDQO_3R-q74 -->

<!-- https://examples.quarto.pub/collapse-callout/ -->


```{r}
#| echo: FALSE
RERUN <- TRUE
```






































::: callout-note
#### Setup on your own computer

**If you want to set up everything on your local machine from scratch
(maybe after the course is over and you want to return to the material on
yor own), just follow the couple of steps right below.**

1. **Clone the repository with the activity materials**. In a shell terminal on
Linux or macOS, in your home directory (or anywhere else, really) you can run:

    ```         
    $ git clone https://github.com/bodkan/simgen ~/simgen
    ```

(For Windows users: You can download a zip file with the whole course
even without git.)

2. **Open RStudio and install all of the following dependencies:**

    ```         
    > install.packages(c("slendr", "combinat", "cowplot", "dplyr", "readr",
                         "tidyr", "ggplot2", "rmarkdown", "yaml"))
    ```

3. **Set up the Python environment used by the _slendr_ R package** for simulation and tree-sequence analysis (still in the R console!):

    ```         
    > slendr::setup_env(agree = TRUE)
    ```

    If everything worked, you should get an optimistic message saying:

    ```         
    ======================================================================
    Python environment for slendr has been successfuly created, and the R
    interface to msprime, tskit, and pyslim modules has been activated.

    In future sessions, activate this environment by calling init_env().
    =======================================================================
    ```

4. **Open your RStudio (unless you did the above steps already in RStudio)**, and you're good to go!

---

**If the `setup_env()` installation procedure fails, try the following:**

1. Delete the failed installation attempt:

```         
slendr::clear_env(force = TRUE, all = TRUE)
```

2. Try installing it again, this time using `pip` as a Python installation method (the default is `conda` which unfortunately fails fairly often):

```         
slendr::setup_env(agree = TRUE, pip = TRUE)
```

In every previous installments of this workshop, this is all that was needed to resolve problems.*

---

**Installing SLiM**

It's unclear whether we will manage to go through the entirety of the
selection-based exercise. However, to be able to do this, having SLiM at
least version 4.2 (and it being available in your unix `$PATH!`) is required. If this isn't possible for your, don't worry. You'll be able to do most of
that exercise even without SLiM by using cached results of the SLiM
simulation that I provided.

:::





























# Organization of the exercises

For each exercise, you will get a brief explanation of the problem at hand and
some information about functions that could be useful to solve the exercise.
**The concrete, specific task will be always written like this in bold. As you
work on each part of each exercises, look for these bold lines.**

**Your goal for each exercise is to write a complete R script script (in
RStudio `File -> New file -> R script`). I suggest
you save the script for each exercise as `exercise1.R`, `exercise2.R`, etc.,
just to keep things tidy and easy to troubleshoot if needed.**

**Each exercise is composed of individual _parts_, which are designed to build
one upon the other in the order they are specified. As you progress through
sequential parts of each exercise, you will be adding code to your script for
that exercise.**

#### Don't worry about the number of exercises you're _"supposed to do"_

How far we manage to crunch through the content of this session will depend on
everybody's level of confidence in programming _and_ population genetics.
The contents of the activities as a whole are designed so that no matter
the moment at which we run out of time, you will take home enough knowledge of
_slendr_ to be able to immediately apply it to your own projects.

**To avoid anyone getting overwhelmed, I'll be stopping the activity at
some "checkpoints", to check in with how everything is going and quickly go
through the example solutions for the parts until that point, and give a quick
explanation.**

If you find yourself way ahead, there are bonus exercises (which I will not be
going through with everyone) -- those should provide at entertainment and
a bit more challenge for you while the rest of us are catching up. If you're
_really_ far ahead, just continue to the next exercise without waiting for
the rest of us.

#### Note on the programming aspect of the exercises

All the exercises will involve "real coding"! If you've never really programmed
entire scripts before, this could feel a little intimidating. Don't worry.
If you're ever lost, just take a peek at the solutions which are (by default
hidden) under each exercise part. Always try to work on a solution on your
own, but never let this be a barrier to your progress. Feel free to copy-paste
bits of my solutions into your own scripts.

If you find yourself [_totally lost_](https://scryfall.com/card/gtc/54/totally-lost),
don't hesitate to read my solutions from the get go, copy-pasting them into your
own script in the RStudio, executing them line by line, and trying to understand
what's going on. **It's the understanding that we're after here. Whether or not
you can implement everything yourself from scratch does not actually matter at all.** (Nobody in history has learned to program from scratch. Everyone
started by copy-pasting code examples written by someone else. So feel free
to do the same!)































# Programming demographic models with _slendr_

### Part 1: Building a demographic model in _slendr_

**Use functions such as `population()`, `gene_flow()`, and `compile_model()`,
which we discussed in the "crash course" at the
start of this session, to program the following toy model of human demographic
history in _slendr_.** (Apologies for my bad handwriting and poor artistic
ability.)

![](images/intro_model1.png){width="50%"}

::: {.aside}
**Note:** You could easily program the model so that different ancestral
populations are represented by separate `population()` commands (i.e.,
your model would start with a population called "human_chimp_ancestor" from
which a "CHIMP" and "hominin_ancestor" populations would split at 6 Mya, etc.) but
generally this is too annoying to do and requires too much code.

Feel free to write the model so that "CHIMP" is the first population, then
"AFR" population splits from it at 6 Mya, etc... Although it probably isn't
the most accurate way to describe the real evolutionary history, it simplifies
the coding significantly.

<br>
[Mya = million years ago;  kya = thousand years ago]
:::


**Hint:** Create a new script `exercise1.R` in your RStudio session using the following "template". Then add a sequence of appropriate `population()` calls using
the syntax from the introductory slides (using the `parent = <pop>` argument
for programming splits of daughter populations -- which will be all except
the CHIMP lineage in our example), etc.

```{r}
#| eval: false
#| code-fold: false
library(slendr)
init_env()

# <replace this with `population()` definitions like in the slides>
# <replace this with your gene-flow definition in variable `gf`>

model <- compile_model(
  populations = list(...), # <put your list of populations here>
  gene_flow = gf,
  generation_time = 30
)
```

::: aside
**Note:** With _slendr_ you can specify time in whatever format is more convenient or readable for your model. For instance here, because we're dealing with historical events which are commonly expressed in times given as"years ago", we can write them in a decreasing order – i.e. 7Mya → 6Mya → ..., as shown above – or, in terms of R code, 7e6 (or 7000000), 6e6 (6000000), etc.

In a later example, you will see that you can also encode the events in the time
direction going "forward" (i.e., the first event starting in generation 1, a
following event in generation 42, and so on).
:::

**Hint:** Remember that _slendr_ is designed with interactivity in mind!
When you write a chunk of code (such as a command to create a population
through a population split, or model compilation to create a `model` object),
execute that bit of code in the R console and inspect the summary information
printed by evaluating the respective R object you just created. You can either
copy-pasted stuff from your script to the R console, or use a convenient 
RStudio shortcut like Ctrl+Enter (Linux and Windows), or Cmd+Enter (Mac).

::: callout-note
#### Click to see the solution

```{r}
#| collapse: true
library(slendr)
init_env()

# Chimpanzee outgroup
chimp <- population("CHIMP", time = 7e6, N = 5000)

# Two populations of anatomically modern humans: Africans and Europeans
afr <- population("AFR", parent = chimp, time = 6e6, N = 15000)
eur <- population("EUR", parent = afr, time = 70e3, N = 3000)

# Neanderthal population splitting at 600 ky ago from modern humans
# (becomes extinct by 40 ky ago)
nea <- population("NEA", parent = afr, time = 600e3, N = 1000, remove = 40e3)

# Neanderthal introgression event (3% admixture between 55-50 kya)
gf <- gene_flow(from = nea, to = eur, rate = 0.03, start = 55000, end = 50000)

# Compile the entire model into a single slendr R object
model <- compile_model(
  populations = list(chimp, nea, afr, eur),
  gene_flow = gf,
  generation_time = 30,
  path = here::here("data/introgression"),  # <--- don't worry about these two
  overwrite = TRUE, force = TRUE            # <--- lines of code (ask me if interested)
)
```
:::

### Part 2: Inspecting the model visually

To visualize a _slendr_ model, you use the function `plot_model()`.
**Plot your compiled `model` to make sure you programmed it correctly!**
Your figure should roughly correspond to my doodle above.

::: aside
**Note:** Plotting of models in _slendr_ can be sometimes a little wonky,
especially if many things are happening at once. When plotting your
model, experiment with arguments `log = TRUE`, `proportions = TRUE`,
`gene_flow = TRUE`. Check `?plot_model` for more information on these.
:::

::: callout-note
#### Click to see the solution

```{r}
#| collapse: false
plot_model(model)
plot_model(model, sizes = FALSE)
plot_model(model, sizes = FALSE, log = TRUE)
plot_model(model, log = TRUE, proportions = TRUE)
```
:::

### Part 3: Simulating genomic data

Once you have a compiled _slendr_ model stored in an R variable (from now on,
`model` will always mean a variable containing a compiled _slendr_ model object relevant for the given exercise,
for simplicity), we can simulate data from it. By default, _slendr_ models
always produce a [tree sequence](https://tskit.dev/tutorials/what_is.html).

::: {.aside}
**Note:** Tree sequence provides an extremely efficient means to store and
work with genomic data at a massive scale. However, you can always get
simulated data even in [traditional file formats](https://www.slendr.net/reference/index.html#tree-sequence-format-conversion),
such as VCF, EIGENSTRAT, or a plain old table of ancestral/derived genotypes.

In this activity we will be only working with tree sequences, because it's much
easier and faster to get interesting statistics from it directly in R.
:::

There are two simulation engines built into _slendr_ implemented by functions
`msprime()` and `slim()`. For traditional, non-spatial, neutral demographic
models, the engine provided by the `msprime()` function is much more efficient,
so we'll be using that for the time being. However, from a popgen theoretical perspective, both simulation functions
will give you the same results for any given compiled _slendr_ model (up to
some level of stochastic noise, of course).

::: aside
**Note:** Yes, this means you don't have to write any _msprime_ (or SLiM) code
to simulate data from a _slendr_ model!
:::

Here's how you can use the function to simulate a tree sequence from the
model you've just created using `compile_model()` in your script:

```{r}
#| eval: false
#| code-fold: false
ts <- msprime(
  model,
  sequence_length = <length of sequence to simulate [as bp]>,
  recombination_rate = <uniform recombination rate [per bp per generation]>
)
```

You will be seeing this kind of pattern over and over again in this exercise, so
it's a good idea to keep it in mind.


**Hint:** The `msprime()` function has also arguments `debug` and `run` which can be extremely useful for debugging.

**Simulate a tree sequence from your compiled `model` using the `msprime()`
engine, storing it to a variable `ts` as shown right above.
Use `sequence_length = 1e6` (so 1 Mb of sequence) and `recombination_rate = 1e-8` (crossover events per base pair per generation). Then experiment with
setting `debug = TRUE` (this prints out _msprime_'s own debugging summary
which you might already be familiar with from your previous activity?) and
then `run = FALSE` (this prints out a raw command-line which can run a _slendr_
simulation in the shell).**

::: callout-note
#### Click to see the solution

```{r}
#| collapse: true
# This simulates a tskit tree sequence from a slendr model. Note that you didn't have
# to write any msprime or tskit Python code!
ts <- msprime(model, sequence_length = 1e6, recombination_rate = 1e-8)

# Setting `debug = TRUE` instructs slendr's built-in msprime script to print
# out msprime's own debugger information. This can be very useful for debugging,
# in addition to the visualization of the model as shown above.
ts <- msprime(model, sequence_length = 1e6, recombination_rate = 1e-8, debug = TRUE)

# For debugging of technical issues (with msprime, with slendr, or both), it is
# very useful to have the `msprime` function dump the "raw" command-line to
# run the simulation on the terminal using plain Python interpreter
msprime(model, sequence_length = 1e6, recombination_rate = 1e-8, run = FALSE)
```
:::

### Part 4: Inspecting the tree-sequence object

As we will see later, _slendr_ provides an R-friendly interface to
accessing a [subset of _tskit_'s functionality](https://tskit.dev/tskit/docs/stable/python-api.html)
for working with tree sequences and for computing various popgen statistics.

For now, **type out the `ts` object in the terminal – what do you see?** You
should get a summary of a tree-sequence object that you're familiar with from
your _msprime_ and _tskit_ activity earlier in the day.

::: {.aside}
**Note:** This is a very important feature of _slendr_ -- when a simulation is
concluded (doesn't matter if it was a `slim()` or `msprime()` simulation),
you will get a normal _tskit_ object. In fact, the fact that _slendr_ supports
(so far, and likely always) only a "subset" of all of _tskit_'s functionality
isn't stopping you to write custom Python/_tskit_ processing code of a tree
sequence generated from a _slendr_ model. Under the hood, a _slendr_ simulation
_really is_ just an _msprime_ (or SLiM) simulation! It's just executed through
a simplified interface.
:::

::: callout-note
#### Click to see the solution

```{r}
# Typing out the object with the result shows that it's a good old tskit
# tree-sequence object
ts
```
:::

The brilliance of the tree-sequence data structure rests on its elegant
table-based implementation (much more information on that is [here](https://tskit.dev/tskit/docs/stable/data-model.html)). _slendr_ isn't
really designed to run very complex low-level manipulations of tree-sequence
data (its strength lies in the convenient interface to popgen statistical
functions implemented by _tskit_), but it does contain a couple of functions
which can be useful for inspecting the lower-level nature of a tree sequence.
Let's look at a couple of them now.

**Use the `ts_table` function to inspect the low-level table-based representation of a tree sequence.** For instance, you can get the table of nodes with `ts_table(ts, "nodes")`, edges with `ts_table(ts, "edges")`, and do the same thing for "individuals", "mutations", and "sites". **Does your tree sequence contain any mutations? If not, why, and how can we even do any popgen with data without any mutations? As you're doing this, take a look at at the [following
figure](https://tskit.dev/tutorials/_images/0327585c23c21d289094cc6394cc71ecc7e43f14197d7961b9d759c5abcc0e29.svg) (this was made from a different tree sequence than you have, but that's
OK) to help you relate the information in the tables to a tree sequence which
those tables (particularly tables of nodes and edges) implicitly encode.**

This should convince you that the final product of a _slendr_ simulation
really is the same kind of tree-sequence object that you learned about in
the previous activities today. You don't have to study these tables in
detail!

::: callout-note
#### Click to see the solution

```{r}
# slendr provides a helper function which allows access to all the low-level
# components of every tree-sequence object
ts_table(ts, "nodes")
ts_table(ts, "edges")
ts_table(ts, "individuals")
# We didn't simulate any mutations, so we only have genealogies for now.
ts_table(ts, "mutations")
ts_table(ts, "sites")
```
:::

There are also two _slendr_-specific functions called `ts_samples()` (which
retrieves the "symbolic names" and dates of all recorded individuals at
the end of a simulation) and `ts_nodes()`. **You can run them simply as
`ts_samples(ts)` and `ts_nodes(ts)`. How many individuals (samples) are
in your tree sequence as you simulated it? How is the result of `ts_nodes()`
different from `ts_samples()`?**

::: callout-note
#### Click to see the solution

```{r}
# slendr provides a convenient function `ts_samples()` which allows us to
# inspect the contents of a simulated tree sequence in a more human-readable,
# simplified way. We can see that our tree sequence contains a massive number
# of individuals. Too many, in fact -- we recorded every single individual alive
# at the end of our simulation, which is something we're unlikely to be ever lucky
# enough to have, regardless of which species we study.
ts_samples(ts)
ts_samples(ts) %>% nrow()

library(dplyr)
ts_samples(ts) %>% group_by(pop) %>% tally

# This function returns a table similar to the one produced by `ts_table(ts, "nodes")`
# above, except that it contains additional slendr metadata (names of individuals
# belonging to each node, spatial coordinates of nodes for spatial models, etc.).
# It's a bit more useful for analyzing tree-sequence data than the "low-level" functions.
ts_nodes(ts) %>% head(5)

ts_nodes(ts) %>% tail(5)
```
:::

### Part 5: Scheduling sampling events

In the table produced by the `ts_samples()` function you saw that the tree
sequence we simulated recorded _everyone_. It's very unlikely, unless we're
extremely lucky, that we'll ever have a sequence of every single individual
in a population that we study. To get a little closer to the scale of the
genomic data that we usually work with on a day-to-day basis, we can restrict
our simulation to only record a subset of individuals.

We can precisely define which individuals (from which populations, and at which times) should be recorded in a tree sequence using the _slendr_ function `schedule_sampling()`. For instance, if we have a `model` with some _slendr_ populations in variables `eur` and `afr`, we can schedule the recording of 5 individuals from each at times 10000 (years ago) and 0 (present-day) (using
the "years before present" direction of time in our current model of
Neanderthal introgression) with the following code:

```{r}
#| eval: false
pop_schedule <- schedule_sampling(model, times = c(10000, 0), list(eur, 5), list(afr, 5))
```

This function simply returns a data frame. As such, we can create multiple of such schedules (of arbitrary complexity and granularity), and then bind them together into a single sampling schedule with a single line of code, like this:

```{r}
#| eval: false

# Note that the `times =` argument of the `schedule_sampling()` function can be
# a vector of times like here...
ancient_times <- c(40000, 30000, 20000, 10000)
eur_samples <- schedule_sampling(model, times = ancient_times, list(eur, 1))

# ... but also just a single number like here
afr_samples <- schedule_sampling(model, times = 0, list(afr, 1))
nea_samples <- schedule_sampling(model, time = 60000, list(nea, 1))

# But whatever the means you create the individual sampling schedules with,
# you can always bind them all to a single table with the `rbind()` function
schedule <- rbind(eur_samples, afr_samples, nea_samples)
schedule
```

**Using the function `schedule_sampling` (and with the help of `rbind` as
shown in the previous code chunk), program the sampling of the following
sample sets at given times, saving it to variable called `schedule`:**

| time  | population | \# individuals |
|-------|:-----------|----------------|
| 70000 | nea        | 1              |
| 40000 | nea        | 1              |
| 0     | chimp      | 1              |
| 0     | afr        | 5              |
| 0     | eur        | 10             |

**Additionally, schedule the sampling of a single `eur` individual at the
following times:**

```{r}
t <- seq(40000, 2000, by = -2000)
```

::: aside
**Note:** You can provide a vector variable (such as `t` in this example) as the
`times =` argument of `schedule_sampling()`.
:::

**In total, you should schedule the recording of 38 individuals.**

::: callout-note
#### Click to see the solution

```{r}
# Here we scheduled the sampling of two Neanderthals at 70kya and 40kya
nea_samples <- schedule_sampling(model, times = c(70000, 40000), list(nea, 1))
nea_samples # (this function produces a plain old data frame!)

# Here we schedule one Chimpanzee sample, 5 African samples, and 10 European samples
present_samples <- schedule_sampling(model, times = 0, list(chimp, 1), list(afr, 5), list(eur, 10))

# We also schedule the recording of one European sample between 50kya and 2kya,
# every 2000 years
times <- seq(40000, 2000, by = -2000)
emh_samples <- schedule_sampling(model, times, list(eur, 1))

# Because those functions produce nothing but a data frame, we can bind
# individual sampling schedules together
schedule <- rbind(nea_samples, present_samples, emh_samples)
schedule
```
:::

**Then, verify the correctness of your overall sampling `schedule` by visualizing
it together with your `model` like this:**

::: aside
**Note:** As you've seen above, the visualization is often a bit wonky and convoluted with overlapping elements and it can be even worse with samples added, but try to experiment with arguments to `plot_model` described above to make the plot a bit more helpful for sanity checking.
:::

```{r}
#| eval: false
plot_model(model, samples = schedule)
```

::: callout-note
#### Click to see the solution

```{r}
plot_model(model, sizes = FALSE, samples = schedule)
```

```{r}
plot_model(model, sizes = FALSE, log = TRUE, samples = schedule)
```
:::

### Part 6: Simulating a defined set of individuals

You have now both a compiled _slendr_ `model` and a well-defined sampling `schedule`.

**Use your combined sampling schedule stored in the `schedule` variable to run a
new tree-sequence simulation from your model (again using the `msprime()` function),
this time restricted to just those individuals scheduled for recording. You can
do this by providing the combined sampling `schedule` as the `samples = schedule` argument
of the function `msprime` you used above.** Just replace the line(s) with your first
`msprime()` from the previous part of this exercise with the new one, which
uses the `schedule` for customized sampling.

**Also, while you're doing this, use the `ts_mutate()` function to 
overlay neutral mutations on the simulated tree sequence right after the
`msprime()` call.** (Take a look at the handounts for a reminder of the `%>%`
pipe patterns I showed you.)

::: callout-note
#### Click to see the solution

```{r}
#| echo: false
model <- read_model(here::here("data/introgression"))
ts <- ts_read(file = here::here("data/introgression.trees"), model = model)
```

```{r}
#| eval: false
# The command below will likely take a few minutes to run, so feel free to go
# down from 100 Mb sequence_length to even 10Mb (it doesn't matter much).
# (The `random_seed =` argument is there for reproducibility purposes.)
ts <-
  msprime(model, sequence_length = 100e6, recombination_rate = 1e-8, samples = schedule, random_seed = 1269258439) %>%
  ts_mutate(mutation_rate = 1e-8, random_seed = 1269258439)
# Time difference of 2.141642 mins

# If you're bothered by ho long this takes, feel free to call these two lines
# to 100% reproduce my results without any expensive computation:
model <- read_model(here::here("data/introgression"))
ts <- ts_read(here::here(file = "data/introgression.trees"), model = model)

# We can save a tree sequence object using a slendr function `ts_write` (this
# can be useful if we want to save the results of a simulation for later use).
dir.create("data", showWarnings = FALSE)
ts_write(ts, "data/introgression.trees")
```
:::

**Inspect the tree-sequence object saved in the `ts` variable by typing
it into the R console again** (this interactivity really helps with catching
nasty bugs early during the programming of your script). **You can
also do a similar thing via the table produced by the `ts_samples()` function.
You should see a much smaller number of individuals being recorded, indicating
that the simulation was much more efficient and produced genomic data for
only the individuals of interest.**

::: aside
**Note:** When you think about it, it's actually quite astonishing how fast
_msprime_ and _tskit_ are when dealing with such a huge amount of sequence
data from tens of thousands of individuals on a simple laptop!
:::

::: callout-note
#### Click to see the solution

```{r}
# Inspect the (tskit/Python-based) summary of the new tree sequence
# (note the much smaller number of "sample nodes"!)
ts

# Get the table of all recorded samples in the tree sequence
ts_samples(ts)

# Compute the count of individuals in different time points
library(dplyr)

ts_samples(ts) %>% group_by(pop, present_day = time == 0) %>% tally %>% select(present_day, pop, n)
```
:::


<!-- End of Bonus exercises -->


























# Computing popgen statistics on tree sequences from _slendr_

In this exercise, you will build on top of the results from Exercise 1.
Specifically, we will learn how to compute popgen statistics on _slendr_-simulated
tree sequences using
[_slendr_'s interface](https://www.slendr.net/reference/index.html#tree-sequence-statistics)
to the _tskit_ Python module.

**First, create a new R script `exercise2.R` and paste in the following code.** This
is one of the possible solutions to the Exercise 1, and it's easier if we all
use it to be on the same page from now on, starting from the same model and
the same simulated tree sequence:

```{r}
#| collapse: true
library(slendr)
init_env()

chimp <- population("CHIMP", time = 7e6, N = 5000)
afr <- population("AFR", parent = chimp, time = 6e6, N = 15000)
eur <- population("EUR", parent = afr, time = 70e3, N = 3000)
nea <- population("NEA", parent = afr, time = 600e3, N = 1000, remove = 40e3)

gf <- gene_flow(from = nea, to = eur, rate = 0.03, start = 55000, end = 50000)

model <- compile_model(
  populations = list(chimp, nea, afr, eur),
  gene_flow = gf,
  generation_time = 30
)

# We will read a cached version of a tree sequence I simulated myself
# to make sure we're all on the same page. That said, if you managed to
# do Exercise 1 on your own, feel free to stick with your own tree sequence!
ts <- ts_read(here::here("data/introgression.trees"), model = model)

cowplot::plot_grid(
  plot_model(model, proportions = TRUE),
  plot_model(model, proportions = TRUE, log = TRUE),
  nrow = 1
)
```

As a sanity check, let's use a couple of tidyverse table-munging tricks
to make sure the tree sequence does contain a set of sample
which matches our intended sampling schedule (particularly the time series
of European individuals and the two Neanderthals):

```{r}
library(dplyr)

# total number of recorded individuals in the tree sequence
ts_samples(ts) %>% nrow
# times of sampling of each recorded European individual
ts_samples(ts) %>% filter(pop == "EUR") %>% pull(time)
# times of sampling of each recorded Neanderthal
ts_samples(ts) %>% filter(pop == "NEA") %>% pull(time)
# count of individuals in each population
ts_samples(ts) %>%
  group_by(pop, present_day = time == 0) %>%
  tally %>%
  select(pop, present_day, n) %>%
  arrange(present_day)
```

::: {.aside}
**Note:** These bits of tidyverse code are extremely helpful when you're working
with large tree sequences with many individuals as sanity checks that your
sampling worked as intended. I'm listing them here in case you've never worked
with the tidyverse family of R packages before (such as the _dplyr_ package
where `filter()`, `group_by()`, `tally()`, and `pull()` come from).
:::

Everything looks good! Having made sure that the `ts` object contains the
individuals we want, let's move to the exercise.

## Part 1: Computing nucleotide diversity

The toy model of ancient human history plotted above makes a fairly clear prediction of what would be the
nucleotide diversity expected in the simulated populations.
**Compute the nucleotide diversity in all populations using the _slendr_ function
[`ts_diversity()`](https://www.slendr.net/reference/ts_diversity.html#ref-examples) in your tree sequence `ts`. Do you get numbers that (relatively between
all populations) match what would expect from the model given the $N_e$ that
you programmed for each?**

**Hint:** Nearly every _slendr_ statistic function interfacing with _tskit_
accepts a `ts` tree-sequence object as its first argument, with further arguments
being either a vector of individual names representing a group of samples to
compute a statistic on, or a (named) list of such vectors (each element of that
list for a group of samples) -- these lists are intended to be equivalent to
the `sample_sets =` argument of many _tskit_ Python methods (which you've learned
about in your activity on _tskit_), except that they allow symbolic names
of individuals, rather then integer indices of nodes in a tree sequence.

Although you can get all the above information by processing the table produced
by the `ts_samples()` function, _slendr_ provides a useful helper function
`ts_names()` which only returns the names of individuals as a vector 
(or a named list of such vectors, one vector per population as shown below).

When you call it directly, you get a plain vector of individual names:

```{r}
ts_names(ts)
```

This is not super helpful, unless we want to compute some statistic for _everyone_
in the tree sequence, regardless of their population assignment. Perhaps a bit
more useful is to call the function like this, because it will produce a result
which can be immediately used as the `sample_sets =` argument mentioned in the
**Hint** above:

```{r}
ts_names(ts, split = "pop")
```

As you can see, this gave us a normal R list, with each element containing
a vector of individual names in a population. Note that we can use standard R
list indexing to get subsets of individuals:

```{r}
names <- ts_names(ts, split = "pop")

names["NEA"]

names[c("EUR", "NEA")]
```

etc.

Many of the following exercises will use these kinds of tricks to instruct
various _slendr_ / _tskit_ functions to compute statistics on subsets of
all individuals sub-sampled in this way.

**After you computed nucleotide diversity per-population, compute it for
each individual separately using the same function `ts_diversity()`** (which,
in this setting, gives you effectively the heterozygosity for each individual).
**If you are familiar with plotting in R, visualize the individual-based
heterozygosities across all populations.**

**Hint:** You can do this by giving a vector of names as `sample_sets =` (so
not an R list of vectors). You could also use the data frame produced by
`ts_samples(ts)` to get the names, just adding the heterozygosities to that
data frame as a new column.

::: callout-note
### Click to see the solution

**Population-based nucleotide diversity:**

```{r}
# Let's first get a named list of individuals in each group we want to be
# working with (slendr tree-sequence statistic functions generally operate
# with this kind of structure)
sample_sets <- ts_names(ts, split = "pop")
sample_sets

# We can use such `sample_sets` object to compute nucleotide diversity (pi)\
# in each population, in a bit of a similar manner to how we would do it
# with the standard tskit in Python
pi_pop <- ts_diversity(ts, sample_sets = sample_sets)
arrange(pi_pop, diversity)
```

You can see that this simple computation fits  the extreme differences in
long-term $N_e$ encoded by your _slendr_ demografr model.

**Per-individual heterozygosity:**

We can do this by passing the vector of individual names directory as the `sample_sets =` argument, rather than in a list of groups as we did above.

```{r}
# For convenience, we first get a table of all individuals (which of course
# contains also their names) and in the next step, we'll just add their
# heterozygosities as a new column.
pi_df <- ts_samples(ts)
pi_df$name

pi_df$diversity <- ts_diversity(ts, sample_sets = pi_df$name)$diversity
pi_df

# Let's plot the results using the ggplot2 package
# (because a picture is worth a thousand numbers!)
library(ggplot2)

ggplot(pi_df, aes(pop, diversity, color = pop, group = pop)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter() +
  theme_bw()
```
:::

## Part 2: Computing pairwise divergence

**Use the function
[`ts_divergence()`](https://www.slendr.net/reference/ts_divergence.html#ref-examples)
to compute genetic divergence between all pairs of populations. Again, do you
get results compatible with our demographic model in terms of expectation
given the split times between populations as you programmed them for your
model?**

**Hint:** Again, you can use the same concept of `sample_sets =` we discussed
in the previous part. In this case, the function computes _pairwise_
divergence between each element of the list given as `sample_sets =` (i.e.,
for each vector of individual names).

::: callout-note
### Click to see the solution

```{r}
sample_sets <- ts_names(ts, split = "pop")

div_df <- ts_divergence(ts, sample_sets)
arrange(div_df, divergence)
```

We can see that the pairwise nucleotide divergences between populations
recapitulate the known population/species relationships we would expect from
our model.
:::

## Part 3: Detecting Neanderthal admixture in Europeans

Let's now pretend its about 2008, we've sequenced the first Neanderthal genome,
and we are working on a project that will
[change human evolution research forever](https://www.science.org/doi/10.1126/science.1188021).
We also have the genomes of a couple of people from Africa and Europe, which we
want to use to answer the most burning question of all evolutionary
anthropology: _"Do some people living today carry Neanderthal ancestry?"_

Earlier you've learned about $f$-statistics of various kinds. You have also
heard that an $f_4$ statistic (or its equivalent $D$ statistic) can be used
as a test of "treeness". Simply speaking, for some "quartet" of individuals
or population samples, they can be used as a hypothesis test of whether the
history of those samples is compatible with there not having been an introgression.

**Compute the $f_4$ test of Neanderthal introgression in EUR individuals using
the _slendr_ function `ts_f4()`.** When you're running it, you will have to
provide individuals to compute the statistic using a slightly different
format. Take a look at the help page available as `?ts_f4` for more information.
**When you're computing the $f_4$, make sure to set `mode = "branch"` argument
of the `ts_f4()` function (we will get to why a bit later).**

::: {.aside}
**Note:** By default, each _slendr_ / _tskit_ statistic function operates
on mutations, and this will switch them to use branch length (as you might
know, $f$-statistics are mathematically defined using branch lengths in trees
and `mode = "branch"` does exactly that).
:::

**Hint:** If you haven't learned this in your $f$-statistics lecture, you want
to compute (and compare) the values of these two statistics using the _slendr_
function `ts_f4()`:

1. $f_4$(\<some African\>, \<another African\>; \<Neanderthal\>, \<Chimp\>)

2. $f_4$(\<some African\>, \<a test European\>; \<Neanderthal\>, \<Chimp\>),

here `<individual>` can be the name of any individual recorded in your
tree sequence, such as names you saw as `name` column in the table returned
by `ts_samples(ts)` (i.e. `"NEA_1"` could be used as a "representative"
\<Neanderthal\> in those equations, similarly for `"CHIMP_1"` as the fourth
sample in the $f_4$ quarted representing the outgroup).

To simplify things a lot, we can understand the above equations as comparing the
counts of so-called BABA and ABBA allele patterns between the quarted of samples
specified in the statistics:

$$
f_4(AFR, X; NEA, CHIMP) = \frac{\#BABA - \#ABBA}{\#SNPs}
$$

The first $f_4$ statistic above is not expected to give values "too different"
from 0 (even in case of Neanderthal introgression into Europeans) because we
don't expect two African individuals to differ "significantly" in terms of how
much alleles they share with a Neanderthal (because their ancestors
never met Neanderthals!). The other should -- if there was
a Neanderthal introgression into Europeans some time in their history -- be
"significantly negative".

**Is the second of those two statistics "much more negative" than the first,
as expected assuming introgression from Neanderthals into Europeans?**

**Why am I putting "significantly" and "much more
negative" in quotes in the previous sentence?
What are we missing here for this being a true hypothesis
test as you might be accustomed to from computing $f$-statistics using
a tool such as ADMIXTOOLS?** (We will get to this again in the following part
of this exercise.)

::: callout-note
### Click to see the solution

```{r}
# Compute the difference in the amount of allele sharing between two African
# individuals and a Neanderthal
f4_null <- ts_f4(ts, W = "AFR_1", X = "AFR_2", Y = "NEA_1", Z = "CHIMP_1", mode = "branch")
f4_null

# Compute the difference in the amount of allele sharing between an African
# individual vs European individual and a Neanderthal
f4_alt <- ts_f4(ts, W = "AFR_1", X = "EUR_1", Y = "NEA_1", Z = "CHIMP_1", mode = "branch")
f4_alt

# We can see that the second test resulted in an f4 value about ~20 times more
# negative than the first test, indicating that a European in our test carries
# "significantly more" Neanderthal alleles compared to the baseline expectation
# of no introgression established by the comparison to an African ...
abs(f4_alt$f4 / f4_null$f4)

# ... although this is not a real test of significance (we have no Z-score or
# standard error which would give us something like a p-value for the hypothesis
# test, as we get by jackknife procedure in ADMIXTOOLS)
```
:::

## Part 4: Detecting Neanderthal admixture in Europeans v2.0

The fact that we don't get something equivalent to a p-value in these kinds of
simulations is generally not a problem, because we're often interested in
establishing a trend of a statistic under various conditions, and understanding
when and how its _expected value_ behaves in a certain way.
If statistical noise is a problem, we work around this by computing a
statistic on multiple simulation replicates or even increasing the sample sizes.

::: {.aside}
**Note:** To see this in practice, you can check out a
[paper](https://www.pnas.org/doi/10.1073/pnas.1814338116#fig02) in which I used
this approach quite successfully on a related problem.
:::

On top of that, p-value of something like an $f$-statistic (whether it's
significantly different from zero) is also strongly affected by quality of
the data, sequencing errors, coverage, etc. (which can certainly be examined
using simulations!). However, these are aspects of modeling which are quite
orthogonal to the problem of investigating the expectations and trends of
statistics given some underlying evolutionary model, which is what we're after
in these exercises.

That said, even in perfect simulated data, what exactly does
"significantly different from zero compared to some baseline expectation"
mean can be blurred by noise with simple single-individual comparisons that we
did above. Let's increase the sample size a bit to see if a statistical
pattern expected in $f_4$ statistic from our Neanderthal introgression model
becomes more apparent.

**Compute the first $f_4$ statistic (the baseline expectation between
a pair of Africans) and the second $f_4$ statistic (comparing an African and
a European), but this time on _all recorded Africans_ and _all recorded
Europeans_, respectively. Plot the *distributions* of those two sets of
statistics. This should remove lots of the uncertainty and make a statistical
trend stand out much more clearly.**

**Hint:** Whenever you need to compute something for many things in sequence,
looping is very useful. One way to do compute, say, an $f_4$ statistic over
many individuals is by using this kind of pattern using R's looping function 
`lapply()`:

```{r}
#| eval: false

# Loop over vector of individual names (variable x) and apply a given ts_f4()
# expression on each individual (note the ts_f4(..., X = x, ...) in the code)
list_f4 <- lapply(
  c("ind_1", "ind_2", ...),
  function(x) ts_f4(ts, W = "AFR_1", X = x, Y = "NEA_1", Z = "CHIMP_1", mode = "branch")
)

# The above gives us a list of data frames, so we need to bind them all into a
# single table for easier interpretation and visualization
df_f4 <- do.call(rbind, list_f4)
```

::: callout-note
### Click to see the solution

```{r}
# This gives us list of vectors of the names of all individuals in each
# population...
sample_sets <- ts_names(ts, split = "pop")
# ... which we can then access like this
sample_sets$AFR # all Africans
sample_sets$EUR # all Europeans

# Let's compute the f4 statistic for all Africans... 
f4_afr_list <- lapply(
  sample_sets$AFR,
  function(x) ts_f4(ts, W = "AFR_1", X = x, Y = "NEA_1", Z = "CHIMP_1", mode = "branch")
)
# ... and Europeans
f4_eur_list <- lapply(
  sample_sets$EUR,
  function(x) ts_f4(ts, W = "AFR_1", X = x, Y = "NEA_1", Z = "CHIMP_1", mode = "branch")
)

# Bind each list of data frames into a single data frame
f4_afr <- do.call(rbind, f4_afr_list)
f4_eur <- do.call(rbind, f4_eur_list)

# Let's add population columns to each of the two results for easier plotting
f4_afr$pop <- "AFR"
f4_eur$pop <- "EUR"

# Bind both tables together
f4_results <- rbind(f4_afr, f4_eur)

# Visualize the results
f4_results %>%
  ggplot(aes(pop, f4, color = pop)) +
  geom_boxplot() +
  geom_jitter() +
  geom_hline(yintercept = 0, linetype = 2) +
  ggtitle("f4(AFR, EUR; NEA, CHIMP)") +
  theme_bw()
```

We can see that the $f_4$ statistic test of Neanderthal introgression in
Europeans indeed does give a much more negative distribution of values compared
to the baseline expectation which compares two Africans to each other.
:::

::: callout-tip
## Bonus exercises

### Bonus 1: `mode = "branch"` vs `mode = "site"`

**Repeat the previous part of the exercise by setting `mode = "site"` in the 
`ts_f4()` function calls** (this is actually the default behavior of all
_slendr_ tree-sequence based _tskit_ functions). This will switch the _tskit_
computation to using mutation counts along each branch of the tree sequence,
rather than using branch length themselves. **Why might the branch-based computation
be a bit better if what we're after is investigating the expected values of statistics
under some model?**


::: callout-note
### Click to see the solution

See [this tutorial](https://tskit.dev/tutorials/no_mutations.html#genealogy-based-measures-are-less-noisy) (and particularly the directly linked section) for explanation.
:::


### Bonus 2: Outgroup $f_3$ statistic

**Use the function `ts_f3()` to compute the outgroup** $f_3$ statistic between pairs of African-European, African-Neanderthal, and European-Neanderthal and a Chimpanzee outgroup.

**Hint:** The $f_3$ statistic is traditionally expressed as $f_3(A, B; C)$, where C represents the outgroup. Unfortunately, in _tskit_ the outgroup is named A, with B and C being the pair of samples from which we trace the length of branches towards the outgroup, so the statistic is interpreted as $f_3(B, C; A)$.

**How do the outgroup f3 results compare to your expectation based on simple population relationships (and to the divergence computation above)?**

**Do you see any impact of introgression on the $f_3$ value when a Neanderthal is included in the computation?**

::: callout-note
### Click to see the solution

```{r}
# f3(A, B; C) = E[ (A - C) * (B - C) ]
# This means that in tskit, C is the outgroup (different from ADMIXTOOLS!)

# We can compute f3 for individuals...
ts_f3(ts, B = "AFR_1", C = "EUR_1", A = "CHIMP_1")

# ... but also whole populations (or population samples)
ts_f3(ts, B = sample_sets["AFR"], C = sample_sets["EUR"], A = "CHIMP_1")

ts_f3(ts, B = sample_sets["AFR"], C = sample_sets["NEA"], A = "CHIMP_1")

ts_f3(ts, B = sample_sets["EUR"], C = sample_sets["NEA"], A = "CHIMP_1")
```
:::

### Bonus 3: Outgroup $f_3$ statistic as a linear combination of $f_2$ statistics

You might have learned that any complex $f$-statistic can be expressed as a linear combination of multiple $f_2$ statistics (which represent simple branch length separating two lineages). **Verify that this is the case by looking up equation *(20b)* in [this amazing paper](https://academic.oup.com/genetics/article/202/4/1485/5930214) and compute an** $f_3$ statistic for any arbitrary trio of individuals of your choosing using this linear combination of $f_2$ statistics.

::: callout-note
### Click to see the solution

```{r}
# standard f3
ts_f3(ts, B = "AFR_1", C = "AFR_2", A = "CHIMP_1")

# a "homemade" f3 statistic as a linear combination of f2 statistics
# f3(A, B; C) = f2(A, C) + f2(B, C) - f2(A, B) / 2
homemade_f3 <- (
  ts_f2(ts, A = "AFR_1", B = "CHIMP_1")$f2 +
  ts_f2(ts, A = "AFR_2", B = "CHIMP_1")$f2 -
  ts_f2(ts, A = "AFR_1", B = "AFR_2")$f2
) / 2
homemade_f3
```
:::

### Bonus 4: Trajectory of Neanderthal ancestry in Europe over time

There used to be a lot of controversy about the question of whether or not did Neanderthal ancestry proportion in Europeans decline or not over the past 40 thousand years (see figure 1 in [this paper](https://www.pnas.org/doi/full/10.1073/pnas.1814338116) figure 2 in [this paper](https://www.nature.com/articles/nature17993)).

Your simulated tree sequence contains a time-series of European individuals over time. Use the _slendr_ function `ts_f4ratio()` to compute (and then plot) the proportion (commonly designated as `alpha`) of Neanderthal ancestry in Europe over time. Use $f_4$-ratio statistic of the following form:

```{r}
#| eval: false
ts_f4ratio(ts, X = <vector of ind. names>, A = "NEA_1", B = "NEA_2", C = "AFR_1", O = "CHIMP_1")
```

::: callout-note
### Click to see the solution

```{r}
# Extract table with names and times of sampled Europeans (ancient and present day)
eur_inds <- ts_samples(ts) %>% filter(pop == "EUR")
eur_inds

# Compute f4-ration statistic (this will take ~30s) -- note that we can provide
# a vector of names for the X sample set to the `ts_f4ratio()` function
nea_ancestry <- ts_f4ratio(ts, X = eur_inds$name, A = "NEA_1", B = "NEA_2", C = "AFR_1", O = "CHIMP_1")

# Add the age of each sample to the table of proportions
nea_ancestry$time <- eur_inds$time
nea_ancestry

nea_ancestry %>%
  ggplot(aes(time, alpha)) +
  geom_point() +
  geom_smooth(method = "lm", linetype = 2, color = "red", linewidth = 0.5) +
  xlim(40000, 0) +
  coord_cartesian(ylim = c(0, 0.1)) +
  labs(x = "time [years ago]", y = "Neanderthal ancestry proportion") +
  theme_bw() +
  ggtitle("Neanderthal ancestry proportion in Europeans over time")

# For good measure, let's test the significance of the decline using a linear model
summary(lm(alpha ~ time, data = nea_ancestry))
```
:::

### Bonus 5: How many unique f4 quartets are there?

In your lecture about $f$-statistics, you've probably learned about various symmetries in $f_4$ (but also other $f$-statistics) depending on the arrangement of the "quartet". As a trivial example, an $f_3(A; B, C)$ and $f_3(A; C, B)$ will give you exactly the same value, and the same thing applies even to more complex $f$-statistics like $f_4$.

**Use simulations to compute how manu unique** $f_4$ values involving a single quartet are there.

**Hint:** Draw some trees to figure out why could that be true. Also, when computing `ts_f4()`, set `mode = "branch"` to avoid the effect of statistical noise due to mutations.

::: callout-note
### Click to see the solution

```{r}
# # install a combinatorics R package
# install.packages("combinat")

library(combinat)

# These are the four samples we can create quartet combinations from
quartet <- c("AFR_1", "EUR_1", "NEA_1", "CHIMP_1")
quartets <- permn(quartet)
quartets

# How many permutations there are in total?
#   4! = 4 * 3 * 2 * 1 = 24
factorial(4)

# We should therefore have 24 different quartet combinations of samples
length(quartets)

# Loop across all quartets, computing the corresponding f4 statistic (we want
# to do this using branch lengths, not mutations, as the mutation-based computation
# would involve statistical noise)
all_f4s <- lapply(quartets, function(q) ts_f4(ts, q[1], q[2], q[3], q[4], mode = "branch"))

# Bind the list of f4 results into a single data frame and inspect the results
all_f4s <- bind_rows(all_f4s) %>% arrange(abs(f4))
print(all_f4s, n = Inf)

# Narrow down the results to only unique f4 values
distinct(all_f4s, f4, .keep_all = TRUE)
distinct(all_f4s, abs(f4), .keep_all = TRUE)
```
:::
::::::
<!-- End of Bonus exercises -->

































# Simulation-based inference of $N_e$

So far we've learned how _slendr_ provides an easy way to define
demographic models in R and simulate (even very large!) tree sequences from them.
This allows us to quickly verify our intuition about some popgen problem
(things like _"Hmmm, I wonder what would an $f_4$ statistic look like if my model
includes this particular gene-flow event?_), in just a few lines of R. There
have been instances in which we've been able to even answer questions like
this directly in a meeting, pretty much on the spot! This makes _slendr_ a
very powerful "popgen calculator".

Now let's take things one step further. Imagine you gathered some empirical
data, like an allele frequency spectrum (AFS) from a population that you
study. That data was, in the real world, produced by some (hidden) biological
process (demographic history) that we want to learn about.
For instance, the population we study had some $N_e$, which we don't
know the value of (the only thing we have is the observed AFS) but we
want to infer that value.

Simulations can be a great tool to estimate the most likely value of such an
unknown parameter. Briefly speaking, in this particular toy example,
we can simulate a large number of AFS vectors (each resulting from a different
assumed $N_e$ value) and then pick just those $N_e$ values (or just one $N_e$
value) which produced a simulated AFS closest to the observed AFS.

This is exactly what you'll be doing just now in Exercise 3.

## Part 1: A self-contained _slendr_ function of $N_e \rightarrow \textrm{AFS}$

**In a new script `exercise3.R` write a custom R function called `simulate_afs()`,
which will take `Ne` as its only parameter. Use this function to compute (and
return) [AFS vectors](https://en.wikipedia.org/wiki/Allele_frequency_spectrum)
for a couple of `Ne` values of your choosing, but staying between
`Ne = 1000` and `Ne = 30000` Plot those AFS vectors and observe how (and why?)
do they differ based on `Ne` parameter you used in each respective simulation.**

**Hint:** The function should create a one-population _forward-time_ model
(our population starting at `time = 1`, with the model
`simulation_length = 100000`
and `generation_time = 1` in `compile_model()`),
simulate 10Mb tree sequence using `msprime()` (recombination rate 1e-8) and then overlay neutral mutations on it at `mutation_rate = 1e-8`), compute AFS for 10 samples and return the AFS vector as result of this custom function.

**Hint:** If you've never programmed before, the concept of a "custom function" might
be very alien to you. Again, if you need help, feel free to start building your
`exercise3.R` solution based on this "template" (just fill in missing relevant
bits of _slendr_ code that you should be already familiar with):

```{r}
#| eval: false
library(slendr)
init_env()

simulate_afs <- function(Ne) {
  # In here you should write code which will:
  #   1. create one population with a given Ne (provided as a function argument)
  #   2. compile a model using `simulation_length =` and `generation_time =`
  #   3. simulate a tree sequence
  #   4. select names of 10 samples (doesn't matter which, "pop_1", "po2_", ...)
  #   5. compute AFS vector from those 10 individuals using `ts_afs()`
  
  # `result` is a variable with your 10-sample AFS vector (we remove the
  # first element because it's not meaningful for our example)
  return(result[-1]) 
}

afs_1 <- simulate_afs(Ne = 1000) # simulate AFS from a Ne = 1000 model...
plot(afs_1, type ="o")           # ... and plot it
```

::: {.aside}
**Note:** Remember that you should drop the first element of the AFS vector
produced by `ts_afs()` (for instance with something like `result[-1]` if
`result` contains the output of `ts_afs()`) technical reasons related to
_tskit_. You don't have to worry about that here, but you can read
[this](https://tskit.dev/tutorials/analysing_tree_sequences.html#sec-tutorial-afs-zeroth-entry) for more detail.
:::

**Hint:** **If the above still doesn't make any sense to you, feel free to
copy-paste the function from the solution below into your script and work with
that function instead!**

When used in R, your custom function should work like this (the simulation
is stochastic, so your numbers will be different, of course):

```{r}
#| echo: false
# simulate_afs <- function(Ne) {
#   n <- 20 # 1 is for the fixed sites included by tskit
#   theta <- 4 * 1e-8 * Ne * 100e6
#   round(theta * 1/1:n)
# }
simulate_afs <- function(Ne) {
  # create a slendr model with a single population of size Ne = N
  pop <- population("pop", N = Ne, time = 1)
  model <- compile_model(pop, generation_time = 1, simulation_length = 100000)

  # simulate a tree sequence
  ts <-
    msprime(model, sequence_length = 10e6, recombination_rate = 1e-8) %>%
    ts_mutate(mutation_rate = 1e-8)

  # get a random sample of names of 10 individuals
  samples <- ts_names(ts) %>% sample(10)

  # compute the AFS vector (dropping the 0-th element added by tskit)
  afs <- ts_afs(ts, sample_sets = list(samples))[-1]

  afs
}
```

```{r}
# This gives us a vector of singletons, doubletons, etc., etc., all the way
# to the number of fixed mutations in our sample of 10 individuals
simulate_afs(Ne = 1000)
```



::: callout-note
#### Click to see the solution
```{r}
# An R function can be understood as a block of a computer program which executes
# a block of code inside the {...} brackets given a certain value of a parameter
# (here 'Ne' just after the word 'function')
simulate_afs <- function(Ne) {
  # create a slendr model with a single population of size Ne = N
  pop <- population("pop", N = Ne, time = 1)
  model <- compile_model(pop, generation_time = 1, simulation_length = 100000)

  # simulate a tree sequence
  ts <-
    msprime(model, sequence_length = 10e6, recombination_rate = 1e-8) %>%
    ts_mutate(mutation_rate = 1e-8)

  # get a random sample of names of 10 individuals
  samples <- ts_names(ts) %>% sample(10)

  # compute the AFS vector (dropping the 0-th element added by tskit)
  afs <- ts_afs(ts, sample_sets = list(samples))[-1]

  afs
}

# Let's use our custom function to simulate AFS vector for Ne = 1k, 10k, and 30k
afs_1k <- simulate_afs(1000)
afs_10k <- simulate_afs(10000)
afs_30k <- simulate_afs(30000)

# Plot the three simulated AFS using base R plotting functionality
plot(afs_30k, type = "o", main = "AFS, Ne = 30000", col = "cyan",)
lines(afs_10k, type = "o", main = "AFS, Ne = 10000", col = "purple")
lines(afs_1k, type = "o", main = "AFS, Ne = 1000", col = "blue")
legend("topright", legend = c("Ne = 1k", "Ne = 10k", "Ne = 30k"),
       fill = c("blue", "purple", "cyan"))
```
:::




## Part 2: Estimating unknown $N_e$ from empirical AFS

```{r}
#| eval: false
#| echo: false
set.seed(42)
TRUE_NE <- 6543

pop <- population("pop", N = TRUE_NE, time = 100000)
model <- compile_model(pop, generation_time = 1, direction = "backward")

ts <-
  msprime(model, sequence_length = 10e6, recombination_rate = 1e-8, random_seed = 42) %>%
  ts_mutate(mutation_rate = 1e-8, random_seed = 42)

samples <- ts_names(ts) %>% sample(10)

afs_observed <- ts_afs(ts, list(samples))
```

Imagine you sequenced 10 samples from a population and computed the following
AFS vector (which contains, sequentially, the number of singletons, doubletons,
etc., in your sample from a population):

<!-- dput(as.vector(observed_afs)) -->

```{r}
afs_observed <- c(2520, 1449, 855, 622, 530, 446, 365, 334, 349, 244,
                  264, 218,  133, 173, 159, 142, 167, 129, 125, 143)
```

You know (maybe from some fossil evidence) that the population probably had
a constant $N_e$ somewhere between 1000 and 30000 for the past 100,000 generations,
and had mutation and recombination rates of 1e-8 (i.e., parameters already
implemented by your `simulate_afs()` function -- how convenient!).

**Use _slendr_ simulations to guess the true (and hidden!) $N_e$ given the observed
AFS by running simulations for a range of $N_e$ values and finding out
which $N_e$ produces the closest AFS vector to the `afs_observed` vector above
using one of the following two approaches.**

- **Option 1** [easy]: Plot AFS vectors for various $N_e$ values (i.e. simulate
several of them using your function `simulate_afs()`), then eyeball
which looks closest to the observed AFS based on the figures alone. (This is,
of course, not how proper statistical inference is done, but it will be
good enough for this exercie!)

- **Option 2** [hard]: Simulate AFS vectors in steps of possible `Ne` (maybe
`lapply()`?), and find the $N_e$ which gives the closest AFS to the observed AFS based on [Mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error).



::: callout-note
#### Click to see the solution to "Option 1"
```{r}
# This is our starting observed AFS which we want to compare simulated AFS vectors to
afs_observed <- c(2520, 1449, 855, 622, 530, 446, 365, 334, 349, 244,
                  264, 218,  133, 173, 159, 142, 167, 129, 125, 143)

# We know that the Ne is between 1000 and 30000, so let's simulate
# a bunch of AFS vectors for different Ne values using our custom
# AFS simulation function
afs_Ne1k <- simulate_afs(Ne = 1000)
afs_Ne5k <- simulate_afs(Ne = 5000)
afs_Ne6k <- simulate_afs(Ne = 6000)
afs_Ne10k <- simulate_afs(Ne = 10000)
afs_Ne20k <- simulate_afs(Ne = 20000)
afs_Ne30k <- simulate_afs(Ne = 30000)

# Plot all simulated AFS vectors, highlighting the observed AFS in black
plot(afs_observed, type = "b", col = "black", lwd = 3,
     xlab = "allele count bin", ylab = "count", ylim = c(0, 13000))
lines(afs_Ne1k, lwd = 2, col = "blue")
lines(afs_Ne5k, lwd = 2, col = "green")
lines(afs_Ne6k, lwd = 2, col = "pink")
lines(afs_Ne10k, lwd = 2, col = "purple")
lines(afs_Ne20k, lwd = 2, col = "orange")
lines(afs_Ne30k, lwd = 2, col = "cyan")
legend("topright",
       legend = c("observed AFS", "Ne = 1000", "Ne = 5000",
                  "Ne = 6000", "Ne = 10000", "Ne = 20000", "Ne = 30000"),
       fill = c("black", "blue", "green", "pink", "purple", "orange", "cyan"))


# !!!!! SPOILER ALERT BEFORE REVEALING THE CORRECT ANSWER !!!!!
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
# true Ne was 6543!
```
:::



::: callout-note
#### Click to see the solution to "Option 2"

```{r, eval=RERUN}
# This is our starting observed AFS which we want to compare simulated AFS vectors to
afs_observed <- c(2520, 1449, 855, 622, 530, 446, 365, 334, 349, 244,
                  264, 218,  133, 173, 159, 142, 167, 129, 125, 143)

# Generate regularly spaced values of potential Ne values (our parameter grid)
Ne_grid <- seq(from = 1000, to = 30000, by = 500)
Ne_grid

# I'm not entirely sure if your workshop cloud instances support big
# parallelization runs -- if not, you can modify the `mc.cores =` argument
# a couple of lines below to a smaller number (`mc.cores = 1` would make
# the simulation run on a single processor, i.e. no parallelization).
library(parallel)

# Compute AFS (in parallel, to make things faster) across the entire grid of possible Ne values
afs_grid <- mclapply(Ne_grid, simulate_afs, mc.cores = detectCores())
names(afs_grid) <- Ne_grid

# Show the first five simulated AFS vectors, for brevity
afs_grid[1:5]



# Plot the observed AFS...
plot(afs_observed, type = "b", col = "black", lwd = 3, xlab = "allele count bin", ylab = "count")
# ... and overlay the simulated AFS vectors on top of it
for (i in seq_along(Ne_grid)) {
  lines(afs_grid[[i]], lwd = 0.5)
}
legend("topright", legend = c("observed AFS", "simulated AFS"), fill = c("black", "gray"))




# Compute mean-squared error of the AFS produced by each Ne value across the grid
errors <- sapply(afs_grid, function(sim_afs) {
  sum((sim_afs - afs_observed)^2) / length(sim_afs)
})

plot(Ne_grid, errors, ylab = "error")
abline(v = Ne_grid[which.min(errors)], col = "red")
legend("topright", legend = paste("minimum error Ne =", Ne_grid[which.min(errors)]), fill = "red")




# Plot the AFS again, but this time highlight the most likely spectrum
# (i.e. the one which gave the lowest RMSE value)
plot(afs_observed, type = "b", col = "black", lwd = 3, xlab = "allele count bin", ylab = "count")
for (i in seq_along(Ne_grid)) {
  color <- if (i == which.min(errors)) "red" else "gray"
  width <- if (i == which.min(errors)) 2 else 0.75
  lines(afs_grid[[i]], lwd = width, col = color)
}
legend("topright", legend = c("observed AFS", paste("best fitting Ne =", Ne_grid[which.min(errors)])),
       fill = c("black", "red"))


# !!!!! SPOILER ALERT BEFORE REVEALING THE CORRECT ANSWER !!!!!
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
# true Ne was 6543!
```
:::








Congratulations, you now know how to infer parameters of evolutionary models
using simulations! What you just did is really very similar to how
simulation-based inference is done in practice (even with methods such as ABC).
Hopefully you can also see how easy does _slendr_ make it.

This kind of approach can be used to infer
all sorts of demographic parameters, even using other summary statistics that
you've also learned to compute... including selection parameters, which we
delve into in the next exercise.
























# Simulating (and detecting) natural selection

The primary motivation for designing _slendr_ was to make demographic modelling
in R as trivially easy and fast as possible, focusing exclusively on neutral
models. However, as _slendr_ became popular, people have been asking for
the possibility of simulating natural selection. After all, a large
part of _slendr_'s functionality deals with population genetic [models across
geographical landscapes](https://www.slendr.net/articles/vignette-06-locations.html),
which requires SLiM. So why not support selection simulations using _slendr_
as well?

In December 2024 I caved in and added support for modifying
_slendr_ demographic models with bits of SLiM code, which allows simulating
pretty much any arbitrary selection scenario you might be interested in. 

This exercise is a quick demonstration of how this works and how you might
simulate selection using _slendr_. We will do this using another toy
model of ancient human history, which we will first use as a basis for simulating
the frequency trajectory of an allele under positive selection, and then implementing a toy selection scan using Tajima's D.

To speed things up, **create a new `exercise4.R` script and copy the following
code as a starting point for this exercise**:

```{r}
#| collapse: true
library(slendr)
init_env(quiet = TRUE)

# This line sources a script in which I provide a few useful helper functions
# which you can use in this exercise
source(here::here("utils.R"))

# African ancestral population
afr <- population("AFR", time = 65000, N = 5000)

# First migrants out of Africa
ooa <- population("OOA", parent = afr, time = 60000, N = 5000, remove = 27000)

# Eastern hunter-gatherers
ehg <- population("EHG", parent = ooa, time = 28000, N = 5000, remove = 6000)

# European population
eur <- population("EUR", parent = ehg, time = 25000, N = 5000)

# Anatolian farmers
ana <- population("ANA", time = 28000, N = 5000, parent = ooa, remove = 4000)

# Yamnaya steppe population
yam <- population("YAM", time = 8000, N = 5000, parent = ehg, remove = 2500)

# Define gene-flow events
gf <- list(
  gene_flow(from = ana, to = yam, rate = 0.75, start = 7500, end = 6000),
  gene_flow(from = ana, to = eur, rate = 0.5, start = 6000, end = 5000),
  gene_flow(from = yam, to = eur, rate = 0.6, start = 4000, end = 3500)
)

# Compile all populations into a single slendr model object
model <- compile_model(
  populations = list(afr, ooa, ehg, eur, ana, yam),
  gene_flow = gf, generation_time = 30
)

# Schedule the sampling from four European populations roughly before their
# disappearance (or before the end of the simulation)
schedule <- rbind(
  schedule_sampling(model, times = 0, list(eur, 50)),
  schedule_sampling(model, times = 6000, list(ehg, 50)),
  schedule_sampling(model, times = 4000, list(ana, 50)),
  schedule_sampling(model, times = 2500, list(yam, 50))
)
```

**Next, visualize the demographic model.** If you did a bit of work in human
population genetics, you might recognize it as a very simplified model
of demographic history of Europe over the past 50 thousand years or so.
As you can see, we are recording 50 individuals from four populations -- for
Europeans we sample 50 individuals at "present-day", for the remaining populations
we're recording 50 individuals just before their disappearance. Also note that
there's quite a bit of gene-flow! This was an important thing we've learned about
human history in the past 10 years or so -- everyone is mixed with pretty much
everyone, there isn't (and never was) anything as a "pure population".

::: {.aside}
**Note:** We didn't discuss it earlier, but _slendr_ also provides the option to
specify a `remove =` argument in a `population()` call which instructs the
simulation engine to delete a population from a simulation at a given point.
For our `msprime()` simulations in earlier examples it wasn't really important,
but for the `slim()` simulation we will be running below, we want to make a
population extinct at a certain timepoint. Which is why our ancient populations
in the starting script model have the `remove =` parameter specified.
:::

```{r}
#| fig-width: 6
#| fig-height: 6
plot_model(model, proportions = TRUE, samples = schedule)
```




### Part 1: Simulating a tree sequence and computing Tajima's D 

Although the point of this exercise is to simulate selection, let's first
simulate a normal neutral model using slendr's `msprime()` engine as a sanity
check. **Simulate 10 Mb of sequence with a recombination rate `1e-8` and a
sampling `schedule` defined above.** Let's not worry about adding any mutations, just to change things up a little bit. We'll be working with
branch-based statistics here (which means adding `mode = "branch"` whenever
we will be computing a statistic, such as Tajima's D).

::: callout-note
#### Click to see the solution

```{r}
ts <- msprime(model, sequence_length = 10e6, recombination_rate = 1e-8, samples = schedule)

ts # no mutations!
```
:::

**Inspect the table of all individuals recorded in our tree sequence using
the function `ts_samples()`, making sure we have all the individuals scheduled
for tree-sequence recording.** (Again, there's no such a thing as too many
sanity checks when doing research!)

::: callout-note
#### Click to see the solution

```{r}
ts_samples(ts)

library(dplyr)
ts_samples(ts) %>% group_by(pop, time) %>% tally
```
:::


As you've already learned in an earlier exercise, _tskit_ functions in _slendr_
generally operate on vectors
(or lists) of individual names, like those produced by `ts_names()` above.
**Get a vector of names of individuals in every population recorded in the
tree sequence, then use this to compute Tajima's D using the _slendr_ function
`ts_tajima()`.** (Use the same approach as you have with `ts_diversity()` or 
`ts_divergence()` above, using the list of names of individuals as the
`sample_sets =` argument for `ts_tajima()`). **Do you see any striking
differences in the Tajima's D values across populations? Check [this](https://en.wikipedia.org/wiki/Tajima%27s_D#Interpreting_Tajima's_D)
for some general guidance.**

::: callout-note
#### Click to see the solution

```{r}
samples <- ts_names(ts, split = "pop")
samples

# Compute genome-wide Tajima's D for each population -- note that we don't
# expect to see any significant differences because no population experienced
# natural selection (yet)
ts_tajima(ts, sample_sets = samples, mode = "branch")
```
:::




## Part 2: Computing Tajima's D in windows

Let's take this one step forward. Even if there is a locus under positive selection
somewhere along our chromosome, it might be quite unlikely that we would find a
Tajima's D value significant enough for the entire chromosome (which is basically
what we did in Part 1 now). Fortunately, thanks to the flexibility of
the _tskit_ module, the _slendr_ function  `ts_tajima()` has an argument
`windows =`, which allows us to specify the coordinates of windows into which
a sequence should be broken into, with Tajima's D computed separately for each
window. Perhaps this will allow us to see the impact of positive selection
after we get to adding selection to our model. So let's first built some code
towards that.

**Define a variable `windows` which will contain a vector of coordinates of
100 windows, starting at position `0`, and ending at position `10e6` (i.e., the end
of our chromosome). Then provide this variable as the `windows =` argument of
`ts_tajima()` on a new, separate line of your script. Save the result of
`ts_tajima()` into the variable `tajima_wins`, and inspect its contents in the
R console.**

**Hint:** You can use the R function `seq()` and its argument `length.out = 100`,
to create the coordinates of window boundaries very easily.

::: callout-note
#### Click to see the solution

```{r}
# Pre-compute genomic windows for window-based computation of Tajima's D
windows <- round(seq(0, ts$sequence_length, length.out = 100))
windows

# Compute genome-wide Tajima's D for each population in individual windows
tajima_wins <- ts_tajima(ts, sample_sets = samples, windows = windows, mode = "branch")
tajima_wins

# You can see that the format of the result is slightly strange, with the
# `D` column containing a vector of numbers (this is done for conciseness)
tajima_wins[1, ]$D
```
:::

The default output format of `ts_tajima()` is not super user-friendly. **Process
the result using a helper function `process_tajima(tajima_wins)` that I provided
for you (perhaps save it as `tajima_df`), and visualize it using another
of my helper functions `plot_tajima(tajima_df)`.**

::: aside
**Note:** Making the `process_tajima()` and `plot_tajima()` function available
in your R code is the purpose of the `source(here::here("utils.R"))` command
at the beginning of your script for this exercise.
:::

::: callout-note
#### Click to see the solution

```{r}
# The helper function `process_tajima()` reformats the results into a normal
# data frame, this time with a new column `window` which indicates the index
# of the window that each `D` value was computed in
tajima_df <- process_tajima(tajima_wins)
tajima_df

# Now let's visualize the window-based Tajima's D along the simulated genome
# using another helper function `plot_tajima()`
plot_tajima(tajima_df)
```

It's no surprise that we don't see any Tajima's D outliers in any of our
windows, because we're still working with a tree sequence produced by our
a purely neutral simulation. But we have everything set up for the next part,
in which we will add selection acting on a beneficial allele.

:::

















## Part 3: Adding positive selection to the base demographic model

Although primarily designed for neutral demographic models, _slendr_ allows
optional simulation of natural selection by providing a "SLiM extension code
snippet" with customization SLiM code as an optional argument `extension =`
of `compile_model()` (a function you're closely familiar with at this point).

Unfortunately we don't have any space to explain SLiM here (and I have no idea,
at the time of writing, whether or not you will have worked with SLiM earlier
in this workshop). Suffice to say that SLiM is another very popular population
genetic simulator software which allows simulation of selection, and which
requires you to write custom code in a different programming language called
Eidos.


**Take a look at the file `slim_extension.txt` provided in your working
directory (it's also part of the GitHub repository [here](https://github.com/bodkan/simgen/blob/main/slim_extension.txt)).
If you worked with SLiM before, glance through the script casually and see
if it makes any sense to you. If you have not worked with SLiM before,
look for the strange `{{elements}}` in curly brackets in the first ten lines
of the script.** Those are the parameters of the selection model we will be
customizing the standard neutral demographic model we started with in the next step.

Specifically, when you inspect the `slim_extension.txt` file, you can see
that this "SLiM extension script" I provided for you has three parameters:

- `origin_pop` -- in which population should a beneficial allele appear,
- `s` -- what should be the selection coefficient of the beneficial allele, and
- `onset_time` -- at which time should the allele appear in the `origin_pop`.

However, at the start, the SLiM extension snippet doesn't contain any concrete
values of those parameters, but only their `{{origin_pop}}`, `{{s}}`, and
`{{onset_time}}` placeholders.

**Use the _slendr_ function `substitute_values()` to substitute concrete values
for those parameters like this:**

```{r}
extension <- substitute_values(
  template = here::here("slim_extension.txt"),
  origin_pop = "EUR",
  s = 0.15,
  onset_time = 12000
)
extension
```

You can see that `substitute_values()` returned a path to a file. **Take a look
at that file in your terminal -- you should see each of the three `{{placeholder}}`
parameters replaced with a concrete given value.**

::: callout-note
#### Click to see the solution

Let's take a look at the first 15 lines of the extension file before and
after calling `substitute_values()`. We'll do this in R for simplicity, but
you can use `less` in plain unix terminal.

**Before -- see the {{placeholder}} parameters in their original form:**
```{r}
#| echo: false
cat(paste(readLines("slim_extension.txt")[1:11], collapse = "\n"))
```

**After -- see the {{placeholder}} parameters with concrete values!**
```{r}
#| echo: false
cat(paste(readLines(extension)[1:11], collapse = "\n"))
```
:::

And that's all the extra work we need to turn our purely neutral demographic
_slendr_ model into a model which includes natural selection! (In this case,
only a simple selection acting on a single locus, as you'll see later, but
this can be generalized to any imaginable selection scenario.)

How do we use the SLiM extension for our simulation? It's very simple -- we
just have to provide the `extension` variable as an additional argument of
good old `compile_model()`. This will compile a new _slendr_ model which will
now include the new functionality for simulating natural selection:

**Compile a new `model` of the history of populations `afr`, `ooa`, `ehg`, 
etc., by following the instructions above, providing a new `extension =`
argument to the `compile_model()` function.**

::: callout-note
#### Click to see the solution

```{r}
model <- compile_model(
  populations = list(afr, ooa, ehg, eur, ana, yam),
  gene_flow = gf, generation_time = 30,
  extension = extension   # <======== this is missing in the neutral example!
)
```
:::



## Part 4: Running a selection simulation using `slim()`

Now we can finally run our selection simulation!

There are two modifications to our previous simulation workflows:

1. Because we need to run a non-neutral simulation, we have to switch from using
the `msprime()` _slendr_ engine to `slim()`. The latter can still interpret the
same demographic model we programmed in R, just like the `msprime()` engine can,
but will run the model using SLiM (and thus leveraging the new SLiM extension code
that we have customized using `substitute_values()` above). We simply do this by
switching from this:

```{r}
#| eval: false
ts <- msprime(model, sequence_length = 10e6, recombination_rate = 1e-8, samples = schedule)
```

to this:

```{r}
#| eval: false
ts <- slim(model, sequence_length = 10e6, recombination_rate = 1e-8, samples = schedule)
```

As you can see, you don't have to modify anything in your model code, just
switching from `msprime` to `slim` in the line of code which produces the
simulation result.

2. The customized model will not only produce a tree sequence, but will
also generate a table of allele frequencies in each population (SLiM experts
might have noticed the revelant SLiM code when they were inspecting
[`slim_extension.txt`](https://github.com/bodkan/simgen/blob/main/slim_extension.txt)). We need to be able to load both of these files after
the simulation and thus need a path to a location we can find those files.
We can do this by calling the `slim()` function as `path <- slim(..., path = TRUE)`
(so with the extra `path =` argument). This will return a path to where the
`slim()` engine saved all files with our desired results.

**Run a simulation from the modified model of selection with the `slim()` engine
as instructed in points number 1. and 2. above, then use the `list.files(path)`
function in R to take a look in the directory. Which files were produced by
the simulation?**

::: callout-note
#### Click to see the solution (you have a working SLiM installation)

```{r, eval=RERUN}
# tstart <- Sys.time()
path <- slim(model, sequence_length = 10e6, recombination_rate = 1e-8, samples = schedule, path = TRUE, random_seed = 59879916)
# tend <- Sys.time()
# tend - tstart # Time difference of 38.82014 secs

# We can verify that the path not only contains a tree-sequence file but also
# the table of allele frequencies.
list.files(path)
```

We can see that the `slim()` simulation generated a tree-sequence file (just
like in previous exercises focused on `msprime()`) but it also created a new
file -- this was done by the SLiM customization snippet we provided to
`compile_model()`.
:::

::: callout-note
#### Click to see the solution (you don't have a working SLiM installation _or_ the simulation takes too long)

```{r}
# If you don't have SLiM set up, just use the simulated results from my own
# run of the same simulation
path <- here::here("data/selection")

# We can verify that the path not only contains a tree-sequence file but also
# the table of allele frequencies.
list.files(path)
```

We can see that the `slim()` simulation generated a tree-sequence file (just
like in previous exercises focused on `msprime()`) but it also created a new
file -- this was done by the SLiM customization snippet we provided to
`compile_model()`.
:::

## Part 5: Investigating allele frequency trajectories

**Use another helper function `read_trajectory(path)` which I provided for this
exercise to read the simulated frequency trajectories of the positively
selected mutation in all of our populations into a variable `traj_df`. Then
run a second helper function `plot_trajectory(traj_df)` to inspect the trajectories
visually.**

**Recall that you used the function `substitute_values()` to parametrize your
selection model so that the allele under selection occurs in Europeans 15 thousand
years ago, and is programmed to be under very strong selection of $s = 0.15$.
Do the trajectories visualized by `plot_trajectory()` make sense given the
demographic model of European prehistory plotted above?**

::: callout-note
#### Click to see the solution

```{r}
traj_df <- read_trajectory(path)
traj_df

plot_trajectory(traj_df)

# Comparing the trajectories side-by-side with the demographic model reveals
# some obvious patterns of both selection and demographic history.
plot_grid(
  plot_model(model),
  plot_trajectory(traj_df),
  nrow = 1, rel_widths = c(0.7, 1)
)
```

We can see that the beneficial allele which appeared in the European population
was under _extremely strong selection_ (look how its allele frequency shoots
up immediately after its first appearance!). However, we can also se how
the following demographic history with multiple admixture events kept "diluting"
the allele frequency (indicated by the dips in the trajectory).

This is the kind of _slendr_ simulation which could be also very useful for simulation-based
inference, like we did in the previous exercise. Just imagine having a comparable
aDNA time series data with empirical allele frequency trajectory over time and
using it in an ABC setting!
:::

## Part 6: Tajima's D (genome-wide and window-based) from the selection model

Recall that your simulation run saved results in the location stored in the
`path` variable:

```{r}
list.files(path)
```

From this `path`, we've already successfuly investigated the frequency trajectories.

Now let's compute Tajima's D on the tree sequence simulated from our selection
model. Hopefully we should see an interesting pattern in our selection scan?
For instance, we don't know yet _where_ in the genome is the putative locus
under selection!

To read a tree sequence simulated with `slim()` by our customized selection setup,
we need to do a bit of work. To simplify things a bit, here's the R code which makes
it possible to do. Just copy it in your `exercise4.R` script as it is:

```{r}
# Let's use my own saved simulation results, so that we're all on the
# same page going forward
path <- here::here("data/selection")

ts <-
  file.path(path, "slim.trees") %>%  # 1. compose full path to the slim.trees file
  ts_read(model) %>%                 # 2. read the tree sequence file into R
  ts_recapitate(Ne = 5000, recombination_rate = 1e-8) # 3. perform recapitation
```

Very briefly, because our tree sequence was generated by SLiM, it's very likely
that not all genealogies along the simulated genome will be fully coalesced
(i.e., not all tree will have a single root). To explain why this is the case
is out of the scope of this session, but read [here](https://tskit.dev/pyslim/docs/latest/tutorial.html) if you're interested
in learning more. For the time being, it suffices to say that we can pass the
(uncoalesced) tree sequence into the `ts_recapitate()` function, which then
takes a SLiM tree sequence and simulates all necessary "ancestral history" that
was missing on the uncoalesced trees, thus ensuring that the entire tree
sequence is fully coalesced and can be correctly computed on.

**Now that you have a `ts` tree sequence object resulting from a new selection
simulation run, repeat the analyses of genome-wide and window-based Tajima's D
from _Part 1_ and _Part 2_ of this exercise, again using the provided helper
functions `process_tajima()` and `plot_tajima()`. Can you identify which locus
has been the likely focal point of the positive selection? Which population
shows evidence of selection? Which doesn't and why (look again at the 
visualization of the demographic model above)?**

::: callout-note
#### Click to see the solution

```{r}
samples <- ts_names(ts, split = "pop")
samples

# Overall Tajima's D across the 10Mb sequence still doesn't reveal any significant
# deviations even in case of selection (again, not entirely unsurprising)
ts_tajima(ts, sample_sets = samples, mode = "branch")
```


```{r}
# So let's look at the window-based computation again...
windows <- as.integer(seq(0, ts$sequence_length, length.out = 100))

# compute genome-wide Tajima's D for each population in individual windows
tajima_wins <- ts_tajima(ts, sample_sets = samples, windows = windows, mode = "branch")
tajima_df <- process_tajima(tajima_wins)

plot_tajima(tajima_df)
```

You should see a clear dip in Tajima's D around the midpoint of the DNA sequence,
but only in Europeans. The beneficial allele appeared in the European population,
and although the plot of the allele frequency trajectories shows that the selection
dynamics has been _dramatically_ affected by gene-flow events (generally causing
a repeated "dilution" of the selection signal in Europeans), there has never been
gene-flow (at least in our model) _from_ Europeans to other populations, so the
beneficial allele never had a chance to "make it" into those populations.

:::










:::::: callout-tip
## Bonus exercises

#### Bonus 1: Investigate the impact of recombination around the selected locus

Vary the uniform recombination rate and observe what happens with Tajima's D
in windows along the genome.

::: callout-note
#### Click to see the solution

Solution: just modify the value of the `recombination_rate =` argument provided
to the `slim()` function above.
:::





#### Bonus 2: Simulate origin of the allele in EHG


Simulate the origin of the beneficial allele in the EHG population -- what
do the trajectories look like now? How does that change the Tajima's D
distribution along the genome in our European populations?

::: callout-note
#### Click to see the solution

Use this extension in the `slim()` call, and repeat the rest of the
selection-based workflow in this exercise.

```{r}
#| eval: false
extension <- substitute_values(
  template = "slim_extension.txt",
  origin_pop = "EHG",
  s = 0.1,
  onset_time = 12000
)
model <- compile_model(
  populations = list(afr, ooa, ehg, eur, ana, yam),
  gene_flow = gf, generation_time = 30,
  extension = extension
)
```
:::



#### Bonus 3: Other statistics in windows

As a practice of your newly acquired tree-sequence computation skills with
_slendr_, calculate some other statistics in the same windows along the
simulated genome, visualize them yourself, and compare the results to the
window-based Tajima's D pattern. For instance, `ts_diversity()`, `ts_divergence()`,
or `ts_segregating()` might be quite interesting to look at.

::: callout-note
#### Click to see the solution

Use the same tree sequence file you've computed Tajima's D on, and then
apply the functions `ts_diversity()`, `ts_divergence()`, and `ts_segregating()`
on that tree sequence.
:::




:::
<!-- End of Bonus exercises -->
























<hr>


# Playing around with PCA patterns

**Note:** We're unlikely to make it to this part, so this is extra bonus which
can be ignored!

In earlier lectures you've learned about the
[Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis),
a dimensional-reduction technique that's a central piece of many (if not most)
papers studying population history of many (hundreds) of individuals genotyped
across many (millions) SNPs.

Although incredibly popular, PCA has recently been a topic of controversy, with
a [highly combative](https://www.nature.com/articles/s41598-022-14395-4) paper
criticizing the application of PCA in all of population genetics, almost to a
point of claiming that none of the results can be published. As an example,
take a look at [Figure 5](https://www.nature.com/articles/s41598-022-14395-4#Fig5)
in the paper, demonstrating that an Indian population can be arbitrarily placed
in proximity to Europeans, East Asians, and Africans, simply by wiggling the
sample sizes of all groups before they applying PCA on their genotype data.

This activity session is not a place to debate the merit of PCA (or any other
popgen method for that matter). What we can do (as we've done in previous
exercises) is to show how _slendr_ can be used in a very easy way to evaluate
patterns, expected behavior, and overall dynamics of many popgen metrics in
a controlled settings -- and in this way, verify our intuition and check the
robustness of a method in question -- using PCA as another example. We'll be
specifically looking at patterns which arise in PCA depending on the exact
spatio-temporal sampling from a demographic model.



## Part 1: Create a model

**Start a new script named `exercise5.R` with the following setup of another
toy demographic model:**

```{r}
#| collapse: true
library(slendr)
init_env(quiet = TRUE)

source(here::here("utils.R"))

popA <- population("popA", time = 3000, N = 5000)
popB <- population("popB", time = 1500, N = 5000, parent = popA)
popC <- population("popC", time = 1500, N = 5000, parent = popA)

model <- compile_model(list(popA, popB, popC), generation_time = 1)

schedule <- schedule_sampling(model, times = seq(3000, 0, by = -200),
                              list(popA, 10), list(popB, 10), list(popC, 10))

plot_model(model, proportions = TRUE, samples = schedule)
```

As you can see, this model describes the demographic history of three populations:
one ancestral population "popA" starting at 3000 generations ago, which splits
into two populations "popB" and "popC" the same time at 1500 generations ago.
We den instruct _slendr_ to record 10 individuals from each of the three populations
starting from 3000 generations ago all the way to 0 generations ago (i.e.
the "present"), every 200 generations (remeber the `seq()` R function!).

## Part 2: Simulate a (mutated!) tree sequence

To be able to run PCA using the [_smartsnp_ R package](https://christianhuber.github.io/smartsnp/)
(below), we will need to simulate data in the
[EIGENSTRAT](https://bodkan.net/admixr/articles/01-tutorial.html#a-note-about-eigenstrat-format)
file format. And to do _that_, we need our tree sequence with mutations.

Recall that all of our previous exercises managed to do away with mutations
completely, owing to the amazing nature of the succint tree sequence data structure
invented by the people behind the _tskit_ project. However, all traditional
popgen software and tools still rely on genotype data, which is why we now
have to simulate mutations as well. Luckily, this is very easy -- instead
of the traditional

```{r}
#| eval: false
ts <- msprime(model, sequence_length = ..., recombination_rate = ..., samples = ...)
```

we will run this:

```{r}
#| eval: false
# First run a normal msprime simulation creating a tree-sequence object, then
# directly pipe it into a function which adds (neutral!) mutations to it
ts <- msprime(model, sequence_length = ..., recombination_rate = ..., samples = ...) %>%
  ts_mutate(mutation_rate = ...)
```

which is equivalent to running this without the `%>%` "pipe operator":

```{r}
#| eval: false
# First run a normal msprime simulation creating a tree-sequence object...
ts_nomuts <- msprime(model, sequence_length = ..., recombination_rate = ..., samples = ...)
# ... then add (neutral!) mutations to it
ts <- ts_mutate(ts_nomuts, mutation_rate = ...)
```

With that out of the way, **simulate a tree sequence from the popA/popB/popC `model`
above, which will be 50 Mb (50e6) long, with a recombination rate 1e-8, and overlay
mutations on it at a rate 1e-8. Check that it has mutations either by typing out `ts`
in the console and looking for a "Mutations" section of the summary, or by using
the function `ts_table(ts, "mutations")`. Then count how many individuals you have
recorded for each population using the table produced by `ts_samples()`.**

::: callout-note
### Click to see the solution
```{r}
# First run a normal msprime simulation creating a tree-sequence object, then
# directly pipe it into a function which adds (neutral!) mutations to it
ts_nomuts <- msprime(model, samples = schedule, sequence_length = 50e6, recombination_rate = 1e-8, random_seed = 1702182272)

# Notice we have no mutations on the tree sequence, just as before...
ts_nomuts

ts <- ts_mutate(ts_nomuts, mutation_rate = 1e-8)

# ... but we have them now!
ts

# Get the table of individuals (and process it a bit for tidier plotting later)
samples <- ts_samples(ts) %>% mutate(pop = factor(pop, levels = c("popA", "popB", "popC")))

# Count how many individuals do we have for each population
samples %>% group_by(pop) %>% count()
```
:::

## Part 3: Converting a tree sequence into EIGENSTRAT

The function to use for converting a tree-sequence object we have in R (in our
exercises the thing we usually had in the `ts` variable) to disk in form of
genotypes in the EIGENSTRAT format is called `ts_eigenstrat()`. The standard
way to call it (but see `?ts_eigenstrat` for more options) is like this:

```{r}
#| eval: false
ts_eigenstrat(ts, prefix = "path/to/a/desired/EIGENSTRAT/prefix")
```

Which creates three files `.ind`, `.snp`, and `.geno` as:
- `path/to/a/desired/EIGENSTRAT/prefix.ind`,
- `path/to/a/desired/EIGENSTRAT/prefix.snp`, and
- `path/to/a/desired/EIGENSTRAT/prefix.geno`,

just as you would expect for any EIGENSTRAT file.

**Take your tree sequence `ts` just just simulated, and convert it to EIGENSTRAT
format under the prefix `data/ABC_all`.**

::: callout-note
### Click to see the solution
```{r}
ts_eigenstrat(ts, "data/ABC_all")
```
:::

**Check that the EIGENSTRAT files really appeared at the path that you specified
(in the terminal).**

## Part 4: Inspect the EIGENSTRAT data produced by _slendr_

Years ago I developed a small R package to help me with $f$-statistics based
projects using the ADMIXTOOLS software (which operates on data in the
EIGENSTRAT file format), called [admixr](https://bodkan.net/admixr/)

**Use the following code to examine one of the EIGENSTRAT data sets you've
just created. Just look at the results and see if they make sense in terms
of what you've learned about this in earlier lectures.**

```{r}
#| eval: false
library(admixr)

eigen <- eigenstrat("<prefix of a trio of EIGENSTRAT .ind/.snp/.geno files")

# Print out a summary of the EIGENSTRAT data
eigen

# Read the .ind file as a table into R
read_ind(eigen)
# Read the .snp file as a table into R
read_snp(eigen)
# Read the .geno file as a table into R
read_geno(eigen)
```

::: callout-note
### Click to see the solution

```{r}
#| eval: false
library(admixr)

eigen <- eigenstrat("data/ABC_all")

# Print out a summary of the EIGENSTRAT data
eigen

# Read the .ind file as a table into R
read_ind(eigen)
# Read the .snp file as a table into R
read_snp(eigen)
# Read the .geno file as a table into R
read_geno(eigen)
```
:::


## Part 5: Principal Component Analysis on the entire simulated data set

Now, at long last, we have everything we need to be able to run ABC on the
data generated by our _slendr_ model. To avoid making this exercise even longer,
I provided a helper function for you called `plot_pca()`. But this function
isn't doing anything magical -- it uses the [_smartsnp_ R package](https://christianhuber.github.io/smartsnp/)
to compute the principal components and visualize the results using _ggplot2_.
This is something many of you could do given enough time but we want to focus
on simulations and PCA, not pure R coding. If you're interested, take a look
at my implementation of `plot_pca()` [here](https://github.com/bodkan/simgen/blob/main/utils.R#L15).

Here's how you can use this function (remeber that you need to put
`source(here::here("utils.R"))` into your script!):

1. Plot PCA while coloring each individual by their population assignment:

```{r}
#| eval: false
plot_pca("path/to/prefix", <tree sequence used to create EIGENSTRAT>, color_by = "pop")
```

2. Plot PCA while coloring each individual by their time of sampling:

```{r}
#| eval: false
plot_pca("path/to/prefix", <tree sequence used to create EIGENSTRAT>, color_by = "time")
```

3. By default, the function plots PC 1 vs PC 2, but you can customize things by
providing an optional argument `pc =` like this:

```{r}
#| eval: false
plot_pca("path/to/prefix", <tree sequence used to create EIGENSTRAT>, color_by = "pop", pc = c(2, 3))
```

**Use the provided `plot_pca()` function to run PCA based on genotypes for all
recorded individuals that you just converted as EIGENSTRAT `"data/ABC_all"` from
the `ts` tree sequence. Visualize PC 1 vs PC 2 by first ccolor each individual
by their population label (`color_by = "pop"`) then by the time of their sampling
(`color_by = "time"`).**

**Does the PCA of PC 1 vs PC 2 capture the relationship between all individuals
across the populations _and across time_?**

::: callout-note
### Click to see the solution

```{r}
plot_pca("data/ABC_all", ts, color_by = "pop", pc = c(1, 2))
plot_pca("data/ABC_all", ts, color_by = "time", pc = c(1, 2))
```

It looks like the PCA from PC 1 vs 2 cannot "partition out" the drift along
the ancestral "popA" lineage prior to the population splits!
:::

**Use `plot_pca()` to compute the PCAon this exact same data, but examine how does
the shape of the PCA scatterplot change when you switch the pairs of PCs plotted
(i.e., PC 2 vs PC 3, PC 3 vs PC 4, PC 4 vs PC 6, etc.). Which pair of PCs does
the best job at recapitulating the demographic model?**

::: aside
**Note:** We're doing this purely for educational purposes and for fun, using
an extremely idealistic demographic model which is perfectly known (by definition,
because we simulated it) and perfect sampling scheme. The point is to explore
what does doing a PCA mean in practice, visually, and to built intuition into it.
:::

::: callout-note
### Click to see the solution

```{r}
#  We can see that the overall shape of the demographic model tree is now nicely
# reflected in the PCA shape
plot_pca("data/ABC_all", ts, color_by = "pop", pc = c(2, 3))
plot_pca("data/ABC_all", ts, color_by = "time", pc = c(2, 3))

plot_pca("data/ABC_all", ts, color_by = "pop", pc = c(3, 4))
plot_pca("data/ABC_all", ts, color_by = "time", pc = c(3, 4))

# Things are getting progressively wilder! 
plot_pca("data/ABC_all", ts, color_by = "pop", pc = c(4, 5))
plot_pca("data/ABC_all", ts, color_by = "time", pc = c(4, 5))

# ...
plot_pca("data/ABC_all", ts, color_by = "pop", pc = c(5, 6))
plot_pca("data/ABC_all", ts, color_by = "time", pc = c(5, 6))
```

:::





































































::: callout-tip
## Bonus exercises






### Bonus 1: Tree-sequence simplification and EIGENSTRAT conversion

One of our goals in this exercise was to investigate how does the shape of
a PCA look like based on the sampling of individuals across populations and
also across time -- all of that from the _same_ demographic history. In order
to do that, we need to be able to select only a defined subset of individuals
from a given tree sequence. Which brings us to the last tree-sequence processing
function in _slendr_ callsed `ts_simplify()`. Implemented on top of the
`simplify()` method in _tskit_, it has a very simple interface:

```{r}
#| eval: false
ts_small <- ts_simplify(ts_big, simplify_to = c(<subset of individuals as a vector>))
```

This function call creates a _new tree sequence_, which is smaller and only
contains a those individuals whose names were specified in `simplify_to =`
(again, we're talking about the "symbolic names" of individuals, such as
"NEA_1", "AFR_42", etc., not integer numbers of _tskit_ nodes).

Whenever you want to create smaller subsets of a large tree sequence, it is
often helpful to work with the table of all individuals in the original tree
sequence, because it contains every individual's `name`, `pop` assignment and
the `time` in which it lived, so let's save it for further use now:

```{r}
samples <- ts_samples(ts)

nrow(samples)
```

For instance, we can get only individuals from "popB" and "popC" sampled at
the present using this code:

```{r}
subset <- filter(samples, pop %in% c("popB", "popC"), time == 0)

nrow(subset)
```

You know that the table of samples contains the `name` of each individual,
which you can access as `subset$name`. **Use the `ts_simplify()` function to
create a new tree sequence called `ts_BC0` which contains only this subset
of individuals. Check that it really does contain only the defined subset
of individuals using `ts_samples(ts_BC0)`.**

::: callout-note
### Click to see the solution

```{r}
ts_BC0 <- ts_simplify(ts, simplify_to = subset$name)

ts_samples(ts_BC0)
```
:::

When you have a smaller tree sequence like this, you can convert it to an
EIGENSTRAT file format using `ts_eigenstrat()` just like we did above.


### Part 2: PCA visualization on subsets of individuals

**TODO**: Try to replicate some features of
[the results](https://www.nature.com/articles/s41598-022-14395-4#Fig5)
discovered for EUR/ASIAN/AFR/Indian populations.


:::






<!-- End of Bonus exercises -->






























## Spatial PCA


**WIP:** Exploring how (and when) PCA patterns recapitulate geographical history of populations, inspired by this figure
by Novembre _et al._ (2008).

![Novembre et al., 2008](images/novembre2008a.jpg)

```{r}
library(slendr)
init_env()

library(admixr)
library(cowplot)

source("utils.R")

model <- landscape_model(rate = 0.3, Ne = 10000)

# pdf("exercise5.pdf", width = 8, height = 5)
# for (n in c(50, 25, 10, 5, 2, 1)) {
  n <- 10
  schedule <- landscape_sampling(model, n)

  ts <- msprime(model, samples = schedule, sequence_length = 20e6, recombination_rate = 1e-8) %>% ts_mutate(1e-8)

  samples <- ts_samples(ts)

  ts_eigenstrat(ts, "data/spatial_pca1")

  plot_pca("data/spatial_pca1", ts, model = "map", color_by = "pop") 

# }
# dev.off()
```






### Per-population Ne values

```{r}
library(slendr)
init_env()

library(admixr)
library(cowplot)

source("utils.R")

Ne <- list(
  p1 = 10000,
  p2 = 10000,
  p3 = 10000,
  p4 = 10000,
  p5 = 100,
  p6 = 10000,
  p7 = 10000,
  p8 = 10000,
  p9 = 10000,
  p10 = 10000
)

model <- landscape_model(rate = 0.3, Ne = Ne)

n <- list(
  p1 = 50,
  p2 = 50,
  p3 = 50,
  p4 = 50,
  p5 = 5,
  p6 = 50,
  p7 = 50,
  p8 = 50,
  p9 = 50,
  p10 = 50
)

schedule <- landscape_sampling(model, n)

ts <- msprime(model, samples = schedule, sequence_length = 20e6, recombination_rate = 1e-8) %>% ts_mutate(1e-8)

ts_eigenstrat(ts, "data/spatial_pca2")

read_ind(eigenstrat("data/spatial_pca2"))
ts_samples(ts)

plot_pca("data/spatial_pca2", ts, model = "map", color = "pop")
```























# Ancestry tracts and chromosome painintg

**WIP:** Extracting ancestry tracts, chromosome painting, and using
admixture tracts and LD patterns for dating admixture events.

```{r}
library(slendr)
init_env()

library(cowplot)

source("utils.R")

popZ <- population("popZ", time = 3000, N = 5000)
popX <- population("popX", time = 1500, N = 5000, parent = popZ)
popY <- population("popY", time = 1500, N = 5000, parent = popZ)

gf <- gene_flow(from = popX, to = popY, rate = 0.2, start = 800, end = 799)

model <- compile_model(list(popZ, popX, popY), generation_time = 1, gene_flow = gf)

schedule <- rbind(
  schedule_sampling(model, times = seq(1500, 0, by = -100), list(popY, 50)),
  schedule_sampling(model, times = 0, list(popX, 10), list(popZ, 10))
)

# Use the function plot_model() to make sure that the model and the sampling schedule
# are defined correctly (there's no such thing as too many sanity checks when doing research)
plot_model(model, proportions = TRUE, samples = schedule)

ts <- msprime(model, samples = schedule, sequence_length = 100e6, recombination_rate = 1e-8)


library(dplyr)
library(ggplot2)

tracts <- ts_tracts(ts, census = 800)
tracts

# Select sampled individuals from different ages (remember, we recorded oldest individuals
# at 1000 generations ago, the youngest individuals at 0 generations ago). Use the function
# plot_tracts to visualize their ancestry tracts. What do you see in terms of the number of
# tracts and the length of tracts across these individuals? Can you eyeball some distinctive
# pattern?

subset_inds <- tracts %>% group_by(time) %>% distinct(name) %>% slice_sample(n = 1) %>% pull(name)
subset_inds

plot_tracts(tracts, subset_inds)

# It looks like the older individual is, the closer they lived to the start of the admixture event,
# and the longer the tracts they carry will be. Compute the average length of a ancestry tracts in each
# sample age group and visualize the length distribution of these tracts based on their age:

tracts %>%
  group_by(time) %>%
  summarise(mean(length))

ggplot(tracts, aes(length, color = factor(time))) +
  geom_density() +
  coord_cartesian(xlim = c(0, 3e6)) +
  theme_bw()


# Now, try to work backwards. Assuming you have the following distribution of tract lengths...

tracts <- filter(tracts, time == 0, length <= 1e6)
bins <- hist(tracts$length, breaks = 50, plot = FALSE)
length <- bins$mids
density <- bins$density

plot(length, density)

lambda_mle <- 1 / mean(tracts$length)
lambda_mle / 1e-8

y_mle <- dexp(length, rate = lambda_mle)
lines(length, y_mle, lty = 2, col = "darkgreen", lwd = 2)

nls_res <- nls(density ~ SSasymp(length, Asym, R0, lrc))
nls_res

lambda_nls <- exp(unname(coef(nls_res)["lrc"]))
lambda_nls / 1e-8

y_nls <- predict(nls_res, newdata = data.frame(length = length))
lines(length, y_nls, lty = 2, col = "purple", lwd = 2)

legend("topright", fill = c("darkgreen", "purple"),
       legend = c(paste("MLE, t =", round(lambda_mle / 1e-8, 1), "generations ago"),
                  paste("MLE, t =", round(lambda_nls / 1e-8, 1), "generations ago")))

```








