# Introduction to _tidyverse_

## Packages introduced in this session

Before we begin, let's introduce some crucial parts of the _tidyverse_
ecosystem, which we will be using extensively during exercises in this chapter
(and throughout the remaining chapters).

1. [_dplyr_](https://dplyr.tidyverse.org) is a centerpiece of the entire R data science universe, providing
important functions for data manipulation, data summarization, and filtering
of tabular data. Many data-frame operations that were annoying or
cumbersome to do during the R bootcamp session are easy and natural to
do with _dplyr_.

2. [_readr_](https://readr.tidyverse.org) provides useful functions for reading
(and writing) tabular data. Think of it as a set of better alternatives to
base R functions such as `read.table()`. (Another very useful package
is [_readxl_](https://readxl.tidyverse.org) which is useful for working with
Excel data).

**Every single script you will be writing in this session will therefore begin
with these three lines of code, which load functions from these two packages.**

```{r}
#| message: false
library(dplyr)
library(readr)
```

## Our example data set

Let's also introduce a second star in this session, our example data set.
The following command will read a metadata table with information about
individuals published in a recent aDNA paper on the history or the Holocene in
West Eurasia, dubbed "MesoNeo"
([reference](https://www.nature.com/articles/s41586-023-06865-0)). **Feel
free to take a couple of minutes to skim the paper to get an idea about
the sort of data we will be using during our exercises.**

If you work with anything related to human
history, you'll probably become closely familiar with this study at some point, so it's useful for that reason too!

**We will read the metadata of individual sequenced as part of this paper using
the `read_tsv()` function (from the above-mentioned _readr_ package),
and store the metadata table in a variable `df`.** Everything in the exercises in
this session will
revolve around working with this data because it represents an excellent
example of the kind of metadata table you will find across the entirety of
computational population genomics, archaeology, ecology, and other fields
which deal with spatial and temporal data.

Yes, this function is quite magical -- it
can read stuff from a file stored on the internet. **If you're curious about
the file itself, just paste the URL address in your browser. Even though
R gives us superpowers in analyzing data, it's never a bad thing to rely
on more mundane ways to look at the information we're dealing with.**

```{r}
#| message: false
df <- read_tsv("https://tinyurl.com/simgen-metadata")
```

---

In addition to the brief overview from the slides for this exercise session
and looking up documentation to functions via R's `?` help system,
I highly encourage you to refer to [this](https://r4ds.hadley.nz/data-transform.html)
chapter of the _R for Data Science_ text book. If you ever need help, it's a
good idea to get into the habit of referring to primary resources. I keep
this book at hand every time I work with R. You can also refer to the
[_dplyr_ cheatsheet](cheatsheets/dplyr.pdf).

---

**Now, begin by creating a new R script in RStudio, (`File` `->` `New file` `->` `R Script`) and
save it somewhere on your computer as `tidy-basics.R` (`File` `->` `Save`). Put
the `library()` calls and the `read_tsv()` command above in this script, and
let's get started!**

## General workflow

In this session, you should try to experiment with code to solve each
little exercise. Make sure you understand how to solve the problem, either
by writing bits and pieces into your `tidy-basics.R` script (evaluating
your code line-by-line to check that it works), or experimenting with
bits of code in your R console and only recording your solution to your
script when you solved it.

In practice, you'll be using both approaches.

## A selection of data-frame inspection functions

**Whenever you get a new source of data, like a table from a collaborator, or
a data sheet downloaded from supplementary materials of a paper (our situation
in this session!), you need to familiarize yourself with it before you do
anything else.**

Here is a list of functions that
you will be using constantly when doing data science to answer the following
basic questions about your data.

1. _"How many observations (rows) and variables (columns) does my data have?"_ -- `nrow()`, `ncol()`

2. _"What variable names (columns) am I going to be working with?"_ -- `colnames()`

3. _"What data types (numbers, strings, logicals, etc.) does it contain?"_ -- `str()`, or better, `glimpse()`

4. _"How can I take a quick look at a subset of the data (first/last couple of
rows)?"_ -- `head()`, `tail()`

5. _"For a specific variable column, what is the distribution of values I can
expect in that column?"_ -- `table()` for "categorical types" (types which take
only a couple of discrete values), `summary()` for
"numeric types", `min()`, `max()`, `which.min()`, `which.max()`. Remember
that you can get values of a given `col` in a data frame `df` by using
the named-list syntax of `df$col`!


**Note:** Feel free to use this list as another cheatsheet of sorts! Also, don't forget to use the
`?function` command in the R console to look up the documentation to see
the possibilities for options and additional features, or even just to
refresh your memory. Every time I look up the `?` help for a function I've
been using for decade, I always learn new tricks.

---

**Before we move on, note that when you type `df` into your R console,
you will see a slightly different format of the output than when we worked
with plain R data frames in the previous chapter.** This format of data frame
data is called a ["tibble"](https://tibble.tidyverse.org) and represents
_tidyverse_'s more user friendly and modern take on data frames. For almost
all practical purposes, from now on, we'll be talking about _tibbles_ as
data frames (tibbles behave the same way, for almost all practical purposes).

---

## Exercise 1: Exploring new data

**Inspect the `df` metadata table using the functions above, and try to
answer the following questions using the functions from the list above
(you should decide which will be appropriate for which question).**

Before you use one of these functions for the first time, take a moment to
skim through its `?function_name`, just to build that habit of not forgetting
that the help pages are always there.


---

**What variables (i.e., columns) do we have in our data? What do the
first couple of rows of the data look like (i.e., what form does the
data take)?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Column names function:

```{r}
colnames(df)
```
:::

---

**How many individuals do we have metadata for?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Number of rows:

```{r}
nrow(df)
```

Number of `unique()` sample IDs (this should ideally always give the same number,
but there's never enough sanity checks in data science):

```{r}
length(unique(df$sampleId))
```
:::

---

**What data types (numbers, strings, logicals, etc.) are our variables of?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

I tend to use `str()` because I'm old and very used to it, but `glimpse()`
is new and better. You can see the data types in the second column after `:`.
("chr" -- character, "num" -- numeric, etc., "dbl" -- double).

```{r}
str(df)

glimpse(df)
```
:::

---

**What are the variables we have that describe the ages (maybe look at those
which have "age" in their name you detected with the `columns()` function earlier)?
Which one would you use to get information
about the ages of most individuals in terms of least number of missing `NA`
values? What is the distribution of dates
of aDNA individuals? What information does the `groupAge` column probably
contain?**

**Hint:** Remember that for a column/vector `df$COLUMN_NAME`, `is.na(df$COLUMN_NAME)`
gives you `TRUE` for each `NA` element in that column variable, and
`sum(is.na(df$COLUMN_NAME))` counts the number of `NA`. Alternatively,
`mean(is.na(df$COLUMN_NAME))` counts the proportion of `NA` values.

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

From `colnames(df)` above we see a number of columns which seem to have something
todo with "age":

```{r}
columns <- colnames(df)
columns
```

We can get those precisely using this trick (don't worry about this weird
command, I'm using it to show you the relevant columns -- I'm happy to explain
if you're interested!):

```{r}
grep("^age.*|^.*Age$", columns, value = TRUE)
```

Now let's take a look at the values available in each of them:

```{r}
mean(!is.na(df$age14C))
mean(!is.na(df$ageHigh))
mean(!is.na(df$ageLow))
mean(!is.na(df$ageAverage))
mean(!is.na(df$ageRaw))
unique(df$groupAge)
```

Interesting, it looks like the `ageAverage` variable has the highest proportion
of non-`NA` values at about `r 100 * round(mean(!is.na(df$ageAverage)), 4)`%.
We also seem to have another column, `groupAge` which clusters individuals into
three groups. We'll stick to these two variables whenever we have a question
regarding a date of an individual.

:::

---

**How many "ancient" individuals do we have in our data?
How many "modern" (i.e., present-day humans) individuals?**

**Hint:** Use the fact that for any vector (and `df$groupAge` is a vector,
remember!), we can get the number of elements of that vector matching a certain
value by applying the `sum()` function on a "conditional" expression, like
`df$groupAge == "Ancient"` (which, on its own, gives `TRUE` / `FALSE` vector).
In other words, in a `TRUE` / `FALSE` vector, each `TRUE` represents 1, and
each `FALSE` represents 0. Obviously then, summing up such vector simply adds
all the 1s together.

**Hint:** Maybe the `table()` function is even more useful here?

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Get logical vector of which individual's `groupAge` value is equal to "Ancient":

```{r}
which_ancient <- df$groupAge == "Ancient"
```

Indeed this gave us a logical vector:

```{r}
head(which_ancient)
```

Which is equivalent to 1 and 0 vector under the hood:

```{r}
head(as.integer(which_ancient))
```

And this means we can sum it like this:

```{r}
sum(which_ancient)
```

We can do the same for "Modern" and "Archaic" individuals:

```{r}
which_modern <- df$groupAge == "Modern"
sum(which_modern)
```

```{r}
which_archaic <- df$groupAge == "Archaic"
sum(which_archaic)
```

The above was a useful practice, but `table()` is probably better here:

```{r}
table(df$groupAge)
```

:::

---

**Who are the mysterious "Archaic" individuals? What are their names (`sampleId`
column) and which publications they come frome (`dataSource` column)?
Use your data-frame row- and column-indexing knowledge from the R
bootcamp session here.**

**Hint:** We need to _filter_ our table down to rows which have
`groupAge == "Archaic"`. This is an indexing operation which you learned about
in the R bootcamp session! Remember that data frames can be indexed into along
two dimensions: rows and columns. You want to filter by the rows here using
a logical vector obtained by `df$groupAge == "Archaic"`.

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Again, this gets us a logical vector which has `TRUE` for each element corresponding
to an "Archaic" individual (whoever that might be):

```{r}
which_archaic <- df$groupAge == "Archaic"
head(which_archaic)
```

And we can then use this vector as an index into our overall data frame, just
like we learned in the bootcamp session:

```{r}
archaic_df <- df[which_archaic, ]
archaic_df$sampleId
```

Our mysterious "Archaic" individuals are two Neanderthals and a Denisovan!

Here are the publications:

```{r}
archaic_df$dataSource
```

:::

---

**Do we have geographical information in our metadata? Maybe countries or
geographical coordinates (or even both)? Which countries do we have individuals
from? (For the moment, let's
interpret whatever geographical variable columns we have in our data frame
`df` as any other numerical or string column variable.)**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Looking at `colnames(df)` we have a number of columns which have something
to do with geography: `country`, `region`, and also the traditional
[`longitude` and `latitude` coordinates](https://en.wikipedia.org/wiki/Geographic_coordinate_system#Latitude_and_longitude).

How can we get an idea about the distribution of countries?
`table()` counts the number of elements in a vector (basically, a histogram
without plotting it), `sort()` then sorts those elements for easier reading:

```{r}
sort(table(df$country), decreasing = TRUE)
```

```{r}
sort(table(df$region), decreasing = TRUE)
```

We will ignore `longitude` and `latitude` for now, because they are most
useful in truly geographical data analysis setting (which we will delve into
a bit later).
:::

---

**What is the publication source of the sequences in our data set (i.e., which are the
publications the individuals' sequences come from)? Does it look like they
all come from a single study? Take a look at the columns again to see
which one might give you the answer.**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

It looks like we have a `dataSource` column which, from the `glimpse()`
result above is a character vector (a vector of "strings"). This suggests
that it represents discrete categories, so let's use the `table()` & `sort()`
 combo again:
 
```{r}
sort(table(df$dataSource), decreasing = TRUE)
```

OK, it looks like we have most individuals (2504) from the
[1000 genomes project](http://www.internationalgenome.org), we have
318 samples from ["this study" ](https://www.nature.com/articles/s41586-023-06865-0),
and then a bunch of individuals from a wide range of other publications.
This is great because it allows us to refer individual specific results
we might obtain later to the respective publications!
:::

---

**What's the distribution of coverage of the samples (you now know which
column has the coverage information)? Compute the `mean()`
and other `summary()` statistics on the coverage information. Why doesn't
the `mean()` function work when you run it on the vector of numbers in the
`coverage` column (i.e, `df$coverage`, use `?mean` to find the solution).
Which individuals have missing coverage? Finally, use the `hist()` function
to visualize this information to get a rough idea about the data.**

These are all basic (maybe a bit boring) questions which you will be asking
yourself _every time_ you read a new table into R. It might not always
be coverage, but you will have to do this to familiarize yourself with the
data and understand its potential issues!

**Note:** Always keep an eye on strange outliers, missing data `NA`, anything
suspicious. Never take any data for granted (that applies both to data from
any third parties, but even to your own data sets!).

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Mean gives us `NA`:

```{r}
mean(df$coverage)
```

This is because computing the mean from a vector containing `NA` values is not
a sensible operation -- should we remove the `NA` values? It's impossible to say
in general, because that would depend on the nature of our data. Sometimes,
encountering `NA` indicates a problem, so we can't just ignore it. In situation
when this is OK, there's a argument which changes the default behavior and
removes the `NA` elements before computing the mean:

```{r}
mean(df$coverage, na.rm = TRUE)
```

Summary takes care of the issue by reporting the number of `NA` values explicitly:

```{r}
summary(df$coverage)
```

We can see that the coverage information is missing for `r sum(is.na(df$coverage))`
individuals, which is the number of individuals in the (present-day) 1000 Genomes
Project data. So it makes sense, we only have coverage for the lower-coverage
aDNA samples, but for present-day individuals (who have imputed genomes), the
coverage does not even make sense here.

Let's plot the coverage values:

```{r}
hist(df$coverage, breaks = 100)
```

:::

---

**Who is the oldest individual in our data set? Who has the highest
coverage? Who has the lowest coverage?** Again, the "age of a sample" here
is just a place holder --- in practice, you'll be asking yourselves this
kind of questions for all kinds of statistics, even statistics you have to
obtain from some statistical inference software, like PLINK, ADMIXTOOLS,
IBDmix, etc.

**Hint:** Use functions `min()`, `max()` or `which.min()`, `which.max()`
to get something you can use for a data-frame indexing operation to subset
to the row (i.e., individual) of interest.
When would you use each of the pair of functions? Look
up their `?help` to understand what they do and how to use them in this context.

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

`which.max()` reports the _index_ (in this case, the row number) of the
maximum value in a vector (in this case, a column vector):

```{r}
which.max(df$ageAverage)
```

This means we can use this number to index into the data frame and find our answer:

```{r}
df[which.max(df$ageAverage), ]

max(df$ageAverage, na.rm = TRUE)
```

Let's use the similar approach for coverage:

```{r}
df[which.min(df$coverage), ]
min(df$coverage, na.rm = TRUE)

df[which.max(df$coverage), ]
max(df$coverage, na.rm = TRUE)
```

:::


## Exercise 2: Selecting columns

**We have now some basic familiarity with the format of our data and which
kinds of variables/columns we have for every individual. We also got
a little bit more practice on using base R for basic data-frame operations,
mostly subsetting. It's time to learn
about techniques for _manipulating_, _modifying_, and _filtering_ this data using _tidyverse_, specifically the _dplyr_ package.

Often times we end up in a situation in which we don't want to have a large
data frame with a huge number of columns. Not as much for the reasons of
the data taking up too much memory, but for convenience. **You can see that
our `df` metadata table has `r ncol(df)` columns, which don't fit on the
screen if we just print it out (note the "13 more variables" in the output).
Just try this yourself again in your R console and observe what kind of
(cluttered) output you get:**

```{r}
df
```

**You can select which columns to pick from a (potentially very large data frame)
with the function `select()`, the simplest and most basic _dplyr_
function.** It has the following general format:

```
select(<data frame>, <column 1>, <column 2>, ...)
```

**As a reminder, this is how we would select columns using a normal base R
subsetting/indexing operation:**

```{r}
smaller_df <- df[, c("sampleId", "region", "coverage", "ageAverage")]

smaller_df
```

**This is the _tidyverse_ approach using `select()`:**

```{r}
smaller_df <- select(df, sampleId, region, coverage, ageAverage)

smaller_df
```

**Note:** The most important thing for you to notice here is the absence of "double
quotes". It might not look like much, but saving yourself from having to type double
quotes for every data-frame operation (like with base R) is incredibly convenient.

---

**Practice `select()` by creating three new data frame objects and
assigning them into the following two new variables:**

1. Data frame `df_ages` which contains all variables related to sample ages
you found above
2. Data frame `df_geo` which contains all variables related to geography you
found above

**Additionally, the first column of these data frames should always be `sampleId`,
so make sure this column is included in the selection.**

**Check visually (or using the `ncol()` functions) that you indeed have a
smaller number of columns in these two new data frames.**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df_ages <- select(df, sampleId, groupAge, age14C, ageHigh, ageLow, ageAverage, ageRaw)
df_ages
```

```{r}
df_geo <- select(df, site, country, region, latitude, longitude)
df_geo
```

:::

**Note:** `select()` allows us to see the contents of columns of interest much 
easier. For instance, in a situation in which we want to analyse the
geographical location of samples, we don't want to see columns unrelated to
that (like haplogrous, sex of an individual, etc. which are all part of the
huge original table) and `select()` is the solution to this.

---

If your table has many columns of interest you might want to select
(and save to a new variable like you did above), typing them all by hand like you
did just now can become tiresome real quick.
**Here are a few helper functions which can be very useful
in that situation, and which you can use inside the `select()` function
as an alternative to manually typing out column names:**

- `starts_with("age")` -- matches columns starting with the string "age"
- `ends_with("age")` -- matches columns ending with the string "age"
- `contains("age")` -- matches columns containing the string "age"

**You can use them in place of normal column names. If we would
modify our `select()` "template" above, we could do this, for instance:**

```
select(df, starts_with("text1"), ends_with("text2"))
```

**Check out the `?help` belonging to those functions (note that they have
`ignore.case = TRUE` set by default!). Then create the `df_ages` table again, but this time use the three helper functions listed above.**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df_ages1 <- select(df, sampleId, groupAge, age14C, ageHigh, ageLow, ageAverage, ageRaw)

df_ages2 <- select(df,
                   sampleId,
                   starts_with("age", ignore.case = FALSE),
                   ends_with("Age", ignore.case = FALSE))
```

The function `contains()` unfortunately doesn't work:

```{r}
select(df, contains("age"))
```

But `matches()` works, because it uses so-called
["regular expression"](https://cran.r-project.org/web/packages/stringr/vignettes/regular-expressions.html)
(`"^age"` indicates that a column name has to start with "age" for it
to be included, disqualifying "coverage"):

```{r}
select(df, sampleId, matches("^age"), groupAge)
```

:::

## Exercise 3: Filtering rows

In the session on base R, we learned how to filter rows using the indexing
operation along the row-based dimension of (two-dimensional) data frames.
For instance, in order to find out which individuals in the `df` metadata are archaics,
we can first created a `TRUE` / `FALSE` vector of the rows corresponding to those
individuals like this:

```{r}
# get a vector of the indices belonging to archaic individuals
which_archaic <- df$groupAge == "Archaic"

# take a peek at the result to make sure we got TRUE / FALSE vector
tail(which_archaic, 10)
```

And then we would use it to index into the data frame (in its first dimension,
before the `,` comma):

```{r}
archaic_df <- df[which_archaic, ]
archaic_df
```

**However, what if we need to filter on multiple conditions? For instance, what
if we want to find all archaic individuals older than 50000 years?**

One option would be to create multiple `TRUE` / `FALSE` vectors, each
corresponding to one of those conditions, and then join them into a single
logical vector by combining `&` and `|` logical operators like you did
in the R bootcamp chapter. For example, we could do this
(**try this code yourself to make sure you understand what's going on**):

```{r}
# who is archaic in our data?
which_archaic <- df$groupAge == "Archaic"
# who is older than 50000 years?
which_old <- df$ageAverage > 50000

# who is BOTH? note the AND logical operator!
which_combined <- which_archaic & which_old
```

Then we can use that logical vector for row-based indexing (in this case,
subsetting of rows) again:

```{r}
df[which_combined, c("sampleId", "ageAverage")]
```

**But this gets tiring very quickly, requires unnecessary amount of typing, and
is very error prone. Imagine having to do this for many more conditions!
The `filter()` function from _tidyverse_ again fixes all of these problems.**

We can rephrase the example situation with the archaics to use the new
function `filter()` like this:

```{r}
#| results: hide
filter(df, groupAge == "Archaic" & ageAverage > 50000)
```

I hope that, even if you never really programmed much before, you appreciate
that this single command involves very little typing and is immediately
readable, almost like this English sentence:

> _"Filter data frame `df` for individuals in which column `groupAge`
is "Archaic" and who are older than 50000 years"_.

Over time you will see that all of _tidyverse_ packages follow these
ergonomic principles.

**Note:** It's worth pointing out again that --- just like this is the
case across the entire _tidyverse_ --- we refer
to columns of our data frame `df` (like the columns `groupAge` or `ageAverage`
right above) as if they were normal variables! Just like we did in `select()`
above. In any _tidyverse_ function we don't write `"groupAge"`, but simply
`groupAge`! When you start, you'll probably make this kind of mistake quite
often. So this is a reminder to keep an eye for this.

---

**Practice filtering with the `filter()` function by finding out
which individual(s) in your `df` metadata table are from a `country`
with the value `"CzechRepublic"`.**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
filter(df, country == "CzechRepublic")
```
:::

---

**Which one of these "Czech" individuals have `coverage` higher than 1? Which individuals have `coverage` higher than 10?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
filter(df, country == "CzechRepublic" & coverage > 1)
```

```{r}
filter(df, country == "CzechRepublic" & coverage > 10)
```

:::




## Exercise 4: The pipe `%>%`

The pipe operator, `%>%`, is the rockstar of the _tidyverse_ R ecosystem,
and the primary reason what makes _tidy_ data workflow so efficient
and easy to read.

First, what is "the pipe"? Whenever you see code like this:

> `something `%>%` f()`

you can read it as:

> _"take **something** and put it as the first argument of f()`"_.

Why would you want to do this? Imagine some complex data processing operation like this:

```{r}
#| eval: false
h(f(g(i(j(input_data)))))
```

This means take `input_data`, compute `j(input_data)`, then compute `i()` on _that_,
so `i(j(input_data))`, then compute `g(i(j(input_data)))`, etc. Of course this
is an extreme example but is surprisingly not that far off from what we often
have to do in data science.

One way to make this easier to read would be perhaps this:

```{r}
#| eval: false
tmp1 <- j(input_data)
tmp2 <- i(tmp1)
tmp3 <- g(tmp2)
tmp4 <- f(tmp3)
result <- f(tmp4)
```

But that's too much typing when we want to get insights into our data as quickly
as possible with as little work as possible.

The pipe approach of _tidyverse_ would make the same thing easier to write
and read like this:

```{r}
#| eval: false
input_data %>% j %>% i %>% g %>% f %>% h
```

This kind of "data transformation chain" is so frequent that RStudio even
provides a built-in shortcut for it:

- `CMD + Shift + M` on macOS
- `CTRL + Shift + M` on Windows and Linux

**Whenevr you will pipe something like this in your solutions, always get
in the habit of using these shortcuts. Eventually this will allow you to
write code as quickly as you can think, trust me!** (And take a peek
at the [cheatsheet](cheatsheets/rstudio.pdf)) of RStudio shortcuts to
refresh your memory on the other useful shortcuts! For instance,
`Alt + -` or `Option + -` inserts the `<-` assignment operator!

---

**Use your newly acquired `select()` and `filter()` skills, powered by the
`%>%` piping operator, and perform the following transformation on the `df`
metadata table, "chaining" the `filter`-ing and `select`-ion operations on the indicated columns:**

1. **First `filter()` the `df` metadata to get only those rows / individuals who are:**

- "Ancient" individuals (column `groupAge`)
- older than 10000 years (column `ageAverage`)
- from Italy (column `country`)
- with `coverage` column higher than 3

2. **Then pipe the result of step 1. to `select()` columns: `sampleId`, `site`, `sex`, and `hgMT` and `hgYMajor`
(mt and Y haplogroups)**.

**As a practice, try to be as silly as you can and write the entire command
with as many uses of `filter()` and `select()` function calls in sequence
as you can.**

**Hint:** Don't write the entire pipeline at once! For `filter()`, start with
one condition, evaluate it, then add another one, etc.,
inspecting the intermediate results as you're seeing them in the R console
after every evaluation from your script (`CTRL / CMD + Enter`). Alternatively,
first build up everything in the console one step at a time, then paste the
result into your script to save the command on disk. This is the _tidyverse_
way of doing data science!

**Hint:** What I mean by this is that the following two commands produce
the exact same result:

```
new_df1 <- filter(df, col1 == "MatchString" & col2 > 10000 & col3 == TRUE) %>%
  select(colX, colY, starts_with("ColNamePrefix"))
```

like doing this instead:

```
new_df1 <- df %>%
  filter(col1 == "MatchString") %>%
  filter(col2 > 10000) %>%
  filter(col3 == TRUE) %>%
  select(colX, colY, starts_with("ColNamePrefix"))
```

(**Just take a moment to read both and compare them.**)

This works because "the thing on the left" (which is always a data frame)
is placed by the `%>%` pipe operator as "the first argument of a function
on the right" (which again expects always a data frame)!

**Hopefully you now see what this idea of _"build a more complex `%>%`
pipeline one step a time"_ can mean in practice. Now apply these ideas
to solve the `filter()` and `select()` exercise above.**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df %>%
  filter(groupAge == "Ancient") %>%
  filter(ageAverage > 10000) %>%
  filter(country == "Italy") %>%
  filter(coverage > 3) %>%
  select(sampleId, site, sex, hgMT, hgYMajor)
```

We could also write the same thing more concisely (not the single `filter()` call):

```{r}
df %>%
  filter(groupAge == "Ancient" & ageAverage > 10000 & country == "Italy" & coverage > 1) %>%
  select(sampleId, site, sex, hgMT, hgYMajor)
```

:::

**Note:** It is always a good practice to split long chains of `%>%` piping
commands into multiple lines, and indenting them neatly one after the other.
Readability matters to avoid bugs in your code!

---

## Exercise 5: More complex conditions

Recall our exercises about logical conditional expressions (`&`, `|`, `!`, etc.).

Whenever you need to do a more complex operation, such as saying that a
variable `columnX` should have a value `"ABC"` or `"ZXC"`, you can already
guess that you can do this by writing
`filter(df, columnX == "ABC" | column == "ZXC")`.

Similarly, you can condition on numerical variables, just as we did in the
exercises on `TRUE` / `FALSE` expressions. For instance, if you
want to condition on a column `varX` being `varX < 1` _or_ `varX > 10`, you could
write `filter(df, varX < 1 | var X > 10)`.

---

**Practice combining multiple filtering conditions into a _tidyverse_
`%>%` "piping chain" by filtering our metadata table to find individuals
for which the following conditions hold:**

1. They are "Ancient" (`groupAge` column)
2. They are from "France" _or_ "Canary Islands" (`country` column)
3. Their coverage less than 0.1 or more than 3 (`coverage` column)

**Hint:** The easiest solution is to write three `filter()` function
calls, one for each condition above:

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df %>%
  filter(groupAge == "Ancient") %>%
  filter(country == "France" | country == "Canary Islands") %>%
  filter(coverage < 0.1 | coverage > 3)
```

Whenever you want to test whether a variable is of a set of multiple possible
values
(like you wanted here for the `country` filter),
you can use the `%in%` operator:

```{r}
df %>%
  filter(groupAge == "Ancient") %>%
  filter(country %in% c("France", "Canary Islands")) %>%
  filter(coverage < 0.1 | coverage > 3)
```

:::

---

**Now select individuals who are from
Germany and have coverage higher than 10 _or_ individuals who are from Estonia
with coverage less or equal than 1. Save your result to `df_subset` variable
and print everything in this table by executing `print(df_subset, n = Inf)`.**

**Supplementary question: Why do we need to `print()` the result like this? What happens when you
just type `df_subset` into your R console (or execute this from your
script using `CTRL / CMD + Enter` on its own?)**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

We can of course compose (infinitely) complex combinations of `&` and `|`
and `!`:

```{r}
df_subset <- filter(df,
                    (country == "Germany" & coverage > 3) |
                    (country == "Estonia" & coverage < 1))

print(df_subset, n = Inf)
```

:::

---

**How is "chaining" several `filter()` command one after another different
from using a single `filter()` command with multiple conditional expressions
joined by `&`? How about the difference from joining them with `|`?**



## Exercise 6: Dropping columns

This one will be easy. If you want to drop a column from a table, just
prefix its name with a minus sign (`-`) in a `select()` function.

**Note:** Yes, this also works with `starts_with()` and its friends above,
just put `-` in front of them!

To demonstrate the dropping of columns in practice, here's our `df`
table again (just one row for brevity):

```{r}
df %>% head(1)
```

Observe what happens when we do this:

```{r}
df %>% select(-sampleId) %>% head(1)
```

And this:

```{r}
df %>% select(-sampleId, -site) %>% head(1)
```

And this:

```{r}
df %>% select(-sampleId, -site, -popId) %>% head(1)
```

The columns prefixed with `-` are dropped from the resulting table!

Rather than typing out a long list of columns to drop, we can also do this
to specify the range of consecutive columns (notice the minus `-` sign):

```{r}
df %>% select(-(sampleId:popId)) %>% head(1)
```

Alternatively, we can also use our well-known `c()` function, which is very useful
whenever we want to drop a non-consecutive set of columns (again notice
the minus `-` sign):

```{r}
df %>% select(-c(sampleId, site, popId)) %>% head(1)
```


**Note:** The same "range syntax" of using `:` and and listing columns with
`c()` applies also to selecting which columns to choose, not just for dropping them.

---

**Use the `:` range in `select()` to drop every column after `country` (i.e.,
all the way to the last column in your table, whichever column this is).**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df %>% select(-(region:hgYMinor))
```

I was personally a bit surprised that this also works without the parentheses.
R is smart!

```{r}
df %>% select(-region:-hgYMinor)
```
:::


## Exercise 7: Renaming columns

**Very often you read data frames in which columns
have names which are either very long, containing characters which are not
allowed, or generally inconvenient.** Imagine a situation, in which you
refer to a particular column very often in your workflow, but it takes too
much effort to type it out, or it uses awkward characters.

After discussing `select()` and `filter()`, let's introduce
another member of the _tidyverse_---the function `rename()`.

The template for using it is again very easy (again, you would replace
the text in `<pointy brackets>` with appropriate symbols):

```
rename(<data frame>, <new name> = <old name>)
```

---

**Create a new data frame `df_subset` by doing the following:**

1. First `select()` the columns `sampleId`, `popId`, `country`, `continent`, 
`groupAge`, `ageAverage`, and `coverage`.

2. Then use the `rename()` function to give some columns a shorter name:
`sampleId -> sample`, `popId -> population`, `groupAge -> set`,
`ageAverage -> age`. Leave the `country` and `coverage` columns as they are
(i.e., don't rename those).

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df_subset <-
  df %>%
  select(sampleId, popId, country, continent, groupAge, ageAverage, coverage) %>%
  rename(sample = sampleId, population = popId, set = groupAge, age = ageAverage)

df_subset
```

We now have a much cleaner table which is much easier to work with!
:::

---

**A shortcut which can be quite useful sometimes is that `select()` also
accepts the `new_name = old_name` syntax of the `rename()` function, which
allows you to both select columns (and rename some of them) all at once.
Create the `df_subset` data frame again, but this time using just `select()`.**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df_subset <- df %>%
  select(sample = sampleId, population = popId,
         country,
         continent,
         set = groupAge, age = ageAverage,
         coverage)

df_subset
```

:::

---

**When would you use one or the other (`select()` vs `rename()`)`?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Answer: `select()` always drops the columns which are not explicitly listed.
`rename()` only renames the columns which are listed, but retains all other
columns even though they are not listed.

:::

---




## Exercise 8: Reorganizing columns

Let's look at another useful application of the `select()` function and
that is reordering columns. Our `df` metadata table has `r ncol(df)` columns.
When we print it out, we only see a couple of them:

```{r}
head(df, 3)
```

Oftentimes when doing data analysis, we often work interactively in the
console, focusing on a specific subset of columns, and need to immediately
see the values of our columns of interest, rather than having them
buried in the rest of the (non-visible) output --- how can we do this?

We already know that we can use `select()` to pick those columns of interest,
but this removes the non-selected columns from the data frame we get. Whenever
we want to retain them, we can add the call to `everything()`, like this:

```
select(<data frame>, <column 1>, <column 2>, ..., everything())
```

Which effectively moves `<column 1>`, `<column 2>`, ... to the "front" of our
table, and adds everything else at the end.

**Select the subset of columns you selected in the previous exercise on renaming
in exactly the same way, but this time add a call to `everything()` at the end
to keep the entire data set intact (with just columns rearranged). Save the
result to `df` again.**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Our data frame before:

```{r}
head(df, 3)
```

```{r}
df <- select(df, sample = sampleId, population = popId, country, continent,
             set = groupAge, age = ageAverage, coverage,
             everything())
```

Our data frame after:

```{r}
head(df, 3)
```

Notice that we prioritized the selected columns of interest (and also
renamed some for more readability), but we still have all the other columns
available!
:::

---

**Experiment with the function `relocate()` (it uses the same format
as `select()`). Try it with our `df` table and
with giving it a names of a couple of columns, similarly to what you did
with `select()` above. What result do you get?**

**What happens when you do the same with `select()` (just use `select()`
instead of `relocate()`) and add `everything()` after the last column?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
relocate(df, flag, ageLow, continent)
```

```{r}
select(df, flag, ageLow, continent, everything())
```

`everything()` is another useful helper function for `select()` operations,
like `starts_with()` _et al._ above.

:::

---

**Is the `select()` & `everything()` combination needed when we have `relocate()`?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

I don't think so --- I only learned about `relocate()` two days ago, after
using R for ten years professionally. :)

:::


## Exercise 10: Sorting rows based on column values

When you need to sort the rows of a data frame based on a value of one or
multiple columns, you will need to use the function `arrange()`. Again, in the spirit
of the consistency across the _tidyverse_ ecosystem, it follows exactly
the same format of first giving the function a data frame to operate on,
followed by a list of columns to sort by.

```
arrange(<data frame>, <column 1>, <column 2>, ...)
```

**Note:** When you want to reverse the order of the sort, you can surround the
column name in a helper function `desc()` (standing for "descending").

```
arrange(<data frame>, desc(<column 1>), desc(<column 2>), ...)
```

---

**Who is the oldest individual in your data who is not an archaic individual?**

**Hint:** Remember that you can `filter()` out rows corresponding to archaic
individuals with the condition `set != "Ancient"` and then pipe `%>%` the result
into `arrange()` for sorting based on the renamed column `age`).

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

It's the famous [Ust'-Ishim individual](https://www.nature.com/articles/nature13810)!

```{r}
df %>% filter(set != "Archaic") %>% arrange(desc(age))
```

:::

---

**Similarly, who is the youngest ancient individual in your data in terms
of its dating?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Apparently it's an individual from [Andaman islands](https://en.wikipedia.org/wiki/Andaman_Islands)
from 90 years ago:

```{r}
df %>% filter(set == "Ancient") %>% arrange(age)
```

:::

---

**Does the same approach work for sorting text? What do you get when you
try sorting based on the `country` column?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Yes, this gives us standard alphabetical sort!

```{r}
df %>% arrange(country)
```
:::

---

**What do you get when you try sorting based on `country` and then `coverage`?
What happens when you sort based on `coverage` and then `country`? Why is there
a difference?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df %>% arrange(country, coverage)
df %>% arrange(coverage, country)
```

:::

---

**Does it matter what order you use `filter()` (on some column) and `arrange()`
(on another column) if you're using both functions? If yes, why? If not, why
not? Think about the amount of work these functions have to do in either of
those two scenarios.**


## Exercise 11: Mutating tables

Mutating a table means adding a column for a new variable. Again, as with
the previously introduced functions `select()`, `filter()`, and `rename()` and
`arrange()`, it follows a consistent _tidyverse_ pattern:

```
df %>% mutate(<new column name> = <vector of the required length>)
```

Not surprisingly, the new column is assigned a vector of the same length
as the number of rows in a data frame. What we often do is this:

```
df %>% mutate(<new column name> = <expression involving other columns>)
``` 

because `mutate()` allows us to refer to other columns in the data frame
already present.

---

**To demonstrate this on a couple of exercises, let's actually _remove_ some
columns from our data frame first. They were originally created by `mutate()`
in the first place and this gives as a useful opportunity for practice
(because we'll now add them back again ourselves):**

```{r}
df <- select(df, -set)
```

The important aspect of mutate is that it is a _vectorized_ operation. We can't
create a column and give values to only some rows. Here are several ways how
we could do this:

### 1. `if_else()`

`if_else()` accepts a logical vector (i.e., a vector of `TRUE` / `FALSE` values),
and produces another vector which contains one value for each `TRUE`, and another
value for each `FALSE`. Here is an example:

```{r}
v <- c(TRUE, TRUE, FALSE, TRUE, FALSE)

if_else(v, 123, -123)
```

Notice that we get 123 for each `TRUE` and -123 for each `FALSE`.

---

**The above can be a bit confusing, so spend a bit of time playing around with
`if_else()`. For instance, create a different logical vector variable (containing an arbitrary
`TRUE` or `FALSE` values, up to you), and have
the function produce values "hello" and "bye" depending on `TRUE` / `FALSE`
state of each element of the logical vector variable.**

Just get familiar with this vectorized thinking
because whenever we do data science, it almost always happens in this
vectorized fashion (such as operating on every single value in a column
of a table at once, like we'll do soon).

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
v <- c(FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, TRUE, FALSE)

if_else(v, "hello", "bye")
```

:::

---

As mentioned, this function is extremely useful whenever we need to generate values of a new
column based on values of another column(s). The general pattern for using
it inside a `mutate()` call is this:

```
df %>%
  mutate(<new_column> = if_else(<logical vector>, value_for_true, value_for_false))
```

---

**You can see that the column `df$sex` contains chromosome sex determination of
each individual as either `XY` or `XX`. Use the `if_else()` function in a
`mutate()` call to create a new column `sex_desc` which
will contain a character vector of either "male" or "female" depending on the
sex chromosome determination.**

**Hint:** Again, you might want to build an intuition first. Create a
small vector of a mix of "XY" and "XX" values and store it in a variable
`sex`. Then experiment with `if_else()`-based assignment of "female" and
"male". When you're sure you got it, apply it to the data frame in the
`mutate()` call to solve this exercise.

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df <- mutate(df, sex_desc = if_else(sex == "XY", "male", "female"))
```

:::

---

**Run `table(df$sex, df$sex_value)` to make sure the assignment of sex
description worked as expected. How do you interpret the results? Did
we miss something? If we did, what is wrong?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
table(df$sex, df$sex_desc)
```

Oh, we can see that there's an "XXY" individual who -- because of our
improperly specified `if_else()` condition (which assigns individuals
as male only when they are "XY") -- got assigned in to the "female" category.

We would've noticed this in our exploratory phase, had we checked the
range of possible values of the `sex` column like this. This is why getting
familiar with all data is so crucial the first time we look at it. We
could've (and should've) ran this at the beginning:

```{r}
table(df$sex)
unique(df$sex)
```

:::

---

It turns out there is an individual with a
[Klinefelter syndrome](https://en.wikipedia.org/wiki/Klinefelter_syndrome). A
male carrying an extra X chromosome. **Clearly the logic of our `if_else()`
is incorrect because this individual was incorrectly assigned as "female".
Take a look at your code again to make sure you see why is that.
How would you fix the `mutate()` (or rather the `if_else()`) command to work correctly and correctly assign the XXY individual as "male"?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

There are a number of options. This is simplest, we can just flip the condition:

```{r}
df <- mutate(df, sex_desc = if_else(sex == "XX", "female", "male"))
```

But it may be better to be more explicit?

```{r}
df <- mutate(df, sex_desc = if_else(sex %in% c("XY", "XXY"), "male", "female"))
```

We can verify that the assignment of sex description worked correctly now:

```{r}
table(df$sex, df$sex_desc)
```

:::

This is a lesson to always remember to check assumptions in your data,
even the ones you consider trivial. Functions such as `table()` and `unique()`
are extremely useful for this.

**Note:** I got this originally wrong when I was preparing these materials.
So this is a real-world cautionary tale.


---

**Let's practice `if_else()` a bit more. First, use `filter()` to look at
the values of the `age` column for present-day individuals in the 1000 Genomes
Project data (`dataSource == "1000g"`). What ages do you see?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

The individuals have `NA` age. This could be annoying if we ever want to plot
some population genetic statistics as functions of age:

```{r}
df %>% filter(dataSource == "1000g")
```

Is there anyone in the 1000 GP data who has an age specified?

```{r}
df %>% filter(dataSource == "1000g") %>% filter(!is.na(age))
```

:::

---

The 1000 GP individuals are missing an `age` value. It should be set to 0.
Let's fix that now and correct the metadata table.

**You already know that you can get a logical vector indicating whether a certain
element of another vector (here, the column `age`) is `NA` or not via the
function `is.na()`. Use `is.na(age)` in the `if_else()` to set the `age`
column with the `mutate()` function so that:**

1. Rows where `is.na(age)` is `TRUE` will be set to 0, and
2. Rows where `is.na(age)` is `FALSE` will be set to `age` (because those values
don't need to be replaced).

**Note:** Using `mutate()` to replace values of an existing column (rather than
creating a new column) is done very often in data science, particularly in 
"clean up" operations like this one. Your data will never be "perfect" (especially
if you get it from some other source) and you will need to do a lot of so-called
"table munging" to prepare it for analysis and plotting, just like we're doing 
here.

**Hint:** Again, if you need help, you can start slowly by creating a toy example variable `age` which
will have a vector of a mix of numbers, including some `NA` values. Then
practice creating an `if_else()` expression which will return 0 in place
of `NA` values in this vector, and keep every other value intact.

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

The individuals have `NA` age. This could be annoying if we ever want to plot
some population genetic statistics as functions of age:

```{r}
df <- df %>% mutate(age = if_else(is.na(age), 0, age))
```

:::



### 2. `case_when()`

`if_else()` does only work on binary `TRUE` or `FALSE` conditions. But what
if we want to create a new column with values of multiple categories, not
just two. Recall our original column `set` (which we dropped from our
`df` table earlier), which had values either "Archaic", "Ancient",
or "Modern".

For this purpose, `case_when()` is perfect. It works in a very similar
manner to `if_else()`, but allows not-just-binary categorization.
Consider this vector of numbers between 1 and 30:

```{r}
v <- 1:20
```

If we wanted to assign a value "less_than_10" or "10_or_more" to each element,
we could use the function `if_else()` like this:

```{r}
if_else(v < 10, "less than 10", "10 or more")
```

What if we want to introduce three or more categories? `case_when()` to the
rescue! **Try running this yourself on the vector `v` created above.**

```{r}
case_when(
  v < 10  ~ "less than 5",
  v == 10 ~ "exactly 10",
  v > 10 ~ "more than 10"
)
```

**Remember how we had a slightly annoying time with doing vectorized conditional
logical expressions on `TRUE` / `FALSE` vectors in our Bootcamp session?
Now it's all paying off!**

Every one of the three conditions actually result in a logical vector,
and `case_when()` decides which value to produced for each element of that
vector based on whichever results in `TRUE`.

The `case_when()` function has a very useful optional argument called `.default =`,
which determines the value it should return whenever a particular location
in the logical vector either results in all `FALSE` (so none of the conditions
would apply), or whenever it produces a `NA` value (so none of the conditions
can apply even in principle).

---

**First, let's start simple and reimplement the `mutate()` operation to
add the `sex_desc` column again (just to practice on something we
already know) this time using `case_when()` and implementing
the three conditions ("XX", "XY", "XXY") individually. Additionally, use the
`.default =` argument of `case_when()` to assign "unknown" as a value of
the `sex_desc` column.**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df <-
  df %>%
  mutate(sex_desc = case_when(
    sex == "XX" ~ "female",
    sex == "XY" ~ "male",
    sex == "XXY" ~ "male",
    .default = "unknown"
  ))
```

Let's check that this worked correctly:

```{r}
table(df$sex, df$sex_desc)
```


:::

---

**In the exercise on conditional expressions, you learned about `&` and `|`
operators which make it possible to combine multiple conditions into a single
`TRUE` / `FALSE` vector. Of course, this means that the conditions inside
the `case_when()` function can also utilize those operators.**

**Create a new column `sex_set` which will contain the following values:**

1. `sex == "XX" & age == 0"` should produce `"female (present-day)"`,
2. `(sex == "XY" | sex == "XXY") & age == 0` should produce `"male (present-day)"`,
3. `sex == "XX" & age > 0` should produce `"female (ancient)"`,
4. `(sex == "XY" | sex == "XXY") & age > 0` should produce `"male (ancient)"`,
5. any other combination should default to "other"

**I provided the code of the logical conditions for you, you just have to put them
in a `case_when()` call appropriately, modifying the solution to the previous
exercise. Verify the outcome by doing `table(df$sex_set)` again.**

**Why did I put parenthesis around the sub-clause involving the `|` OR operator? 
What happens if we don't do this?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df <-
  df %>%
  mutate(sex_set = case_when(
    sex == "XX" & age == 0                   ~ "female (present-day)",
    sex == "XX" & age > 0                    ~ "female (ancient)",
    (sex == "XY" | sex == "XXY") & age == 0  ~ "male (present-day)",
    (sex == "XY" | sex == "XXY") & age > 0   ~ "male (ancient)",
    .default = "other"
  ))
```

Let's check that this worked correctly:

```{r}
table(df$sex_set)
```


:::

**Note:** Again, admittedly this might seem arbitrary to you --- why would we
need something like this for data analysis and statistics? But this kind of
"arbitrary categorization" is extremely useful to generate factor categories
for plotting, especially for plotting with _ggplot2_ later. So keep this in mind
for the time being.


---

**Remember how we removed the `set` column, which originally contained
values "Modern", "Ancient", or "Archaic", depending on whether the individual
was a present-day modern human, ancient modern human, or an archaic individual,
respectively? Use what you learned in this exercise on `if_else()` or
`case_when()` to reconstruct this column again based on information in the
remaining columns.** (There is many possible solutions, so don't be afraid
to think creatively!)

**Hint:** Using information in `age` and the names of the three archaic
individuals (combined with the `.default` argument of `case_when()`
is probably the easiest solution).

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
# let's pick out individual names of the archaics
archaics <- c("Vindija33.19", "AltaiNeandertal", "Denisova")

df <-
  df %>%
  mutate(set = case_when(
    age == 0             ~ "Modern",  # whoever has sampling date 0 is "Modern"
    sample %in% archaics ~ "Archaic", # whoever is among archaics is "Archaic"
    .default             = "Ancient"  # and only the third category remains
  ),
  .after = coverage)
```

:::

---

**Note the use of `.after =` in my solution to the previous exercise. Look up
the help of `mutate()` in `?mutate` to see what it does and why I used it.**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Usually, when we add a new column using `mutate()` we don't want to add it to
the very end of the table (where we cannot see it), which is what the function
does by default. `.after` and `.before` are options which allow us to specify
where among the already present columns in the data frame do we want to add
the new column

:::



## Exercise 12: Summarizing tables

You have already learned how to operate on rows (`filter()`, `arrange()`),
columns (`select()`, `rename()`), and both rows and columns (`mutate()`).
**We have one remaining piece of the puzzle and that is operating on _groups of rows_. This takes
the _tidyverse_ functionality to an entire whole level and allows you to do
many more powerful things with tabular data and compute summary statistics.**

In this section, we will will cover the functions `group_by()`, `summarize()`,
and various associated "slice functions".

### 1. `group_by()`

**Take a look at the first couple of rows of our metadata table again:**

```{r}
df
```

**Now run the following code in your R console and carefully inspect the
output (and compare it to the result of the previous command):**

```{r}
group_by(df, continent)
```

You can probably see that the data hasn't changed at all (same number of
columns, same number of rows, all remains the same), but you can notice
a new piece of information in the output:

```
[...]
# Groups:   continent [4]
[...]
```

**The output says that the data has been grouped by the column/variable continent.
This means that any subsequent operation will be applied not to individual
rows, like it was with our `mutate()` operations earlier, but they will
now work "per continent".** You can imagine this as the `group_by()` function
adding a tiny bit of invisible annotation data which instructs downstream
functions to work per group.

Before we move on to computing summary statistics, let's also note that **we
can, of course, group based on multiple columns**. For instance, in our data,
we should probably not summarize based on continent but also on age, splitting
individuals based on the broad age sets like this:

```{r}
group_by(df, continent, set)
```


**Grouping is just the first step though, and doesn't do anything on its own...**

### 2. `summarize()`

Why is grouping introduced above even useful? The most simple use case is
computing summary statistics on the data on a "per group" basis using the
`summarize()` function. This function works a little bit like `mutate()`,
because it creates new columns, but it only creates one row for each group.

For instance, we could compute the mean coverage of each individual in a
group like this, and arrange the groups by the coverage available:

```{r}
df %>%
  group_by(continent, set) %>%                   # first create groups
  summarize(mean_coverage = mean(coverage)) %>%  # then create summaries for each group
  arrange(desc(mean_coverage))
```

**Notice that the result has only three columns, unlike the many columns in the
original `df` table! We have a column for each variable we grouped over, and
we have the new column with our summary statistic. Additionally, we only have
`r group_by(df, continent, set) %>% summarize(mean(coverage)) %>% nrow`
rows, one row for each combination of `continent` and `set`. So, this is
kind of similar to `mutate()` (it creates new variables/columns), but different.**

---

**What does the `NA` value for "Modern" individuals in the output of the
previous code chunk? Take a look at the original data `df` to answer this
question.**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

The reason is simple -- the present-day individuals are high quality imputed
genotypes, which is why the sequencing coverage column is full of `NA` (not
available, i.e., missing data) values. Computing a mean of such values is,
itself, an `NA` value too:

```{r}
mean(c(NA, NA, NA))
```

:::

---

`summarize()` is even more powerful than that because we can compute many
different things at the same time, for each group! This next exercise is an
example of that.

**As you know, the mean (such as the `mean_coverage` we computed right above)
is computed as a sum of a set of values divided
by their count. Use the `group_by()` and `summarize()` code exactly like you did above,
but instead of computing the `mean()` directly, compute it manually by doing
(in a `summarize()` call):**

1. `sum_coverage = sum(coverage)` (the sum of coverages for each group),
2. `n = n()` (the number of values in each group),
3. `mean_coverage = sum_coverage / n` (using the two quantities just computed to calculate the mean)

**To make things a little tidier, `arrange()` the results by the column
`mean_coverage`.**

**Once you have your results, save this new table in a variable called
`df_summary`.**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Here's our entire pipeline to compute the mean coverage "manually" instead
of the built-in function `mean()` like we did in the previous exercise:

```{r}
df_summary <-
  df %>%                               # take the data frame df...
  group_by(continent, set) %>%         # ... group rows on continent and set...
  summarize(                           # ... and compute for each group:
    sum_coverage = sum(coverage),      #       1. the sum of all coverage
    n = n(),                           #       2. the number of coverages
    mean_coverage = sum_coverage / n   #       3. the mean using 1. and 2.above
  ) %>%
  arrange(mean_coverage)               # ... then sort by average coverage

df_summary
```

:::

---

**Look up the function `?ungroup`. Why is it useful?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Although grouping is very useful for computing summary statistics across
groups of values, we often need to discard the grouping because we might
want to continue working with the summarized table we obtained with
other _tidyverse_ functions without working on the pre-defined groups.
For instance, we might want to follow up a `summarize()` operation with
a `mutate()` operation, without working with groups.

:::

---

**How would you compute 95% confidence interval for the mean coverage using the
`group_by()` and `summarize()`? Remember that you will need standard deviation
(R function `sd()`) and also the number of observations (function `n()`).
Look up confidence interval equation on Wikipedia.**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

If you're here this means that you needed extra challenge, so I'm sure you
figured it out. 😜 Reach out to me if you want to discuss the solution!

:::

---

### `slice_` functions

Finally, in addition to computing various summaries on groups using `summarize()`,
we have five "slicing" functions at our disposal, each of which extracts
specific row(s) from each defined group. These functions are:

- `slice_head(n = 1)` takes the first row,
- `slice_tail(n = 1)` takes the last row,
- `slice_min(<column>, n = 1)` takes the row with the smallest value of <column>,
- `slice_max(<column>, n = 1)` takes the row with the largest value of <column>,
- `slice_sample(<col> = 1)` takes one random row.

---

**What is the lowest coverage individual in each `continent` and `set` group? While you're doing
this, `filter()` out the `set` of "Modern" individuals because they are not
meaningful anyway (they have `NA` coverages), composing the entire command
as a series of steps in a `%>%` pipeline.**

**At the end of your `%>%`
pipeline, pipe your results into `relocate(continent, set)` to make
everything even tidier.**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df %>%
  filter(set != "Modern") %>%
  group_by(continent, set) %>%
  slice_min(coverage, n = 1) %>%
  relocate(continent, set)
```

:::

---

**Modify your `%>%` pipeline for the exercise above to answer who is the
oldest individual we have available on each continent?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df %>%
  filter(set != "Modern") %>%
  group_by(continent) %>%
  slice_max(age) %>%
  relocate(continent, set, age)
```

:::

## Exercise 13: Writing (and reading) tables

Above you create a summary table `df_summary`. Let's try writing
it to disk using `write_tsv()`. The `.tsv` stands for "tab-separated values"
(TSV) which is a file format similar to another file you might have heard about,
the "comma-separated values" (CSV) file. They can be opened with Excel too,
in case this is useful at some point, but they are very useful for
computational workflows.

---

**Look up the help in `?write_tsv`. What are the parameters you need to specify
at minimum to save the file? This function has all of the useful options saved
as defaults, so there's not much you have to do. Still, look up the options!**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

The only thing we really want to do is specify the data frame to write to disk
and the filename:

```{r}
write_tsv(df_summary, "~/Desktop/df_summary.tsv")
```

```{r}
#| echo: false
unlink("~/Desktop/df_summary.tsv")
```

:::

---

**What are other means of writing files you found in `?write_tsv`? How
would you read a CSV file into R? What about other file formats?**

**Hint:** Look up `?read_csv`.

---

**A very useful R package is _readxl_, for reading and writing Excel files.
Google this package, and install it with `install.packages("readxl")`. If
you want and have some Excel file on your computer, try reading it into
R with `library(readxl); read_excel("path/to/your/excel/file")`.**


## "Take home exercises"

**Which country has the highest number of aDNA samples (i.e., samples for
which `set == "Ancient`"). How many samples from Estonia, Lithuania, or Latvia
do we have?**

**Hint:** Use the `group_by(country)` and `summarize()` combo again,
together with the `n()` helper function.

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution


Count the number of samples from each country, and order them by this count:

```{r}
countries_df <-
  df %>%
  filter(set == "Ancient") %>%
  group_by(country) %>%
  summarise(n = n()) %>%
  arrange(desc(n))

countries_df
```

```{r}
countries_df %>% filter(country %in% c("Estonia", "Lithuania", "Latvia"))
```


:::

---

**Is there any evidence (real or not --- just out of curiousity!) of a
potential bias in terms of how many `set == "Ancient"` samples
do we have from a country and the geographical location of that country, such
as the average longitude or latitude of samples from there? Compute a linear
model using the `lm` function of the count the `n` count of samples from
each country as a function of `avg_lat`
or `avg_lon` (average latitude and longitude) of samples in each country (all
of these quantities computed with `group_by()` and `summarize()` like you did
above).**

**Hint:** If you're not familiar with this, look up the linear model function `?lm` or perhaps
[this tutorial](https://www.geeksforgeeks.org/r-language/how-to-use-lm-function-in-r-to-fit-linear-models/) to see how you can build a linear regression
between a response variable (like our count `n`) and an explanatory
variable (such as average longitude or latitude of a country), as computed
by the `group_by()` and `summarize()` combination (the result of which
you should save to a new data frame variable, to be used in the `lm()`
function).

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Let's first compute our summary statistics:

```{r}
countries_df <-
  df %>%
  filter(set == "Ancient") %>%
  group_by(country) %>%
  summarise(n = n(),
            avg_lat = mean(latitude, na.rm = TRUE),
            avg_lon = mean(longitude, na.rm = TRUE))

countries_df
```

Compute the linear regression of `n` as a function of `avg_lon` -- there
doesn't appear to be any significant relationship between the two variables:

```{r}
lm_res <- lm(n ~ avg_lon, data = countries_df)
summary(lm_res)

plot(countries_df$avg_lon, countries_df$n)
abline(lm_res)
```

Compute the linear regression of `n` as a function of `avg_lat` -- it looks
like there's a significant relationship between the number of samples from
a country and geographical latitude? Is this a signal of aDNA surviving
in more colder climates? :)

```{r}
lm_res <- lm(n ~ avg_lat, data = countries_df)
summary(lm_res)

plot(countries_df$avg_lat, countries_df$n)
abline(lm_res)
```

Does it mean anything? Hard to tell, but it does make for a fun exercise. :)
:::

---

**Take your own data and play around with it using the concepts you
learned above.** If the data isn't in a form that's readily readable
as a table with something like `read_tsv()`, please talk to me! _tidyverse_
is huge and there are packages for munging all kinds of data. I'm
happy to help you out.

**Don't worry about _"getting somewhere"_. Playing and experimenting
(and doing silly things) is the best way to learn.**

**For more inspiration on other things you could do with your data,
take a look at the [_dplyr_ cheatsheet](https://bodkan.net/simgen/cheatsheets/dplyr.pdf).**

In each following session, you'll have the opportunity to do the same.
The next session will focus on more real-world insights from data, and
the section after that will focus on creating beautiful visualizations
using the _ggplot2_ package.

## Closing remarks

Hopefully you're now learning that really integrating these couple of _tidyverse_
"verbs" gives you an entirely new natural way of not just working and modifying
and filtering data, but _"thinking in data"_ (as pretentious and hippy this must
seem to you right now).

When you watch experts doing data science using _tidyverse_ in real time, like
an experienced colleague helping you out, you will see that they can "think
about the data" at the same time as they are typing _tidyverse_ commands. Over time,
with practice, you will get there too.

And this is actually just the beginning! The true power of data science arrives
when we learn _ggplot2_ visualization later. _ggplot2_ uses the same philosophical
principles ad the _dplyr_ commands we practiced above, but allows you to
_"visualize information at the same time as you're thinking about it"_.

