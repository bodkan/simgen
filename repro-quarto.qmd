# Quarto reports and slides

From the previous section, you have now set up a proper computational project
structure. You've added your pipeline R scripts which download, manipulate,
filter, and otherwise process "raw data" into "processed data", the latter
being the starting point of data analysis, visualization, and statistical
inference.

In other words, we've set up technical processing, but how do we actually
do science with all that in the most reproducible way possible? Let's introduce
the [Quarto system](https://quarto.org) for reproducible scientific research.

## Exercise XYZ: Recording R session information

**The following command should be included at the end of your "Quarto reports".
When you run it, how would you read and interpret the information it provides?
What do you think is the most important information which might be missing in
case you need to pick up someone else's project or script?**

```{r}
sessionInfo()
```





## Further practice

### Option 1: Your own data

I'm sure you have done some form of table munging and data visualization in your
projects. If you're done with the exercises above, here are some ideas for you:

1. If you haven't used R for processing of tabular data in your work -- maybe
you have done similar table munging in Excel or Google Sheets in a manual
(i.e., non-reproducible) way? -- write your processing steps as a self-contained
R script:

- If you used Excel, you can use the R package [_readxl_](https://readxl.tidyverse.org),
- If you used Google Sheets, you can use the R package [_googlesheets4_](https://googlesheets4.tidyverse.org)

2. If you used Excel or base R for plotting figures, use what you learned
about _ggplot2_ to make beautiful, publication-ready plots as we did in our
exercises.

Yes, this will put you in an uncharted territory. This is scary and totally
normal when doing research. I'm happy to help you out with questions
and issues!

## Bonus content

Here are a couple of topics which can be extremely helpful for pushing
reproducibility even further, but that we don't have bandwith to go to,
unfortunately. I'm mentioning them in case you'd like to do more studying
on your own, perhaps as you get further into your own research projects.
I use all of these in my own work, but I consider them much more advanced
topics:

1. [_renv_](https://rstudio.github.io/renv/articles/renv.html) --
an R package which allows you to lock the exact versions of
R packages used in your project (and have anyone else restore those exact
versions of packages at a later point). 

2. [_targets_](https://books.ropensci.org/targets/) -- an R package for
building and orchestrating truly complex computational pipelines. For instance,
it can track whether or not a particular function or piece of data has changed
and, therefore, whether other components of a pipeline downstream from it need
to be re-run (unlike our `01_download_data.R` or `02_process_data.R` scripts
which need to be re-run in their entirety after each modification).

3. [_git_](https://git-scm.com) -- an incredibly powerful version control system,
which is also at the back end of the online code repository service
[GitHub](https://github.com).