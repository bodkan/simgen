# Working with tables

Before we begin, let's introduce the two important parts of the _tidyverse_
ecosystem, which we will be using extensively during exercises in this chapter.

1. [_dplyr_](https://dplyr.tidyverse.org) is a centerpiece of the entire R data science universe, providing
important functions for data manipulation, data summarization, and filtering
of tabular data;

2. [_tidyr_](https://tidyr.tidyverse.org) is an R package which helps us morph untidy data into tidy data frames;

3. [_readr_](https://readr.tidyverse.org) is an R package which provides very convenient functions for reading
(and writing) tabular data. Think of it as a set of better alternatives to
base R functions such as `read.table()`, etc. (Another very useful package
is [_readxl_](https://readxl.tidyverse.org) which is useful for working with
Excel data).

Every single script you will be writing in this session will begin with these
two lines of code.

```{r}
#| message: false
library(dplyr)
library(tidyr)
library(readr)
```

Let's also introduce a second star in this session, our example data set.
The following command will read a metadata table with information about
individuals published in a recent aDNA paper on the history or the Holocene in
West Eurasia, dubbed "MesoNeo"
([reference](https://www.nature.com/articles/s41586-023-06865-0)).
We use the `read_tsv()` function (from the above-mentioned _readr_ package)
and store it in a variable `df`. Everything in the section on _tidyverse_ will
revolve around working with this data because it represents an excellent
example of this kind of metadata table you will find across the entirety of
computational population genomics. (Yes, this function is quite magical -- it
can read stuff from a file stored on the internet. If you're curious about
the file itself, just paste the URL address in your browser.)

```{r}
#| message: false
df <- read_tsv("https://tinyurl.com/qwe-asd-zxc")
```

---

**Create a new R script in RStudio, (`File` `->` `New file` `->` `R Script`) and
save it somewhere on your computer as `tidy-tables.R` (`File` -> `Save`). Put
the `library()` calls and the `read_tsv()` command above in this script, and
let's get started!**

---

## A selection of data-frame inspection functions

Whenever you get a new source of data, like a table from a collaborator, data
sheet downloaded from supplementary materials of a paper, you need to familiarize
yourself with it before you do anything else. Here is a list of functions that
you will be using constantly when doing data science to answer basic questions:
Use this list as a cheatsheet of sorts! **Also, use `?<function>` to look up
their documentation to see the possibilities for options and additional features.**

1. _"How many observations (rows) and variables (columns) does my data have?"_ -- `nrow()`, `ncol()`

2. _"What variable (column) names am I going to be working with?"_ -- `colnames()`

3. _"What data types (numbers, strings, logicals, etc.) does it contain?"_ -- `str()`, or better, `glimpse()`

4. _"How can I take a quick look at a subset of the data?"_ -- `head()`, `tail()`

5. _"For a specific variable column, what is the distribution of values I can
expect in that column?"_ -- `table()` for "discrete types", `summary()` for
"numeric types", `min()`, `max()`, `which.min()`, `which.max()`

---

**Before we move on, note that when you type `df` into your R console,
you will see a slightly different format of the output than when we worked
with plain R data frames in the previous chapter. This format of data frame
data is called a ["tibble"](https://tibble.tidyverse.org) and represents
_tidyverse_'s more user friendly and modern take on data frames. For almost
all practical purposes, from now on, we'll be talking about _tibbles_ as
data frames.**

---

## Exercise 1: Exploring new data

**Inspect the `df` metadata using the functions above, and try to answer the
following questions using the functions from the list above (you should decide
which will be appropriate for which question).**

---

**Before you use one of these functions for the first time, take a moment to
skim through its `?function_name`.**

---

1. **How many individuals do we have metadata for?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Number of rows:

```{r}
nrow(df)
```

Number of `unique()` sample IDs (this should ideally always give the same number,
but there's never enough sanity checks in data science):

```{r}
length(unique(df$sampleId))
```
:::

---

2. **What data types do the variables in our data have?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Column names function:

```{r}
colnames(df)
```
:::

---

3. **What data types (numbers, strings, logicals, etc.) are our variables of?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

I tend to use `str()` because I'm old and very used to it, but `glimpse()`
is new and better.

```{r}
str(df)

glimpse(df)
```
:::

---

4. **What is the distribution of dates of aDNA individuals? What are the variables
we have that describe the ages (maybe look at those which have "age" in their name)?
Which one would you use to get information about the ages of most individuals?**

**Hint:** Remember that for a column/vector `df$<variable>`, `is.na(df$<variable>)`
gives you `TRUE` for each `NA` element in that column variable, and
`sum(is.na(df$<variable>))` counts the number of `NA`. Alternatively,
`mean(is.na(df$<variable>))` counts the proportion of `NA` values!

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

From `colnames(df)` above we see a number of columns which seem to have something
todo with "age":

```{r}
columns <- colnames(df)

# don't worry about this weird command, I'm using it to show you the relevant columns
# -- ask me if you're interested! :)
grep("^age.*|^.*Age$", columns, value = TRUE)
```

```{r}
mean(!is.na(df$age14C))
mean(!is.na(df$ageHigh))
mean(!is.na(df$ageLow))
mean(!is.na(df$ageAverage))
mean(!is.na(df$ageRaw))
unique(df$groupAge)
```

Interesting, it looks like the `ageAverage` variable has the highest proportion
of non-`NA` values at about `r 100 * round(mean(!is.na(df$ageAverage)), 4)`%.
We also seem to have another column, `groupAge` which clusters individuals into
three groups. We'll stick to these two variables whenever we have a question
regarding a date of an individual.

:::

---

4. **How many ancient individuals do we have? How many "modern" (i.e., present-day
humans) individuals?**

**Hint:** Use the fact that for any vector (and `df$groupAge` is a vector,
remember!), we can get the number of elements of that vector matching a certain
value by applying the `sum()` function on a "conditional" expression, like
`df$groupAge == "Ancient"` which, on its own, gives `TRUE` / `FALSE` vector.

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
which_ancient <- df$groupAge == "Ancient"

head(which_ancient)

sum(which_ancient)
```

```{r}
which_modern <- df$groupAge == "Modern"
sum(which_modern)
```

```{r}
which_archaic <- df$groupAge == "Archaic"
sum(which_archaic)
```
:::

---

5. **Who are the mysterious "Archaic" individuals (column `groupAge`)?**

**Hint:** We need to _filter_ our table down to rows which have
`groupAge == "Archaic"`. This is an indexing operation which you learned about
in the R bootcamp session! Remember that data frames can be indexed into along
two dimensions: rows and columns. You want to filter by the rows here.

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Again, this gets us a logical vector which has `TRUE` for each element corresponding
to an "Archaic" individual (whoever that might be):

```{r}
which_archaic <- df$groupAge == "Archaic"
head(which_archaic)
```

And we can then use this vector as an index into our overall data frame, just
like we learned in the bootcamp session:

```{r}
archaic_df <- df[which_archaic, ]
archaic_df$sampleId
```

Our mysterious "Archaic" individuals are two Neanderthals and a Denisovan!
:::

---

6. **Do we have geographical information? Countries or geographical coordinates or
both? Which countries do we have individuals from? (We'll talk about spatial
data science later in detail, so let's interpret whatever geographical
variable columns we have in our data frame `df` as any other numerical or string
column variable.)**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Looking at `colnames(df)` we have a number of columns which have something
to do with geography: `country`, `region`, and also the traditional
[`longitude` and `latitude` coordinates](https://en.wikipedia.org/wiki/Geographic_coordinate_system#Latitude_and_longitude).

`table()` counts the number of elements in a vector (basically, a histogram
without plotting it), `sort()` then sorts those elements for easier reading:

```{r}
sort(table(df$country), decreasing = TRUE)
```

```{r}
sort(table(df$region), decreasing = TRUE)
```

We will ignore `longitude` and `latitude` for now, because they are most
useful in truly geographical data analysis setting (which we will delve into
a bit later).
:::

---

7. **What is the source of the sequences in our data set? Does it look like they
all come from a single study?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

It looks like we have a `dataSource` column which, from the `glimpse()`
result above is a character vector (a vector of "strings"). This suggests
that it represents discrete categories, so let's use the `table()` & `sort()`
 combo again:
 
```{r}
sort(table(df$dataSource), decreasing = TRUE)
```

OK, it looks like we have most individuals (2504) from the
[1000 genomes project](http://www.internationalgenome.org), we have
318 samples from ["this study" ](https://www.nature.com/articles/s41586-023-06865-0),
and then a bunch of individuals from a wide range of other publications.
This is great because it allows us to refer individual specific results
we might obtain later to the respective publications!
:::

---

8. **What's the distribution of coverage of the samples? Compute the `mean()`
and other `summary()` statistics on the coverage information. Why doesn't
the `mean()` function work when you run it on the vector of numbers in the
`coverage` column (use `?mean` to find the solution). Which individuals
have missing coverage? Finally, use the `hist()` function to visualize this
information.**

These are all basic questions which you will be asking yourself _every time_
you read a new table into R. It might not be coverage, but you will have
to do this to familiarize yourself with the data and understand its potential
issues!

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Mean gives us `NA`:

```{r}
mean(df$coverage)
```

This is because computing the mean from a vector containing `NA` values is not
a sensible operation -- should we remove the `NA` values? It's impossible to say
in general, because that would depend on the nature of our data. Sometimes,
encountering `NA` indicates a problem, so we can't just ignore it. In situation
when this is OK, there's a argument which changes the default behavior and
removes the `NA` elements before computing the mean:

```{r}
mean(df$coverage, na.rm = TRUE)
```

Summary takes care of the issue by reporting the number of `NA` values explicitly:

```{r}
summary(df$coverage)
```

We can see that the coverage information is missing for `r sum(is.na(df$coverage))`
individuals, which is the number of individuals in the (present-day) 1000 Genomes
Project data. So it makes sense, we only have coverage for the lower-coverage
aDNA samples, but for present-day individuals (who have imputed genomes), the
coverage does not even make sense here.

Let's plot the coverage values:

```{r}
hist(df$coverage, breaks = 100)
```

:::

---

9. **Who is the oldest individuals in our data set? Who has the highest
coverage? Who has the lowest coverage?**

**Hint:** Use functions `min()`, `max()`, `which.min()`, `which.max()`. Look
up their `?help` to understand what they do and how to use them in this context.

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

`which.max()` reports the _index_ (in this case, the row number) of the
maximum value in a vector (in this case, a column vector):

```{r}
which.max(df$ageAverage)
```

This means we can use this number to index into the data frame and find our answer:

```{r}
df[which.max(df$ageAverage), ]

max(df$ageAverage, na.rm = TRUE)
```

Let's use the similar approach for coverage:

```{r}
df[which.min(df$coverage), ]
min(df$coverage, na.rm = TRUE)

df[which.max(df$coverage), ]
max(df$coverage, na.rm = TRUE)
```

:::

---


## Exercise 2: Selecting columns

Often times we end up in a situation in which we don't want to have a large
data frame with a huge number of columns. Not as much for the reasons of
the data taking up too much memory, but for convenience. You can see that
our `df` metadata table has `r ncol(df)` columns, which don't fit on the
screen if we just print it out:

```{r}
df
```

We can select which columns to select with the function `select()`. It has
the following general format:

```
select(<data frame object or tibble>, <column 1>, <column 2>, ...)
```

This is how we would select columns using a normal base R subsetting/indexing
operation:

```{r}
df[, c("sampleId", "region", "coverage", "ageAverage")]
```

This is the _tidyverse_ approach using `select()`:

```{r}
select(df, sampleId, region, coverage, ageAverage)
```

**Note:** The most important thing for you to note now is the absence of "double
quotes". It might not look like much, but saving yourself from having to type double
quotes for every data-frame operation (like with base R) is incredibly convenient.

---

**Practice `select()` by creating three new data frame objects:**

1. Data frame `df_ages` which contains all variables related to sample ages
2. Data frame `df_geo` which contains all variables related to geography

The first column of these data frames should always be `sampleId`

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df_ages <- select(df, sampleId, groupAge, age14C, ageHigh, ageLow, ageAverage, ageRaw)
df_ages
```

```{r}
df_geo <- select(df, site, country, region, latitude, longitude)
df_geo
```

:::

---

Note that `select()` allows us to see the contents of columns of interest much easier.
In a situation in which we want to analyse the geographical location of samples,
we don't want to see columns unrelated to that -- `select()` helps here.

If your tables has many columns of interest, typing them all by hand can become
tiresome real quick. **Here are a few helper functions which can be very useful
in that situation:**

- `starts_with("age")` -- matches columns starting with the string "age"
- `ends_with("age")` -- matches columns ending with the string "age"
- `contains("age")` -- matches columns containing the string "age"

**Note:** You can use them in place of normal column names. If we would
modify our `select()` "template" above, we could do this, for instance:

```
select(<data frame object or tibble>, starts_with("text1"), ends_with("text2"))
```

---

**Check out the `?help` belonging to those functions. Note that they have
`ignore.case = TRUE` set by default! It will become important soon!**

---

**Create the `df_ages` table again, with the same criteria, but use the
three helper functions listed above. How many different ways of using them
can you come up with to arrive at the same result?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df_ages1 <- select(df, sampleId, groupAge, age14C, ageHigh, ageLow, ageAverage, ageRaw)

df_ages2 <- select(df,
                   sampleId,
                   starts_with("age", ignore.case = FALSE),
                   ends_with("Age", ignore.case = FALSE))

```

:::

---

## Exercise 3: Filtering rows

In the chapter on base R, we learned how to filter rows using the indexing
operation along the row-based dimension of (two-dimensional) data frames.
For instance, in order to find out which individuals in the metadata are archaic,
we first created a `TRUE` / `FALSE` vector of the rows corresponding to those
individuals like this:

```{r}
# get a vector of the indices belonging to archaic individuals
which_archaic <- df$groupAge == "Archaic"

# take a peek at the result to make sure we got TRUE / FALSE vector
tail(which_archaic, 10)
```

And then we would use it to index into the data frame (in its first dimension,
before the `,` comma):

```{r}
archaic_df <- df[which_archaic, ]
archaic_df
```

However, what if we need to filter on multiple conditions? For instance, what
if we want to find only archaic individuals older than 50000 years?

One option would be to create multiple `TRUE` / `FALSE` vectors, each
corresponding to one of those conditions, and then join them into a single
logical vector by combining `&` and `|` logical operators. For example, we
could do this:

```{r}
# who is archaic in our data?
which_archaic <- df$groupAge == "Archaic"
# who is older than 50000 years?
which_old <- df$ageAverage > 50000

# who is BOTH? note the AND logical operator!
which_combined <- which_archaic & which_old
```

```{r}
df[which_combined, ]
```

But this gets tiring very quickly, requires unnecessary amount of typing, and
is very error prone. Imagine having to do this for many more conditions!
The `filter()` function from _tidyverse_ fixes all of these problems.

We can rephrase the example situation with the archaics to use the new
function like this:

```{r}
#| results: hide
filter(df, groupAge == "Archaic", ageAverage > 50000)
```

I hope that, even if you never really programmed much before, you appreciate
that this single command involves very little typing and is immediately
readable, almost like this English sentence:

> _"Filter the data frame `df` for individuals in which column `groupAge`
is "Archaic", and who are older than 50000 years"_.

Over time you will see that all of _tidyverse_ packages follow these
ergonomic principles.

---

**Practice filtering with the `filter()` function by finding out
which individual(s) in your `df` metadata table are from a `country`
with the value `"CzechRepublic"`.**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
filter(df, country == "CzechRepublic")
```
:::

---

**Which one of these individuals have `coverage` higher than 1? Which
individuals have `coverage` higher than 10?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
filter(df, country == "CzechRepublic", coverage > 1)
```

```{r}
filter(df, country == "CzechRepublic", coverage > 10)
```

:::

---



## Exercise 4: The pipe `%>%`

The pipe operator, `%>%`, is the rockstar of the _tidyverse_ R ecosystem,
and the primary reason what makes _tidy_ data workflow so efficient
and easy to read.

First, what is "the pipe"?: whenever you see something like
`something `%>%` f()`, you can read it as

> _"take **something** and put it as the first argument of f()`".

Why would you want to do this? Imagine some complex data processing operation like this:

```{r}
#| eval: false
h(f(g(i(j(input_data)))))
```

This means take `input_data`, compute `j(input_data)`, then compute `i()` on _that_,
so `i(j(input_data))`, then compute `g(i(j(input_data)))`, etc. Of course this
is an extreme example but is surprisingly not that far off from what we often
have to do in data science.

One way to make this easier to read would be perhaps this:

```{r}
#| eval: false
tmp1 <- j(input_data)
tmp2 <- i(tmp1)
tmp3 <- g(tmp2)
tmp4 <- f(tmp3)
result <- f(tmp4)
```

But that's too much typing when we want to get insights into our data as quickly
as possible with as little work as possible.

The pipe approach of _tidyverse_ would make the same thing easier to write
and read like this:

```{r}
#| eval: false
input_data %>% j %>% i %>% g %>% f %>% h
```

This kind of "data transformation chain" is so frequent that RStudio even
provides a built-in shortcut for it:

- CMD + Shift + M on macOS
- CTRL + Shift + M on Windows and Linux

---

**Use your newly acquired `select()` and `filter()` skills, powered by the
`%>%` piping operator to perform the following transformation on the `df`
metadata table (`filter`-ing and `select`-ion operations on the specified columns):**

1. **Filter individuals for:**

- **"Ancient" individuals (`groupAge`)**
- **older than 10000 years (`ageAverage`)**
- **from Italy (`country`)**
- **with `coverage` higher than 3**

2. **Select columns: `sampleId`, `site`, `sex`, and `hgMT` and `hgYMajor`
(mt and Y haplogoups)**.

**As a practice, try to be as silly as you can and write the entire command
with as many uses of `filter()` and `select()` function calls in sequence
as you can.**

**Hint:** Don't write the entire pipeline at once, start with one condition,
evaluate it, then another another one, etc.,
inspecting the intermediate results as you're getting them. This is the
_tidyverse_ way of doing data science.

**Hint:** What I mean by this is that the following two commands produce
the exact same result:

```{r}
#| eval: FALSE
new_df1 <- filter(df, col1 == "MatchString", col2 > 10000, col3 == TRUE) %>%
  select(colX, colY, starts_with("ColNamePrefix"))
```

And doing this instead:

```{r}
#| eval: FALSE
new_df1 <- df %>%
  filter(col1 == "MatchString") %>%
  filter(col2 > 10000) %>%
  filter(col3 == TRUE) %>%
  select(colX, colY, starts_with("ColNamePrefix"))
```

_This works because "the thing on the left" (which is always a data frame)
is placed by the `%>%` pipe operator as "the first argument of a function
on the right" (which again expects always a data frame)!_

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df %>%
  filter(groupAge == "Ancient") %>%
  filter(ageAverage > 10000) %>%
  filter(country == "Italy") %>%
  filter(coverage > 3) %>%
  select(sampleId, site, sex, hgMT, hgYMajor)
```

We could also write the same thing more concisely (not the single `filter()` call):

```{r}
df %>%
  filter(groupAge == "Ancient", ageAverage > 10000, country == "Italy", coverage > 1) %>%
  select(sampleId, site, sex, hgMT, hgYMajor)
```

:::

**Note:** It is always a good practice to split long chains of `%>%` piping
commands into multiple lines, and indenting them neatly one after the other.
Readability matters to avoid bugs in your code!

---

## Exercise 5: More complex conditions

Recall our exercises about logical conditional expression (`&`, `|`, `!`, etc.).
You can interpret `,` in a `filter()` command as a logical `&`. In fact, you
could also write the solution to the previous exercise:

```{r}
#| results: hide
df %>%
  filter(groupAge == "Ancient", ageAverage > 10000, country == "Italy", coverage > 1) %>%
  select(sampleId, site, sex, hgMT, hgYMajor)
```

as this:

```{r}
#| results: hide
df %>%
  filter(groupAge == "Ancient" & ageAverage > 10000 & country == "Italy" & coverage > 1) %>%
  select(sampleId, site, sex, hgMT, hgYMajor)
```


Whenever you need to do a more complex operation, such as saying that a
variable `columnX` should have a value `"ABC"` or `"ZXC"`, you can write
`filter(df, columnX == "ABC" | column == "ZXC")`.

Similarly, you can condition on numerical variables, just as we did in the
exercises on `TRUE` / `FALSE` expressions. For instance, if you
want to condition on a column `varX` being `varX < 1` _or_ `varX > 10`, you could
write `filter(df, varX < 1 | var X > 10)`.

---

**Practice combining multiple filtering conditions into a _tidyverse_ piping
chain by filtering our metadata table to find individuals for which the
following conditions hold:**

- **They are "Ancient" (`groupAge`).**
- **The `country` is "France" _or_ "Canary Islands".**
- **Their `coverage` less than 0.1 or more than 3.**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df %>%
  filter(groupAge == "Ancient") %>%
  filter(country == "France" | country == "Canary Islands") %>%
  filter(coverage < 0.1 | coverage > 3)
```

Whenever you want to test whether a variable is of a set of multiple possible
values, you can use the `%in%` operator:

```{r}
df %>%
  filter(groupAge == "Ancient") %>%
  filter(country %in% c("France", "Canary Islands")) %>%
  filter(coverage < 0.1 | coverage > 3)
```

:::

---



## Exercise 6: Dropping columns

This one will be easy. If you want to drop a column from a table, just
prefix it with a minus sign (`-`). Yes, this also works with `starts_with()`
and its friends above, just put `-` in front of them!

Here's our `df` table again (just one row for brevity):

```{r}
df %>% head(1)
```

Observe what happens when we do this:

```{r}
df %>% select(-sampleId) %>% head(1)
```

And this:

```{r}
df %>% select(-sampleId, -site) %>% head(1)
```

And this:

```{r}
df %>% select(-sampleId, -site, -popId) %>% head(1)
```

The given columns are dropped from the resulting table!

Rather than typing out a long list of columns to drop, we can also do this
to specify the range of consecutive columns:

```{r}
df %>% select(-(sampleId:popId)) %>% head(1)
```

Alternatively, we can also use our well-known `c()` function, which is very useful
whenever we want to drop a non-consecutive set of columns:

```{r}
df %>% select(-c(sampleId, site, popId)) %>% head(1)
```


**Note:** The same "range syntax" of using `:` and and listing columns with
`c()` applies also to selecting which columns to choose, not just for dropping them.

---

**Use the `:` range in `select()` to drop every column after `country` (i.e.,
all the way to the last column in your table).**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df %>% select(-(region:shape))
```

I was personally a bit surprised that this also works without the parentheses.
R is smart!

```{r}
df %>% select(-region:-shape)
```
:::



## Exercise 7: Renaming columns

This one is rather simple. Very often you read data frames in which columns
have names which are either very long, containing characters which are not
allowed, or generally inconvenient. Imagine a situation, in which you
refer to a particular column very often in your workflow, but it takes too
much time to type it out.

After discussing `select()` and `filter()`, let's introduce another member of the _tidyverse_ -- the function `rename()`.

The template for using it is again very easy (again, you would replace
the text in `<here>` with appropriate symbols):

```{r}
#| eval: false
rename(<data frame object>, <new_name> = <old name>)
```

---

**Create a new data frame `df_subset`. First `select()` the columns `sampleId`,
`popId`, `country`, `groupAge`, `ageAverage`, and `coverage`. Then use the
`rename()` function to give them a shorter name: `sampleId` -> `sample`,
`popId` -> `population`, `groupAge` -> `group`, `ageAverage` -> `age`. Leave
the `country` and `coverage` columns as they are.**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df_subset <-
  df %>%
  select(sampleId, popId, country, groupAge, ageAverage, coverage) %>%
  rename(sample = sampleId, population = popId, group = groupAge, age = ageAverage)
```

We now have a much cleaner table which is much easier to work with!
:::

---

**A shortcut which can be quite useful sometimes is that `select()` also
accepts the `new_name = old_name` syntax of the `rename()` function, which
allows you to both select columns (and rename some of them) all at once.
Create the `df_subset` data frame again, but this time using just `select()`.**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df_subset <- df %>%
  select(sample = sampleId, population = popId,
         country,
         group = groupAge, age = ageAverage,
         coverage)
```

:::

---

**When would you use one or the other?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Answer: `select()` always drops the columns which are not explicitly listed.
`rename()` only renames the columns which are listed, but retains everything().

:::

---




## Exercise 8: Reorganizing columns

Let's look at another useful application of the `select()` function and
that is reordering columns. Our `df` metadata table has `r ncol(df)` columns.
When we print it out, we only see a couple of them:

```{r}
head(df, 3)
```

Oftentimes when doing data analysis, we often work interactively in the
console, focusing on a specific subset of columns, and need to immediately
see the values of our columns of interest -- how can we do this?

We already know that we can use `select()` to pick those columns of interest,
but this removes the non-selected columns from the data frame we get. Whenever
we want to retain them, we can add the call to `everything()`, like this:

```{r}
#| eval: false
select(df, <column 1>, <column 2>, ..., everything())
```

Which effectively moves `<column 1>`, `<column 2>`, ... to the "front" of our
table, and adds everything else at the end.

**Select the subset of columns you selected in the previous exercise on renaming
in exactly the same way, but this time add a call to `everything()` at the end
to keep the entire data set intact (with just columns rearranged). Save the
result to `df` again.**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Our data frame before:

```{r}
head(df, 3)
```

```{r}
df <- select(df,  sample = sampleId, population = popId, country,
             group = groupAge, age = ageAverage, coverage,
             everything()
)
```

Our data frame after:

```{r}
head(df, 3)
```

Notice that we prioritized the selected columns of interest (and also
renamed some for more readability), but we still have all the other columns
available!
:::

## Exercise 9: Sorting tables

## Exercise 10: Mutating tables

## Exercise 12: Summarizing tables
