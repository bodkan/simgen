# Working with tables

Before we begin, let's introduce the two important parts of the _tidyverse_
ecosystem, which we will be using extensively during exercises in this chapter.

1. [_dplyr_](https://dplyr.tidyverse.org) is a centerpiece of the entire R data science universe, providing
important functions for data manipulation, data summarization, and filtering
of tabular data;

2. [_tidyr_](https://tidyr.tidyverse.org) is an R package which helps us morph untidy data into tidy data frames;

3. [_readr_](https://readr.tidyverse.org) is an R package which provides very convenient functions for reading
(and writing) tabular data. Think of it as a set of better alternatives to
base R functions such as `read.table()`, etc. (Another very useful package
is [_readxl_](https://readxl.tidyverse.org) which is useful for working with
Excel data).

Every single script you will be writing in this session will begin with these
two lines of code.

```{r}
#| message: false
library(dplyr)
library(tidyr)
library(readr)
```

Let's also introduce a second star in this session, our example data set.
The following command will read a metadata table with information about
individuals published in a recent aDNA paper on the history or the Holocene in
West Eurasia, dubbed "MesoNeo"
([reference](https://www.nature.com/articles/s41586-023-06865-0)).
We use the `read_tsv()` function (from the above-mentioned _readr_ package)
and store it in a variable `df`. Everything in the section on _tidyverse_ will
revolve around working with this data because it represents an excellent
example of this kind of metadata table you will find across the entirety of
computational population genomics. (Yes, this function is quite magical -- it
can read stuff from a file stored on the internet. If you're curious about
the file itself, just paste the URL address in your browser.)

```{r}
#| message: false
df <- read_tsv("https://tinyurl.com/qwe-asd-zxc")
```

**Create a new R script in RStudio, (`File` `->` `New file` `->` `R Script`) and
save it somewhere on your computer as `tidy-tables.R` (`File` -> `Save`). Put
the `library()` calls and the `read_tsv()` command above in this script, and
let's get started!**

## A selection of data-frame inspection functions

Whenever you get a new source of data, like a table from a collaborator, data
sheet downloaded from supplementary materials of a paper, you need to familiarize
yourself with it before you do anything else. Here is a list of functions that
you will be using constantly when doing data science to answer basic questions:
Use this list as a cheatsheet of sorts! **Also, use `?<function>` to look up
their documentation to see the possibilities for options and additional features.**

1. _"How many observations (rows) and variables (columns) does my data have?"_ -- `nrow()`, `ncol()`

2. _"What variable (column) names am I going to be working with?"_ -- `colnames()`

3. _"What data types (numbers, strings, logicals, etc.) does it contain?"_ -- `str()`, or better, `glimpse()`

4. _"How can I take a quick look at a subset of the data?"_ -- `head()`, `tail()`

5. _"For a specific variable column, what is the distribution of values I can
expect in that column?"_ -- `table()` for "discrete types", `summary()` for
"numeric types", `min()`, `max()`, `which.min()`, `which.max()`

**Before we move on, note that when you type `df` into your R console,
you will see a slightly different format of the output than when we worked
with plain R data frames in the previous chapter. This format of data frame
data is called a ["tibble"](https://tibble.tidyverse.org) and represents
_tidyverse_'s more user friendly and modern take on data frames. For almost
all practical purposes, from now on, we'll be talking about _tibbles_ as
data frames.**

## Exercise 1: Familiarization with new data

**Inspect the `df` metadata using the functions above, and try to answer the
following questions using the functions from the list above (you should decide
which will be appropriate for which question).**

**Before you use one of these functions for the first time, take a moment to
skim through its `?function_name`.**

1. **How many individuals do we have metadata for?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Number of rows:

```{r}
nrow(df)
```

Number of `unique()` sample IDs (this should ideally always give the same number,
but there's never enough sanity checks in data science):

```{r}
length(unique(df$sampleId))
```
:::

2. **What data types do the variables in our data have?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Column names function:

```{r}
colnames(df)
```
:::

3. **What data types (numbers, strings, logicals, etc.) are our variables of?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

I tend to use `str()` because I'm old and very used to it, but `glimpse()`
is new and better.

```{r}
str(df)

glimpse(df)
```
:::

3. **What is the distribution of dates of aDNA individuals? What are the variables
we have that describe the ages (maybe look at those which have "age" in their name)?
Which one would you use to get information about the ages of most individuals?**

**Hint:** Remember that for a column/vector `df$<variable>`, `is.na(df$<variable>)`
gives you `TRUE` for each `NA` element in that column variable, and
`sum(is.na(df$<variable>))` counts the number of `NA`. Alternatively,
`mean(is.na(df$<variable>))` counts the proportion of `NA` values!

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

From `colnames(df)` above we see a number of columns which seem to have something
todo with "age":

```{r}
columns <- colnames(df)

# don't worry about this weird command, I'm using it to show you the relevant columns
# -- ask me if you're interested! :)
grep("^age.*|^.*Age$", columns, value = TRUE)
```

```{r}
mean(!is.na(df$age14C))
mean(!is.na(df$ageHigh))
mean(!is.na(df$ageLow))
mean(!is.na(df$ageAverage))
mean(!is.na(df$ageRaw))
unique(df$groupAge)
```

Interesting, it looks like the `ageAverage` variable has the highest proportion
of non-`NA` values at about `r 100 * round(mean(!is.na(df$ageAverage)), 4)`%.
We also seem to have another column, `groupAge` which clusters individuals into
three groups. We'll stick to these two variables whenever we have a question
regarding a date of an individual.

:::

4. **How many ancient individuals do we have? How many "modern" (i.e., present-day
humans) individuals?**

**Hint:** Use the fact that for any vector (and `df$groupAge` is a vector,
remember!), we can get the number of elements of that vector matching a certain
value by applying the `sum()` function on a "conditional" expression, like
`df$groupAge == "Ancient"` which, on its own, gives `TRUE` / `FALSE` vector.

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
which_ancient <- df$groupAge == "Ancient"

head(which_ancient)

sum(which_ancient)
```

```{r}
which_modern <- df$groupAge == "Modern"
sum(which_modern)
```

```{r}
which_archaic <- df$groupAge == "Archaic"
sum(which_archaic)
```
:::

5. **Who are the mysterious "Archaic" individuals (column `groupAge`)?**

**Hint:** We need to _filter_ our table down to rows which have
`groupAge == "Archaic"`. This is an indexing operation which you learned about
in the R bootcamp session! Remember that data frames can be indexed into along
two dimensions: rows and columns. You want to filter by the rows here.

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Again, this gets us a logical vector which has `TRUE` for each element corresponding
to an "Archaic" individual (whoever that might be):

```{r}
which_archaic <- df$groupAge == "Archaic"
head(which_archaic)
```

And we can then use this vector as an index into our overall data frame, just
like we learned in the bootcamp session:

```{r}
archaic_df <- df[which_archaic, ]
archaic_df$sampleId
```

Our mysterious "Archaic" individuals are two Neanderthals and a Denisovan!
:::


6. **Do we have geographical information? Countries or geographical coordinates or
both? Which countries do we have individuals from? (We'll talk about spatial
data science later in detail, so let's interpret whatever geographical
variable columns we have in our data frame `df` as any other numerical or string
column variable.)**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Looking at `colnames(df)` we have a number of columns which have something
to do with geography: `country`, `region`, and also the traditional
[`longitude` and `latitude` coordinates](https://en.wikipedia.org/wiki/Geographic_coordinate_system#Latitude_and_longitude).

`table()` counts the number of elements in a vector (basically, a histogram
without plotting it), `sort()` then sorts those elements for easier reading:

```{r}
sort(table(df$country), decreasing = TRUE)
```

```{r}
sort(table(df$region), decreasing = TRUE)
```

We will ignore `longitude` and `latitude` for now, because they are most
useful in truly geographical data analysis setting (which we will delve into
a bit later).
:::

6. **What is the source of the sequences in our data set? Does it look like they
all come from a single study?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

It looks like we have a `dataSource` column which, from the `glimpse()`
result above is a character vector (a vector of "strings"). This suggests
that it represents discrete categories, so let's use the `table()` & `sort()`
 combo again:
 
```{r}
sort(table(df$dataSource), decreasing = TRUE)
```

OK, it looks like we have most individuals (2504) from the
[1000 genomes project](http://www.internationalgenome.org), we have
318 samples from ["this study" ](https://www.nature.com/articles/s41586-023-06865-0),
and then a bunch of individuals from a wide range of other publications.
This is great because it allows us to refer individual specific results
we might obtain later to the respective publications!
:::


7. **What's the distribution of coverage of the samples? Compute the `mean()`
and other `summary()` statistics on the coverage information. Why doesn't
the `mean()` function work when you run it on the vector of numbers in the
`coverage` column (use `?mean` to find the solution). Which individuals
have missing coverage? Finally, use the `hist()` function to visualize this
information.**

These are all basic questions which you will be asking yourself _every time_
you read a new table into R. It might not be coverage, but you will have
to do this to familiarize yourself with the data and understand its potential
issues!

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

Mean gives us `NA`:

```{r}
mean(df$coverage)
```

This is because computing the mean from a vector containing `NA` values is not
a sensible operation -- should we remove the `NA` values? It's impossible to say
in general, because that would depend on the nature of our data. Sometimes,
encountering `NA` indicates a problem, so we can't just ignore it. In situation
when this is OK, there's a argument which changes the default behavior and
removes the `NA` elements before computing the mean:

```{r}
mean(df$coverage, na.rm = TRUE)
```

Summary takes care of the issue by reporting the number of `NA` values explicitly:

```{r}
summary(df$coverage)
```

We can see that the coverage information is missing for `r sum(is.na(df$coverage))`
individuals, which is the number of individuals in the (present-day) 1000 Genomes
Project data. So it makes sense, we only have coverage for the lower-coverage
aDNA samples, but for present-day individuals (who have imputed genomes), the
coverage does not even make sense here.

Let's plot the coverage values:

```{r}
hist(df$coverage, breaks = 100)
```

:::

8. **Who is the oldest individuals in our data set? Who has the highest
coverage? Who has the lowest coverage?**

**Hint:** Use functions `min()`, `max()`, `which.min()`, `which.max()`. Look
up their `?help` to understand what they do and how to use them in this context.

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

`which.max()` reports the _index_ (in this case, the row number) of the
maximum value in a vector (in this case, a column vector):

```{r}
which.max(df$ageAverage)
```

This means we can use this number to index into the data frame and find our answer:

```{r}
df[which.max(df$ageAverage), ]

max(df$ageAverage, na.rm = TRUE)
```

Let's use the similar approach for coverage:

```{r}
df[which.min(df$coverage), ]
min(df$coverage, na.rm = TRUE)

df[which.max(df$coverage), ]
max(df$coverage, na.rm = TRUE)
```

:::


## Exercise 2: Selecting columns

Often times we end up in a situation in which we don't want to have a large
data frame with a huge number of columns. Not as much for the reasons of
the data taking up too much memory, but for convenience. You can see that
our `df` metadata table has `r ncol(df)` columns, which don't fit on the
screen if we just print it out:

```{r}
df
```

We can select which columns to select with the function `select()`. It has
the following general format:

```
select(<data frame object or tibble>, <column 1>, <column 2>, ...)
```

This is how we would select columns using a normal base R subsetting/indexing
operation:

```{r}
df[, c("sampleId", "region", "coverage", "ageAverage")]
```

This is the _tidyverse_ approach using `select()`:

```{r}
select(df, sampleId, region, coverage, ageAverage)
```

**Note:** The most important thing for you to note now is the absence of "double
quotes". It might not look like much, but saving yourself from having to type double
quotes for every data-frame operation (like with base R) is incredibly convenient.

**Practice `select()` by creating three new data frame objects:**

1. Data frame `df_ages` which contains all variables related to sample ages
2. Data frame `df_geo` which contains all variables related to geography

The first column of these data frames should always be `sampleId`

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df_ages <- select(df, sampleId, groupAge, age14C, ageHigh, ageLow, ageAverage, ageRaw)
df_ages
```

```{r}
df_geo <- select(df, site, country, region, latitude, longitude)
df_geo
```

:::

Note that `select()` allows us to see the contents of columns of interest much easier.
In a situation in which we want to analyse the geographical location of samples,
we don't want to see columns unrelated to that -- `select()` helps here.

If your tables has many columns of interest, typing them all by hand can become
tiresome real quick. **Here are a few helper functions which can be very useful
in that situation:**

- `starts_with("age")` -- matches columns starting with the string "age"
- `ends_with("age")` -- matches columns ending with the string "age"
- `contains("age")` -- matches columns containing the string "age"

**Note:** You can use them in place of normal column names. If we would
modify our `select()` "template" above, we could do this, for instance:

```
select(<data frame object or tibble>, starts_with("text1"), ends_with("text2"))
```

**Check out the `?help` belonging to those functions. Note that they have
`ignore.case = TRUE` set by default! It will become important soon!**

**Create the `df_ages` table again, with the same criteria, but use the
three helper functions listed above. How many different ways of using them
can you come up with to arrive at the same result?**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df_ages1 <- select(df, sampleId, groupAge, age14C, ageHigh, ageLow, ageAverage, ageRaw)

df_ages2 <- select(df,
                   sampleId,
                   starts_with("age", ignore.case = FALSE),
                   ends_with("Age", ignore.case = FALSE))

```


:::

## Exercise 3: Filtering rows

In the chapter on base R, we learned how to filter rows using the indexing
operation along the row-based dimension of (two-dimensional) data frames.
For instance, in order to find out which individuals in the metadata are archaic,
we first created a `TRUE` / `FALSE` vector of the rows corresponding to those
individuals like this:

```{r}
# get a vector of the indices belonging to archaic individuals
which_archaic <- df$groupAge == "Archaic"

# take a peek at the result to make sure we got TRUE / FALSE vector
tail(which_archaic, 10)
```

And then we would use it to index into the data frame (in its first dimension,
before the `,` comma):

```{r}
archaic_df <- df[which_archaic, ]
archaic_df
```

However, what if we need to filter on multiple conditions? For instance, what
if we want to find only archaic individuals older than 50000 years?

One option would be to create multiple `TRUE` / `FALSE` vectors, each corresponding
to one of those conditions, and then join them into a single logical vector
by combining `&` and `|` logical operators. For example, we could do this:

```{r}
# who is archaic in our data?
which_archaic <- df$groupAge == "Archaic"
# who is older than 50000 years?
which_old <- df$ageAverage > 50000

# who is BOTH? note the AND logical operator!
which_combined <- which_archaic & which_old
```

```{r}
df[which_combined, ]
```

But this gets tiring very quickly, requires unnecessary amount of typing, and
is very error prone. Imagine having to do this for many more conditions!
The `filter()` function from _tidyverse_ fixes all of these problems.

Again, we can rephrase the example situation with the archaics to use the new
function like this:

```{r}
filter(df, groupAge == "Archaic", ageAverage > 50000)
```

A single command, very little typing, and immediately readable, almost like English!
_"Filter data frame `df` for individuals in which column `groupAge` is "Archaic",
and who are older than 50000 years"_. When you think about it, it's pretty much
impossible to find a solution requiring less code.


## Exercise 4: The pipe `%>%`

The pipe operator, `%>%`, is the rockstar of the _tidyverse_ R ecosystem,
and the primary reason what makes _tidy_ data approach so efficient, quick
to type, and easy to read.

First, a bit of a general introduction: whenever you see something like
`something `%>%` f()`, you can read it as "take <something> and put it as the
first argument of f()`".

Why would you do this? Imagine some complex data processing operation like this:

```{r}
#| eval: false
h(f(g(i(j(input_data)))))
```

I.e., take `input_data`, compute `j(input_data)`, then compute `i()` on _that_,
so `i(j(input_data))`, then compute `g(i(j(input_data)))`, etc. Of course this
is a ridiculous example but it is not that far off from what we often have to do.

One way to make this easier to read would be perhaps this:

```{r}
#| eval: false
tmp1 <- j(input_data)
tmp2 <- i(tmp1)
tmp3 <- g(tmp2)
tmp4 <- f(tmp3)
result <- f(tmp4)
```

But that's just too much typing, especially during exploratory phase of data
analysis in which we want to get insights into our data as quickly as possible,
and when we spend most of our time not writing scripts but playing with our
data in the R console!

The pipe approach of _tidyverse_ would make this infinitely easier to write
and read (note that we are allowed to even drop the function call parentheses):

```{r}
#| eval: false
input_data %>% j %>% i %>% g %>% f %>% h
```

This kind of "data transformation chain" is so frequent that RStudio even
provides a built-in shortcut for it:

- CMD + Shift + M on macOS
- CTRL + Shift + M on Windows and Linux


**Use your newly acquired `select()` and `filter()` skills, powered by the
`%>%` piping operator to perform the following transformation on the `df`
metadata table (`filter`-ing and `select`-ion operations on the specified columns):**

1. filter for "Ancient" individuals (`groupAge`) who are older than 10000 years
(`ageAverage`), are from Italy (`country`), and have `coverage` higher than 3
2. select `sampleId`, `site`, `sex`, and `hgMT` and `hgYMajor` haplogoups

**As a practice, try to be as silly as you can and write the entire command
with as many uses of `filter()` and `select()` function calls in sequence
as you can. Don't write the entire pipeline at once, start with one condition,
evaluate it, then another another one, etc.,
inspecting the intermediate results as you're getting them.** This is the
_tidyverse_ way of doing data science.

**Hint:** What I mean by this is that the following two commands produce
the exact same result:

```{r}
#| eval: FALSE
new_df1 <- filter(df, col1 == "MatchString", col2 > 10000, col3 == TRUE) %>%
  select(colX, colY, starts_with("ColNamePrefix"))
```

And doing this instead:

```{r}
#| eval: FALSE
new_df1 <- df %>%
  filter(col1 == "MatchString") %>%
  filter(col2 > 10000) %>%
  filter(col3 == TRUE) %>%
  select(colX, colY, starts_with("ColNamePrefix"))
```

_This works because "the thing on the left" (which is always a data frame)
is placed by the `%>%` pipe operator as "the first argument of a function
on the right" (which again expects always a data frame)!_

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df %>%
  filter(groupAge == "Ancient") %>%
  filter(ageAverage > 10000) %>%
  filter(country == "Italy") %>%
  filter(coverage > 3) %>%
  select(sampleId, site, sex, hgMT, hgYMajor)
```

We could also write the same thing more concisely (not the single `filter()` call):

```{r}
df %>%
  filter(groupAge == "Ancient", ageAverage > 10000, country == "Italy", coverage > 1) %>%
  select(sampleId, site, sex, hgMT, hgYMajor)
```

:::

**Note:** It is always a good practice to split long chains of `%>%` piping
commands into multiple lines, and indenting them neatly one after the other.
Readability matters to avoid bugs in your code!

## Exercise 5: More complex conditions

You can interpret `,` in a `filter()` command as a logical `&`. In this case
of the previous exercise, every single condition must apply to arrive at the
individual `r df %>% filter(groupAge == "Ancient", ageAverage > 10000, country == "Italy", coverage > 1) %>% select(sampleId, site, sex, hgMT, hgYMajor) %>% pull(sampleId)`.

Whenever you need to do a more complex operation, such as saying that a
variable `columnX` should have a value `"ABC"` or `"ZXC"`, you can write
`filter(df, columnX == "ABC" | column == "ZXC")`. Alternatively, you could
write the the same thing using the `%in%` operator as 
`filter(df, columnX %in% c("ABC", "ZXC"))`, which is very useful when you
want to match multiple values than you'd be willing to type out by hand
(but have them stored in a variable).

Similarly, you can condition on numerical variables. For instance, if you
want to condition on variable being `varX < 1` or `varX > 10`, you could
write `filter(df, varX < 1 | var X > 10)`.

**Practice combining multiple filtering conditions into a _tidyverse_ piping
chain by filtering our metadata table to find individuals for which the
following conditions hold:**

- they are "Ancient" in `groupAge`
- `country` is either "France" or "Canary Islands"
- `coverage` either less than 0.1 or more than 3

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

```{r}
df %>%
  filter(groupAge == "Ancient") %>%
  filter(country == "France" | country == "Canary Islands") %>%
  filter(coverage < 0.1 | coverage > 3)
```

:::

---



## Exercise 6: Dropping columns

This one will be easy. If you want to drop a column from a table, just
prefix it with a minus sign (`-`). Yes, this also works with `starts_with()`
and its friends above, just put `-` in front of them!




## Exercise 7: Renaming columns

## Exercise 8: Reorganizing columns

## Exercise 9: Sorting tables

## Exercise 10: Mutating tables

## Exercise 12: Summarizing tables
