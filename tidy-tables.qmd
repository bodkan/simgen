# Manipulating tabular data

Before we begin, let's introduce the two important parts of the _tidyverse_
ecosystem, which we will be using here:

1. _dplyr_ is a centerpiece of the entire R data science universe, providing
important functions for data manipulation, data summarization, and filtering
of tabular data;

2. _tidyr_ is an R package which helps us morph untidy data into tidy data frames;

3. _readr_ is an R package which provides very convenient functions for reading
(and writing) tabular data. Think of it as a set of better alternatives to
base R functions such as `read.table()`, etc.

Every single script you will be writing in this session will begin with these
two lines of code.

```{r}
#| message: false
library(dplyr)
library(tidyr)
library(readr)
```

Let's also introduce a second star in this session, our example data set.
This commands reads a metadata table from a recent huge aDNA paper on the
history or the Holocene in West Eurasia, dubbed "MesoNeo" 
([reference](https://www.nature.com/articles/s41586-023-06865-0)).
You can read it like this, which will save it to a variable `df`.
_Everything_ in the section on tidyverse will revolve around this data.

```{r}
#| message: false
df <- read_tsv("https://tinyurl.com/qwe-asd-zxc")
```

## Part XYZ: Inspecting data

Whenever you get a new source of data, like a table from a collaborator, data
sheet downloaded from supplementary materials of a paper, you need to familiarize
yourself with it before you do anything else. Here is a list of functions that
you will be using constantly when doing data science to answer basic questions:
Use this list as a cheatsheet of sorts!

1. _"How many observations (rows) and variables (columns) does my data have?"_ -- `nrow()`, `ncol()`

2. _"What variable (column) names am I going to be working with?"_ -- `colnames()`

3. _"How can I take a quick look at a subset of the data?"_ -- `head()`, `tail()`

4. _"What data types (numbers, strings, logicals, etc.) does it contain?"_ -- `str()`, or better, `glimpse()`

5. _"For a specific variable column, what is the distribution of values I can expect?"_ -- `table()` for "discrete types", `summary()` for "numeric types"

**Inspect the `df` metadata using the functions above, and try to answer the
following questions using the functions from the list above (you should decide
which will be appropriate for which question)**:

1. How many individuals do we have metadata for?

2. What data types do the variables in our data carry?

3. What is the distribution of dates of aDNA individuals? What are the variables
we have that describe the ages? Which one would you use to get information about
the ages of most individuals?

4. How many present-day individuals do we have? How many aDNA individuals?

5. Do we have geographical information? Countries or geographical coordinates or
both? Which countries do we have individuals from? (We'll talk about spatial
data science later in detail.) 

6. What is the source of the data? Does it look like it comes from a single study?

7. What's the distribution of coverage of the samples? Do we have that information
for every individual? Feel free to use the base R function `hist()` for this.**

::: {.callout-note collapse="true" icon=false}
#### Click to see the solution

1. How many individuals do we have metadata for?

```{r}
# number of rows of the table
nrow(df)
# number of unique sample IDs (this should ideally always give the same number,
# but there's never enough sanity checks in data science)
length(unique(df$sampleId))
```

2. What variables (columns) do we have for each individual, and what are their
data types?

```{r}
colnames(df)
```

(I tend to use `str()` because I'm old and very used to it, but `glimpse()`
is new and better.)

```{r}
str(df)

glimpse(df)
```

3. What is the distribution of dates of aDNA individuals? What are the variables
we have that describe the ages? Which one would you use to get information about
the ages of most individuals. For instance, let's pick a variable which has
the least number of `NA` values.

**Hint:** Remember that for a column/vector `df$<variable>`, `is.na(df$<variable>)`
gives you `TRUE` for each `NA` element in that column variable, and
`sum(is.na(df$<variable>))` counts the number of `NA`. Alternatively,
`mean(is.na(df$<variable>))` counts the proportion of `NA` values!

From `colnames(df)` above we see a number of columns which seem to have something
todo with "age":

```{r}
columns <- colnames(df)

# don't worry about this weird command, I'm using it to show you the relevant columns
# -- ask me if you're interested! :)
grep("^age.*|^.*Age$", columns, value = TRUE)
```


4. How many present-day individuals do we have? How many aDNA individuals?

Let's practice
```{r}

```


## Part XYZ: Selecting columns

## Part XYZ: Filtering rows

**Base R indexing recap**

## Part XYZ: Selecting columns

## Part XYZ: Summarizing data